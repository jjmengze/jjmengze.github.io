[{"content":"在 1992 年的 SQL 標準中，對於資料庫交易的隔離級別做了定義，這些定義是基於能夠防止某些不正常現象的來區分。不正常現象包括\n髒讀 (Dirty Read) 不可重复讀(Non-Repeatable Read/read skew) 幻讀（Phantom Read） 這些定義在當時被認為是有點模糊的，也有專家如Jim Gray指出這些定義是不合理的。當時，多版本並發控制（MVCC）和快照隔離（Snapshot\nIsolation）這些技術還未被發明。這些技術的出現使得隔離級別的定義變得更加清晰。\n以下文章將簡要的說明 Isolation Level 的定義以及整理想關的知識體系如 ACID 、Transaction 、 Lock等\u0026hellip;。\nACID 在2000年，Google 的副總裁 Eric Brewer 首次提出 CAP 理論。在 CAP 理論誕生前有個對於資料庫的運作至關重要的概念：ACID。\n隨著網際網路和雲端技術的興起，資料庫是期研究的重點之一。以 wiki 提出的例子來說。\n在你我他都要存取資料的背景下，「交易」（Transaction）的概念顯得尤為重要。\n以轉帳為例：當你從銀行轉帳時，資料庫首先從你的帳戶扣除金額，然後在收款方的帳戶中增加相同的金額。這一系列動作構成了一個「交易」。資料庫系統必須確保這樣的交易是安全和有效的，這就引入了ACID特性。\nACID特性包括：\n原子性（Atomicity）：指的是 Transaction 內的所有操作要麼全部成功，要麼全部失敗。 一致性（Consistency）：保證 Transaction 將資料庫從一個合法狀態轉移到另一個合法狀態，即Transaction完成後，所有資料都應該符合規則。 隔離性（Isolation）：即使多個 Transaction 同時進行，系統也能保證它們執行的結果，就如同它們是按順序一個接一個執行的一樣。 持久性（Durability）：一旦 Transaction 完成，它對資料的修改就是永久的，即使系統發生故障也不會丟失。\n然而，不同資料庫實現 ACID 的方式各不相同。特別是在隔離性方面，各有不同的解釋和實踐。雖然從高層次上看，這些原則聽起來很理想，但具體到實施細節時，就會出現很多挑戰。因此，當系統聲稱它符合\nACID 時，實際上能提供哪些保證，這一點往往並不那麼明確。 transaction 當我們在談論資料庫交易（Transactions）時，一定會涉及到多個 Transactions\n同時訪問同一筆資料。資料庫為了防止資料不一致和其他問題，通常會實施一些鎖定機制（Locks）。這些鎖定機制可以分為幾種類型，每種都有其特定的用途和規則。\nExclusive Lock 專屬鎖（Exclusive Lock，也稱為寫鎖或X-Lock）\n當一個 Transaction 對某筆資料加上 Exclusive Lock 後，該 Transaction 就可以對這筆資料進行修改。持有 Exclusive Lock\n鎖定期間，其他的交易既不能讀取也不能修改這筆資料。\n如何得到 Exclusive Lock 如果一筆資料已經被其他 Transaction 加上共享鎖（Shared Lock）或是其他類型的鎖，那麼就無法對這筆資料加上 Exclusive Lock。\n想要得到 Exclusive Lock 必須等待直到該筆資料的所有鎖都被釋放，才能對其上鎖。\n1 2 3 SELECT * FROM table WHERE id = 1 FOR UPDATE; Shared Lock 共享鎖（Shared Lock，也稱為讀鎖或S-Lock）\n當一筆資料被加上 S-Lock 時，其他 Transaction 就不能對這筆資料加上 Exclusive Lock ，持有這種鎖的 Transaction 不能修改該筆資料，其他\nTransaction 仍然可以對這筆資料進行讀取。\n如何得到 Shared Lock Transaction 可以同時對同一筆資料加上 Shared Lock ，在資料存在 Shared Lock 的情況下，不能對該資料加上 Exclusive Lock。一個例外是。\n但如果一個 Transaction 已經對某筆資料加上了 Shared Lock，它可以將鎖「升級」為 Exclusive Lock。\n1 2 3 SELECT * FROM table WHERE id = 1 LOCK IN SHARE MODE; 範圍鎖（Range Lock） 幻讀 Phantom\n後續章節會提到，\n範圍鎖是一種特殊類型的鎖，用於鎖定一系列的資料 主要目的是防止所謂的「幻讀」(Phantom)現象。\n舉例來說，如果你在資料庫中執行一個範圍查詢並加上鎖定，比如選擇體重大於或等於80公斤的人並使用 Exclusive Lock\n模式，不僅被查詢到的資料會被加上 Exclusive Lock 。\n從170公分到無限大的範圍內的資料也會被加上 Exclusive Lock 。在這個範圍內的資料無法被讀取或修改。\n如何得到 Range Lock 1 2 3 SELECT * FROM table WHERE weight \u0026gt; 80 FOR UPDATE; 不同的資料庫在不同的 Isolation Leve 會有不同的行為\n以Mysql為例 以 Mysql 且 DB engine 為 InnoDB 為例:\n在 Isolation Level 設定為 可重複讀取（Repeatable Read）隔離級別下，由於資料庫使用實作模式為快照隔離（Snapshot Isolation），其他\nTransaction 可以讀取 Range Lock 範圍內的快照資料，但仍然不能進行修改。\n在 Isolation Level 設定為 串型執行 Serializable 的時候，其他 Transaction 不能讀取也不能寫入介於 Range Lock 內的資料。\nIsolation Level Read Uncommitted Dirty Read 假設有兩筆 Transaction 正在進行，分別是 Transaction A 和 Transaction B。\nTransaction A 更新了一筆資料，例如將資料的數量欄位（amount）從 10 更新成 6，更新但尚未確認（commit）。\nTransaction B 嘗試讀取這筆資料，這時候 Transaction B 可以讀到 Transaction A 更新但尚未確認（commit）的資料（例如，amount =\n6）。\n這種情況被稱為「髒讀」（Dirty Read）。這可能導致 Transaction B 基於未確定的資料做出錯誤的決策，從而影響系統的一致性。\nRead Committed Non-repeatable Read / Read Skew 假設有兩筆 Transaction 正在進行，分別是 Transaction A 和 Transaction B。\nTransaction A 更新了一筆資料，例如將資料的數量欄位（amount）從 10 更新成 6，更新但尚未確認（commit）。\n這時候 Transaction B 第一次嘗試讀取這筆資料時，這時候 Transaction B\n只能看到這筆資料最後一次被確認保存的版本，也就是數量(amount = 10)。\nTransaction A 將該次資料的改動進行最終確認與保存（也就是 \u0026ldquo;Commit\u0026rdquo; 該筆資料 ）。\nTransaction B 在第二次讀取中會看到新的資料，也就是數量(amount = 6)。\n因為 Transaction B 在同一個 Transaction 過程中兩次讀取同一條資料卻得到了不同的結果，這種情況被稱為「不可重複讀」（Non-repeatable\nRead / Read Skew)。 這種現像在資料庫管理中是需要避免的，因為它會導致資料的不一致性和潛在的錯誤。\nRepeatable Read snapshot isolation 其實也沒有準確的定義\n因此MySQL 和 PG, Oracle 等等的實作也是有很大的差別的.\npg, oracle 等等版本實作的定義：ref:https://www.postgresql.org/docs/13/transaction-iso.html#XACT-READ-COMMITTED\nUPDATE, DELETE, SELECT FOR UPDATE, and SELECT FOR SHARE commands behave the same as SELECT in terms of searching for\ntarget rows: they will only find target rows that were committed as of the transaction start time. However, such a\ntarget row might have already been updated (or deleted or locked) by another concurrent transaction by the time it is\nfound. In this case, the repeatable read transaction will wait for the first updating transaction to commit or roll\nback (if it is still in progress). If the first updater rolls back, then its effects are negated and the repeatable read\ntransaction can proceed with updating the originally found row. But if the first updater commits (and actually updated\nor deleted the row, not just locked it) then the repeatable read transaction will be rolled back with the message\nfirst-committer-wins 的定義：\n一個交易（稱為trx1）從開始到提交的過程中，如果另一個教義（稱為trx2）修改了trx1正在修改的資料，並且在trx1提交之前，trx2已經提交完成，那麼trx1將會被終止（abort）。簡單來說，就是誰先提交，誰就能成功，後來的教義如果與先提交的教義有衝突，後來的教義就會失敗。\nMySQL 版本實作的定義： ref:https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html\nBut when InnoDB Repeatable Read transactions modify the database, it is possible to get phantom reads added into the\nstatic view of the database, just as the ANSI description allows. Moreover, InnoDB relaxes the ANSI description for\nRepeatable Read isolation in that it will also allow non-repeatable reads during an UPDATE or DELETE. Specifically, it\nwill write to newly committed records within its read view. And because of gap locking, it will actually wait on other\ntransactions that have pending records that may become committed within its read view. So not only is an UPDATE or\nDELETE affected by pending or newly committed records that satisfy the predicate, but also \u0026lsquo;SELECT … LOCK IN SHARE MODE\u0026rsquo;\nand \u0026lsquo;SELECT … FOR UPDATE\u0026rsquo;.\nThis WRITE COMMITTED implementation of REPEATABLE READ is not typical of any other database that I am aware of. But it\nhas some real advantages over a standard \u0026lsquo;Snapshot\u0026rsquo; isolation. When an update conflict would occur in other database\nengines that implement a snapshot isolation for Repeatable Read, an error message would typically say that you need to\nrestart your transaction in order to see the current data. So the normal activity would be to restart the entire\ntransaction and do the same changes over again. But InnoDB allows you to just keep going with the current transaction by\nwaiting on other records which might join your view of the data and including them on the fly when the UPDATE or DELETE\nis done. This WRITE COMMITTED implementation combined with implicit record and gap locking actually adds a serializable\ncomponent to Repeatable Read isolation.\n並不遵循上述的 first-committer-wins\n原則。在這種情況下，如果trx1和trx2都修改了同一條資料，而trx2先提交了，當trx1後來提交時，它會直接覆蓋掉trx2之前的提交。這意味著，即使trx2先提交，trx1仍然有可能成功，這與\nfirst-committer-wins 的規則相違背。\nLose update 假設有兩筆 Transaction 正在進行，分別是 Transaction A 和 Transaction B。\nTransaction A 和 Transaction B 同時嘗試更新資料庫中同一筆資料的數量(amount)欄位。理想情況下，Transaction B 更新之前應該要考慮到\nTransaction A剛剛所做的更改。\n如果兩個 Transaction 不確定先後順序的話，有可能出現 Transaction B 會不知情地覆蓋掉 Transaction A 的更新，導致 Transaction\nA 的更改遺失。這就是所謂的「丟失更新」（Lost Update）。\n為了防止這種情況，可以使用不同的隔離級別：\n可序列化隔離(Serializable Isolation）：這是最嚴格的 Isolation Level，在這個級別之下可以防止丟失更新問題。Serializable 強制讓\nTransaction 排隊執行（一次只執行一個）來保持資料的一致性。 可重複讀隔離(Repeatable Read Isolation）：這個隔離級別的行為取決於資料庫的具體實作方式。 在 MySQL 的 InnoDB engine 中，Repeatable Read Isolation ，實作基於快照隔離（Snapshot Isolation），每個 Transaction\n看到都是自己的 Snapshot無法防止 Lose Update 的問題。 在 Postgresql 的 Repeatable Read Isolation 實作基於 first-committer-wins 也就是先寫先贏，後寫吐 error ，故可以防止這個問題。 Phantom 假設有兩筆 Transaction 正在進行，分別是 Transaction A 和 Transaction B。\n假設你在做一個列表，要記下所有數量少於5個的物品。\nTransaction A 第一次檢查時，發現有三樣物品符合條件，分別是 A 、 B 和 C 。同一個 Transaction 隔一段時間再次檢查列表時，發現突然多了一樣物品\nD，(沒有改變查詢條件)。\n為什麼會多出來一個商品 D ?\n這個多出來的物品 D 是由於另一個 Transaction B 在 Transaction A 的兩次檢查之間添加的。\n對於 Transaction A 來說這種情況就像是是物品 D 憑空出現，這種狀況稱為「幻讀」(Phantom)\n。這種現像在資料庫管理中是需要避免的，因為它會導致資料的不一致性和潛在的錯誤。\nWrite Skew 假設有兩筆 Transaction 正在進行，分別是 Transaction A 和 Transaction B。\n想象一下，你和你的同事都在嘗試賣出2個同樣的物品A。在賣出之前，你們都會檢查現有的庫存以確保至少有2個可以賣。如果檢查結果顯示庫存足夠，那麼你們就會進行賣出操作。但是，如果你們同時進行檢查，都認為庫存足夠，然後各自減去2個庫存，就會出現問題。最終可能導致庫存數字是負數，這明顯是不對的，因為實際上庫存不能是負數。這種因為兩個進程基於相同的前提做出的決策，但最終結果卻互相衝突，就是所謂「寫入扭曲」。\nSerializable 資料庫處理交易（Transactions）時最嚴格的隔離方式。它確保了當多個交易同時對資料庫進行操作時，其最終的結果與這些交易一個接一個、依次序進行時的結果是完全一樣的。這就好比是在排隊服務，每個人都會依次得到服務，前一個人完成之後下一個人才能開始。這樣的處理方式可以避免許多資料不一致的問題，例如剛剛提到的「幻讀」（Phantom\nRead）和「寫入扭曲」（Write Skew）。\n真序列化（Serial Order）： 這種方式確保交易是真正按照順序一個接一個進行的。這種方法通常適用於那些讀寫速度非常快的記憶體內（In-Memory）資料庫，例如\nRedis。儘管這種方法可以避免所有的同步問題，但效能較差，因為它完全不允許交易並行。\n兩階段鎖定（Two-phase Locking）： 這種方式透過共享鎖定（Shared Lock）和排他鎖定（Exclusive Lock）來運作，確保任何時候資料都只會被一個交易所讀取或更新。這種方式還會用到範圍鎖定（Range\nLock）來預防幻讀。大多數傳統的資料庫系統都是使用這種實作方式。\n可串行化快照隔離（Serializable Snapshot Isolation, SSI）： SSI 與其他方法不同，它使用的是樂觀的並行控制（Optimistic Concurrency\nControl）。這種策略假設交易之間發生衝突的可能性很低，因此不會在交易過程中就進行鎖定，而是允許交易正常進行，直到最後提交（Commit）階段才檢查是否有衝突發生。這種方法對效能的影響較小，但由於它是在2008年才被提出，所以許多資料庫還沒有採用。不過，PostgreSQL\n從9.1版本開始就採用了 SSI。\n總結來說，Serializable 隔離級別可以提供非常強的資料一致性保證，但可能會對效能造成影響。在選擇隔離級別時，您需要在資料一致性和系統效能之間做出權衡。\n小結 如何預防不同的「競爭條件」（Race Conditions）\n競爭條件是指多個交易同時操作相同的資料時可能會導致資料不一致的問題。\n不同的隔離級別能預防不同類型的競爭條件，例如「未提交讀」（Read Uncommitted）、「已提交讀」（Read Committed）、「可重複讀」（Repeatable\nRead）和「串行化」（Serializable），可參考下列表格。\n其中， Repeatable Read 的隔離級別在不同的資料庫系統中行為可能會有所不同。因為各個資料庫系統在實作 Repeatable Read\n時，採用了不同的技術和方法來實現這個隔離級別。\n因此，當開發者或資料庫管理員選擇使用「可重複讀」隔離級別時，需要仔細閱讀該資料庫的官方文堅，了解該資料庫的具體實作機制，以確定該資料庫能預防哪些類型的競爭條件。只有深入了解了這些細節，才能夠確保資料庫交易的正確性和一致性，並避免可能的資料問題。\n","description":"","id":2,"section":"posts","tags":["db"],"title":"再複習一次 Isolation Level","uri":"https://blog.jjmengze.website/zh-tw/posts/db/isolation-level/"},{"content":" 今年下半年從小道消息得知前公司營運以及財務狀況及急轉直下，在解散前的n個月我就先執行轉換跑道的計畫(面試準備期)，最終也取得我認為還滿棒公司的認可。\n這邊先跟有在追蹤我的讀者們說聲抱歉，原因出於下半年的職涯轉換需要大量的時間準備以及新工作上任需要了解公司內部的分工、架構、流程⋯⋯等等，這段期間沒辦法系統性的整理閱讀到技術、文章、source code，期許明年有更多的時間可以挖掘開源的相關技術。\n接下來就簡單地回顧一下2021吧！\n回顧 最印象深刻的事情是 2022 相對2021年來說，似乎在股市更好賺錢，前期稍微跟上這一波賺熱錢的風潮忘了投資理財的初衷，幾次當沖吃到了甜頭，膽子大了以為自己技術到了有足夠的能力分析局勢，中後期狠狠的被股票教訓一番。\nblog 經營\nblog 大概寫了二十四篇文章，大多數記錄 kubernetes source tracing 的心得以及 kubernetes 元件的核心思想，仍然覺得寫得不到位，無法把所想的完整透過文字表達出來。 推薦 hwchiu 的每日技術文章(矽谷牛的耕田筆記) 每天可以讀不同面向的文章，感謝大大的堅持不懈。 參與社群\n2021年初到年終這段時間，疫情以及職涯轉換跑道兩大因素參加社群的時間比去年又減少了不少，反而回去讀了不少資料結構以及演算法 主要參與了三個社群 SRE TW 線上 meetup CNTUG 線上 meetup Golang TW 線上 meetup 運動\n一週三練，雖然有時候偷懶兩練，從inbody中可以看到骨骼肌明顯的增長，體指卻沒有明顯的下降\u0026hellip;沒有控制飲食想吃就吃，還是一些垃圾食物xDDD 煮飯\n開發了一道菜，真的普通但料理速度極快 煎牛排 （料理速度極快，懶得想要吃什麼時候牛排直接煎下去就對了！） 線上課程\n上了一門多益線上課程，一個禮拜大概花了五到六個小時上課（新的課程加複習） 主要是看 Cindy老師的教學影片，內容大多數是單字以及考試用的詞彙，心得是我單字量有變多但是比較針對考試非日常的口說與寫作。 上課之前我有做過模擬測驗大概五百八十分左右，上完課再做其他的模擬試題大概進步到六百七十，多多少少有幫助xDD 通勤時間勁量保持英聽的習慣，cindy 老師的新聞簡報 的或是 好想講英文 都是滿不錯的題材 最後如果讀者有養貓的我想請問:\n如何讓貓咪學會刷牙，已經嘗試過醫師建議 花了九個月左右的時間讓貓咪習慣牙刷碰臉 到開口放入牙刷這一步完全吃閉門羹Orz 主要不想讓貓貓只為了洗牙而全身麻醉，可憐的貓貓 展望 2022 年投資理財更需要根據技術線圖說話，不盲目跟從不盲目當沖。\n產出有廣度有深度的技術文章，持續閱讀各式開源 source code 透過 blog 方式記錄其專案的核心思想，透過此方式更加貼近 cloud native 與 linux kernel 的世界。 持續加強英文聽說能力，目前一週大概有 3 hr 可以練習聽說的技能，期望未來的一年每一週能在該技能花上 6 hr 靜下心好好的跟讀與聽懂新聞、 ted 演講、影集的日常對話。 保持運動的好習慣，並且減少吃垃圾食物的次數。 保持閱讀的習慣，不論是雜書或是論文，訓練自己的耐心。 小結 這個世界是殘酷⋯⋯而且殘酷的非常的美好⋯⋯，生活工作健康不論什麼對於人生來說都是一場試煉，翻過了這個山頭回頭看這些都是成就我的一部分。\n但願 2022 年個人能力有明顯的增強，同時能提供周遭的人事物更多的幫助。\n","description":"","id":3,"section":"posts","tags":["life"],"title":"2021回顧總結","uri":"https://blog.jjmengze.website/zh-tw/posts/record/2021/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.21 版本（終於版更了，這部分跟 1.19 沒什麼差別），所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n本篇文章將介紹 kubelet 如何 cache pod status 有助於我們後續在拆解 kubelet 的部分功能，cache 的實作也相當的簡單，如果對於 kubernetes 其他 cache 也感興趣的朋友歡迎看看筆者之前寫的文章Kubernetes util tool 使用 cache LRU 定格一瞬間、Kubernetes Indexers local cache 之美 （I）、Kubernetes Indexers local cache 之美 （II），其中包含 kubernetes 使用 cache 的各式情境。\ninterface Cache interface 定義了兩種獲取 PodStatus 的方法：一種是非阻塞的 Get()另外一種為阻塞 GetNewerThan() 方法，分別用於不同的情境。\n另外也定義了 Set 以及 Delete 的方法用來新增 pod status 以及刪除對應的 pod status 以及透過 UpdateTime 確保 cache 的新鮮度，我們就來看看 interface 實際上對應到的 code 。\n1 2 3 4 5 6 7 8 9 type Cache interface { Get(types.UID) (*PodStatus, error) Set(types.UID, *PodStatus, error, time.Time) // GetNewerThan is a blocking call that only returns the status // when it is newer than the given time. GetNewerThan(types.UID, time.Time) (*PodStatus, error) Delete(types.UID) UpdateTime(time.Time) } 了解 cache 定義的 interface 後就來看看哪個物件實作 interface 囉。\nstruct cache struct 物件實作了 cache interface 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 type cache struct { lock sync.RWMutex //確保資料的一致性讀多寫少，使用rw鎖 pods map[types.UID]*data //使用 map 儲存 key 為 types.UID value為 pod status data timestamp *time.Time //透過權域的timestamp來確認資料的新鮮度 subscribers map[types.UID][]*subRecord //透過 map 紀錄哪一個 type.UID 目前是誰監聽 } //data 資料結構儲存 pod status type data struct { status *PodStatus //儲存 pod status // Error got when trying to inspect the pod. err error //儲存 pod inspect 錯誤 modified time.Time //上一次 pod 被修改的時間 } //透過 subRecord 可以回傳給監聽者 type subRecord struct { time time.Time ch chan *data //透過 channel 回傳 pod status } 了解了資料結構後，我們來看一下怎麼把 cache 建立起來。\nNew function 1 2 3 4 5 // NewCache creates a pod cache. func NewCache() Cache { //簡單的初始化 cache 會用到的 map return \u0026amp;cache{pods: map[types.UID]*data{}, subscribers: map[types.UID][]*subRecord{}} } 接著了解一下 cache 物件如何實作 cache interface 的需要吧！\nimpliment Set 會設置 Pod 的 PodStatus 到 cache 中\nset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 func (c *cache) Set(id types.UID, status *PodStatus, err error, timestamp time.Time) { //防止競爭上鎖 c.lock.Lock() defer c.lock.Unlock() //最後通知所有的訂閱者哪個 types.UID 發生了變化 defer c.notify(id, timestamp) //設定 cache 對應的資料，透過 types.UID 對應到 PodStatus c.pods[id] = \u0026amp;data{status: status, err: err, modified: timestamp} } // 如果滿足要求，則通知為具有給定 id 的 pod 發送通知。請注意，調用者應該獲取鎖。 func (c *cache) notify(id types.UID, timestamp time.Time) { //取得監聽某個 types.UID 的 subRecord slice 物件 //slice 的長度表示有多人在監聽這個 types.UID list, ok := c.subscribers[id] //如果在 map 中找不到對應的 subRecord slice 物件表示沒有人在監聽 if !ok { // No one to notify. return } newList := []*subRecord{} //取出 subRecord slice 的每個元素（subRecord） for i, r := range list { //如果該 subRecord 要的資料在 cache 內是不新鮮（透過 timestamp 進行比對。當前 cache 的 timestamp 太舊、subRecord 想要的 timestamp 較新） if timestamp.Before(r.time) { // 那些追蹤較新資料的追蹤者需要保留起來，下次 cache 更新 timestamp 與資料的時候可以通知那些追蹤者。 newList = append(newList, list[i]) continue } //反之 subRecord 追蹤的元素對於 cache 來說是新鮮的資料（透過 timestamp 進行比對。當前 cache 的 timestamp 比 subRecord 想要的 timestamp 要新），就透過 channel 回傳 r.ch \u0026lt;- c.get(id) //關閉channel close(r.ch) } //檢查 newlist 長度，若長度為零代表沒有遺留追蹤較新資料的觀察者，可以把觀察這筆資料的所有觀察者清除。 //反之更新 追蹤較新資料的觀察者到追蹤特定 types.UID 到 map 中 if len(newList) == 0 { delete(c.subscribers, id) } else { c.subscribers[id] = newList } } get 輸入 type id 取的某一個 pod status 當前狀態 ，要注意這是一個 nonblock 操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func (c *cache) Get(id types.UID) (*PodStatus, error) { //防止競爭上鎖 c.lock.RLock() defer c.lock.RUnlock() //透過 types.UID 取得對應的 pod status d := c.get(id) return d.status, d.err } func (c *cache) get(id types.UID) *data { //從 map 中透過 types.UID 找到對應的資料 d, ok := c.pods[id] //如果沒有找到的話就建立一個預設的資料回傳\tif !ok { return makeDefaultData(id) } //如果有找到直接回傳 map 中對應的資料\treturn d } //建立一個空資料的 pod status func makeDefaultData(id types.UID) *data { return \u0026amp;data{status: \u0026amp;PodStatus{ID: id}, err: nil} } GetNewerThan 透過 GetNewerThan function 我們可以傳入 types.UID 以及 minTime (用來對比新鮮度)可以得到對應的 pod status ，要注意它是一個 block 操作(直到取的 pod status 為止)。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 func (c *cache) GetNewerThan(id types.UID, minTime time.Time) (*PodStatus, error) { //得到某一個 types.UID 對應的資料變化的 channel ch := c.subscribe(id, minTime) //從 channel 取得資料變化 d := \u0026lt;-ch //回傳資料狀態，以及錯誤訊息 return d.status, d.err } func (c *cache) subscribe(id types.UID, timestamp time.Time) chan *data { //建立 channel 以供後續傳輸資料變化 ch := make(chan *data, 1) //防止競爭上鎖 c.lock.Lock() defer c.lock.Unlock() //透過 id 以及 timestamp 查詢 cache 中對應的資料 ，若當時 cache 的新限度滿足客戶端的需求，就能早到資料 d := c.getIfNewerThan(id, timestamp) //如果找得到資料就回傳給 channel if d != nil { ch \u0026lt;- d return ch } // 找不到資料就會加入 cache 某一 types.UID 的 subscribers 行列，等到有資料後可以透過 channel 通知使用者。 c.subscribers[id] = append(c.subscribers[id], \u0026amp;subRecord{time: timestamp, ch: ch}) //回傳channel以供後續資料傳輸使用 return ch } func (c *cache) getIfNewerThan(id types.UID, minTime time.Time) *data { //透過 type uid 從 map 取處得對應的 pod status d, ok := c.pods[id] //透過 timestamp 檢查 cache 是否準備好提供服務，以及透過 cache timestamp 對比輸入物件的 timestamp 檢查當前的 cache 新鮮度 //情境1:假設沒有 cache 沒有設定timestamp就代表無法比對新鮮度。globalTimestampIsNewer 為 false //情境2:假設 cache 最後一次捕捉到的資料是 10:00 的資料，使用者要求 11:00 的資料，表示 cache 不新鮮。globalTimestampIsNewer 為 false //情境3:假設 cache 最後一次捕捉到的資料是 11:00 的資料，使用者要求 10:00 的資料，表示 cache 目前儲存的資料是新鮮的。globalTimestampIsNewer 為 true globalTimestampIsNewer := (c.timestamp != nil \u0026amp;\u0026amp; c.timestamp.After(minTime)) //判斷 cache 中是否有對應的 pod 以及 cache 新不新鮮 //如果 cache 沒有對應的 pod 但是 cache 的保存資料是新鮮的 ，就回傳一個 default 的資料 if !ok \u0026amp;\u0026amp; globalTimestampIsNewer { return makeDefaultData(id) } //如果 cache 有對應的 pod 同時 cache 保存的新鮮度是足夠的話就回傳資料 if ok \u0026amp;\u0026amp; (d.modified.After(minTime) || globalTimestampIsNewer) { return d } //cache 不存在以及 cache 保存的不夠新鮮度回傳 nil return nil } Delete 某個 podstatus 已經不需要了，所以透過 delete function 清理掉。\n1 2 3 4 5 6 7 func (c *cache) Delete(id types.UID) { //防止競爭加鎖 c.lock.Lock() defer c.lock.Unlock() //透過 types.UID 刪除 map 中對應的資料 delete(c.pods, id) } UpdateTime 更改 cache 的 timestamp 並通知所有的訂閱者，可以更新 pod status 給訂閱者。\n1 2 3 4 5 6 7 8 9 func (c *cache) UpdateTime(timestamp time.Time) { c.lock.Lock() defer c.lock.Unlock() c.timestamp = \u0026amp;timestamp // Notify all the subscribers if the condition is met. for id := range c.subscribers { c.notify(id, *c.timestamp) } } 小結 kubelet 使用 cache 來儲存 pod status 的狀態，並且透過 timestamp 確保 cache 的新鮮度，若是 cache 更新 timestamp 也會通知 subscribers 訂閱的 pod status 當前的狀態。\n此外也提供兩種獲取 PodStatus 的方法：一種是非阻塞的 Get()另外一種為阻塞 GetNewerThan() 方法，分別用於不同的情境。\n本篇文章雖然篇幅不長知識量也不大，但作為後續分析系統每個元件都是重要的一份子呢xD，如果文中有錯希望大家不吝嗇提出，讓我們互相交流學習。\n","description":"","id":4,"section":"posts","tags":["kubernetes","source-code"],"title":"Kubernetes kubelet cache pod status 怎麼一回事","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/kubelet/cache/pod_status_cache/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n本篇文章將介紹隱藏在 kubelet 的 WorkQueue ，後續在拆解 kubelet 的部分功能會用到 WorkQueue ，該 Worker Queue 僅為簡單的 queue 實作，如果對 kubernetes 其他較為複雜的 Queue 實作有興趣的朋友歡迎參考筆者早期的三篇文章分別是Kubernetes RateLimite work queue 設計真d不錯、Kubernetes delaying work queue 設計真d不錯以及Kubernetes common work queue 設計真d不錯。\nWorkQueue WorkQueue interface 定義一系列的方法，實作的物件需要透過時間戳 timestamp 記對 type.UID 進行排隊，並且可以取的目前在 Queue 中的所有 work 。\n1 2 3 4 5 6 type WorkQueue interface { // 回傳所有準備好的 type.UID 。 GetWork() []types.UID // 插入新 types.UID 到 queue 中 Enqueue(item types.UID, delay time.Duration) } struct basicWorkQueue 實作了 WorkQueue 我們來看一下他的資料結構。\n1 2 3 4 5 6 7 var _ WorkQueue = \u0026amp;basicWorkQueue{} type basicWorkQueue struct { clock clock.Clock\t//用來記錄當前時間 lock sync.Mutex\t//避免map中的物件競爭 queue map[types.UID]time.Time\t//用來儲存所有工作以及延遲時間 } New function 也非常簡單，簡單的建立 map 以及透過使用者傳入 clock 當作 WorkQueue 當前的時間。\n1 2 3 4 5 // NewBasicWorkQueue returns a new basic WorkQueue with the provided clock func NewBasicWorkQueue(clock clock.Clock) WorkQueue { queue := make(map[types.UID]time.Time) return \u0026amp;basicWorkQueue{queue: queue, clock: clock} } implement 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func (q *basicWorkQueue) GetWork() []types.UID { //防止競爭加鎖 q.lock.Lock() defer q.lock.Unlock() //取得目前時間 now := q.clock.Now() var items []types.UID //遞迴取得每個 type UID for k, v := range q.queue { //判斷每個 type UID 延遲時間是否已經到了，如果延遲時間到了從 map 中移除並且加入已經延遲時間已到的 slice 中。 if v.Before(now) { items = append(items, k) delete(q.queue, k) } } //回傳已延遲完的工作 return items } func (q *basicWorkQueue) Enqueue(item types.UID, delay time.Duration) { //防止競爭加鎖 q.lock.Lock() defer q.lock.Unlock() // type UID 設定要延遲多久才開始工作 q.queue[item] = q.clock.Now().Add(delay) } test case 篇幅感覺太短了xD，來介紹一下這個 function 的測試案例。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 func TestGetWork(t *testing.T) { //建立 basicWorkQueue 以及當假的 clock q, clock := newTestBasicWorkQueue() //queue加入 foo1 延遲時間 -1 分鐘 q.Enqueue(types.UID(\u0026#34;foo1\u0026#34;), -1*time.Minute) //queue加入 foo2 延遲時間 -1 分鐘 q.Enqueue(types.UID(\u0026#34;foo2\u0026#34;), -1*time.Minute) //queue加入 foo3 延遲時間 1 分鐘 q.Enqueue(types.UID(\u0026#34;foo3\u0026#34;), 1*time.Minute) //queue加入 foo4 延遲時間 - 分鐘 q.Enqueue(types.UID(\u0026#34;foo4\u0026#34;), 1*time.Minute) //預期取的可以從 queue 中拿到 \u0026#34;foo1\u0026#34; \u0026#34;foo2\u0026#34; expected := []types.UID{types.UID(\u0026#34;foo1\u0026#34;), types.UID(\u0026#34;foo2\u0026#34;)} //比對預期得結果與實際的結果\t， GetWork 從 queue 中拿資料，會取的延遲時間到的資料 //因為 foo1 foo2 延遲時間為 -1 分鐘，所以 getwork 可以拿到 foo1 foo2 的資料 compareResults(t, expected, q.GetWork()) //比對預期得結果與實際的結果\t，此時 foo1 foo2 已經從 queue 中 pop 出去了 //再去拿資料的時候由於其他資料還沒到延遲時間所以 get work 會是空的 compareResults(t, []types.UID{}, q.GetWork()) //將時間 mock 往後調整一個小時 clock.Step(time.Hour) //預期取的可以從 queue 中拿到 \u0026#34;foo3\u0026#34; \u0026#34;foo4\u0026#34; expected = []types.UID{types.UID(\u0026#34;foo3\u0026#34;), types.UID(\u0026#34;foo4\u0026#34;)} //此時 time 往後調整一個小時了 //比對預期得結果與實際的結果\t， GetWork 從 queue 中拿資料，會取的延遲時間到的資料 //因為 foo3 foo4 延遲時間為 1 分鐘，所以 getwork 可以拿到 foo3 foo4 的資料 compareResults(t, expected, q.GetWork()) //比對預期得結果與實際的結果\t，此時 foo3 foo4 已經從 queue 中 pop 出去了 //再去拿資料的時候queue已經空了所以 get work 時會拿到空的資料 compareResults(t, []types.UID{}, q.GetWork()) } //建立 queue 以及假的 clock (可以 mock 用) func newTestBasicWorkQueue() (*basicWorkQueue, *clock.FakeClock) { //建立假的 clock 可以 mock 用 fakeClock := clock.NewFakeClock(time.Now()) //建立 basicWorkQueue 物件設定 fake clock wq := \u0026amp;basicWorkQueue{ clock: fakeClock, queue: make(map[types.UID]time.Time), } return wq, fakeClock } //比對預期得結果與實際的結果 func compareResults(t *testing.T, expected, actual []types.UID) { //建立一個 set expectedSet := sets.NewString() //將預期的答案放入 set for _, u := range expected { expectedSet.Insert(string(u)) } //用來儲存實際上queue回傳資料用的set actualSet := sets.NewString() //將實際上queue回傳資料放入 set for _, u := range actual { actualSet.Insert(string(u)) } //判斷 預期的答案 與實際的答案是否一致。 if !expectedSet.Equal(actualSet) { t.Errorf(\u0026#34;Expected %#v, got %#v\u0026#34;, expectedSet.List(), actualSet.List()) } } 小結 kubelet 中所採用的 WorkQueue 實作上非常的簡單，把需要延遲行為的 type UID 物件放入 queue 中，取出時再判斷 queue 中哪些延遲時間已經到達整成 slice ，再回傳給使用者。本篇文章雖然篇幅不長知識量也不大，但作為後續分析系統每個元件都是重要的一份子呢xD，如果文中有錯希望大家不吝嗇提出，讓我們互相交流學習。\n","description":"","id":5,"section":"posts","tags":["kubernetes","source-code"],"title":"隱藏在kubelet的WorkQueue","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/kubelet/queue/work-queue/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n希望閱讀文章的朋友可以先去看前一篇Kubernetes kubelet 探測 pod 的生命症狀探針得如何出生-1\n，了解一下前後文的關係比較好理解本篇要說明的重點。\n另外想要了解kubernetes probe 有哪些型態以及底層是如何實作的可以參考以下三篇文章Kubernetes kubelet 探測 pod 的生命症狀 Http Get，Kubernetes kubelet 探測 pod 的生命症狀 tcp socket以及Kubernetes kubelet 探測 pod 的生命症狀 Exec，可以從這三篇文章了解 kubernetes probe 如何提供 tcp socket、 exec 以及 http get 三種方法的基本操作。\n上一篇文章Kubernetes kubelet 探測 pod 的生命症狀探針得如何出生-1\n，livenessManager readinessManager startupManager 以及 probeManager 的物件是從哪裡生成的，用在什麼地方，以及作用在哪裡。因此本篇文章會將重點聚焦在這些物件之後的相關的調用鍊。\n這邊來回顧一下 proberesults.Manager 是怎麼產生的\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // Kubelet is the main kubelet implementation. type Kubelet struct //用來控制哪些 pod 要加入到 probe manager // Handles container probing. probeManager prober.Manager // 用來探測 container liveness 、 readiness 、 startup probe 結果儲存 // Manages container health check results. livenessManager proberesults.Manager readinessManager proberesults.Manager startupManager proberesults.Manager ... } func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...)(*Kubelet, error) { ... //在這裡透過 proberesults.NewManager() 生成 並且設定 kubelet 的 livenessManager readinessManager startupManager，用來儲存 container 探測 liveness 、 readiness 、 startup prob 的結果 //後面會繼續解析 proberesults.NewManager 得實作 klet.livenessManager = proberesults.NewManager() klet.readinessManager = proberesults.NewManager() klet.startupManager = proberesults.NewManager() ... // kubelet 的 probemanager 則是需要組合上面提到的三種 manager 以及 runner 與 recorder 透過 prober.NewManager 新增對應的物件，用以用來控制哪些 pod 要加入到 prob manager。 //後面會繼續解析 prober.NewManager 得實作 klet.probeManager = prober.NewManager( klet.statusManager, klet.livenessManager, klet.readinessManager, klet.startupManager, klet.runner, kubeDeps.Recorder) ... } 回顧完 proberesults.Manager 是怎麼產生的，我們就接著來了解一下上篇未講完的內容吧。\n上一個章節中我們有談到 proberesults.Manager 是一個 interface 他定義了許多方法ref，我們在kubelet 的 syncLoopIteration 階段可以發現這邊會呼叫 proberesults.Manager 的 update function 等待 channel 把資料送來ref，就沒有看到其他地方有呼叫 proberesults.Manager 所定義的其他方法了。\n我們循著線索找到 prober.Manager 是由 三種 manager 分別是 container liveness 、 readiness 、 startup 組合而成的（他們的型態都為 proberesults.Manager ）。\n所以說 proberesults.Manager 定義的其他 function 會在 prober.Manager 裡面被調用囉？是沒錯！所以我們需要進一步的來分析 prober.Manager 是什麼。\nprober.Manager 我們了解完 livenessManager 、 readinessManager 以及 startupManager 之後，需要進一分析三個組再一起變成的 probeManager 到底是什麼東西，我們先來看他的型態 prober.Manager 。\ninterface source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type Manager interface { // 為每個 container probe 創建新的 probe worker，在建立 pod 時呼叫。 AddPod(pod *v1.Pod) // 為刪除已存在的 container probe worker，在移除 pod 時呼叫。 RemovePod(pod *v1.Pod) // CleanupPods handles cleaning up pods which should no longer be running. // It takes a map of \u0026#34;desired pods\u0026#34; which should not be cleaned up. CleanupPods(desiredPods map[types.UID]sets.Empty) // UpdatePodStatus modifies the given PodStatus with the appropriate Ready state for each // container based on container running status, cached probe results and worker states. UpdatePodStatus(types.UID, *v1.PodStatus) } struct 了解完 prober.Manager 所定義的 function 之後我們要來看實作的物件是誰囉！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 type manager struct { // 用來記錄哪個 container 對應哪一個 probe worker workers map[probeKey]*worker // 防止存取 worker 競爭 workerLock sync.RWMutex // 提供 pod IP 和 container ID statusManager status.Manager // 用來存取 readiness probe 的結果 readinessManager results.Manager // 用來存取 liveness probe 的結果 livenessManager results.Manager // 用來存取 startup probe 的結果 startupManager results.Manager // probe 實作層，可以參考[Kubernetes kubelet 探測 pod 的生命症狀 Http Get](https://blog.jjmengze.website/posts/kubernetes/source-code/kubelet/prob/kubernetes-kubelet-http-get/)，[Kubernetes kubelet 探測 pod 的生命症狀 tcp socket](https://blog.jjmengze.website/posts/kubernetes/source-code/kubelet/prob/kubernetes-kubelet-tcp-socket/)以及[Kubernetes kubelet 探測 pod 的生命症狀 Exec](https://blog.jjmengze.website/posts/kubernetes/source-code/kubelet/prob/kubernetes-kubelet-exec/) prober *prober start time.Time } // probekey 包含 probe ，pod id 、 container name 、 以及目前在 probe 型態，用來識別 worker ，也可以視為 worker 唯一辨別方式。 type probeKey struct { podUID types.UID containerName string probeType probeType } //這個部分可以回顧[Kubernetes kubelet 探測 pod 的生命症狀 Http Get](https://blog.jjmengze.website/posts/kubernetes/source-code/kubelet/prob/kubernetes-kubelet-http-get/)，[Kubernetes kubelet 探測 pod 的生命症狀 tcp socket](https://blog.jjmengze.website/posts/kubernetes/source-code/kubelet/prob/kubernetes-kubelet-tcp-socket/)以及[Kubernetes kubelet 探測 pod 的生命症狀 Exec](https://blog.jjmengze.website/posts/kubernetes/source-code/kubelet/prob/kubernetes-kubelet-exec/) type prober struct { //注入 probe exec 實作對象 exec execprobe.Prober //注入 http get 實作對象 readinessHTTP httpprobe.Prober livenessHTTP httpprobe.Prober startupHTTP httpprobe.Prober //注入 tcp socket probe 實作對象 tcp tcpprobe.Prober //注入 CRI 控制器用來對 container 下達命令 runner kubecontainer.CommandRunner //注入事件紀錄收集器 recorder record.EventRecorder } New function 看完了資料結構是如何定義後我們來看怎麼把這個物件建立起來。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func NewManager( statusManager status.Manager, livenessManager results.Manager, readinessManager results.Manager, startupManager results.Manager, runner kubecontainer.CommandRunner, recorder record.EventRecorder) Manager { //建立實際上各種執行 probe 的物件（Http get、Tcp socket、Exec） prober := newProber(runner, recorder) //剩下的是把輸入的物件進行簡單的組合 return \u0026amp;manager{ statusManager: statusManager, prober: prober, readinessManager: readinessManager, livenessManager: livenessManager, startupManager: startupManager, workers: make(map[probeKey]*worker), start: clock.RealClock{}.Now(), } } AddPod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 //當 kubelet 接收到有 pod 到加入到這個節點時，會觸發 HandlePodAdditions function ，並且傳入有哪些 pod 要加入。 func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) { ... // for 迴圈地回所有要加入的 pod for _, pod := range pods { ... //加入到 probeManager 中 kl.probeManager.AddPod(pod) ... } } 處理 pod 是否需要 probe ，如果有需要則建立對應的 probe worker （worker 後面我們會看到是什麼，這邊先知道有這個事情發生即可）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 func (m *manager) AddPod(pod *v1.Pod) { //確保 map 一致性上鎖 m.workerLock.Lock() defer m.workerLock.Unlock() //以傳入 pod uid 組合成 probeKey 物件 key := probeKey{podUID: pod.UID} //遞迴 pod spec container 欄位 for _, c := range pod.Spec.Containers { //設定 probeKey containerName 為 container name key.containerName = c.Name //如果 pod spec container 有設定 StartupProbe 的話 if c.StartupProbe != nil { //將 key probeType 設定成 startup key.probeType = startup //透過 worker map 檢查是否曾經處理過相同的 key ，如果有表示已經有 worker 在處理了，直接略過不理。 if _, ok := m.workers[key]; ok { klog.ErrorS(nil, \u0026#34;Startup probe already exists for container\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;containerName\u0026#34;, c.Name) return } //建立新的 worker，並且傳入 pod 資訊、 container 資訊以及 manager 本身，後續會看到 worker 到底是什麼這邊先了解有這個步驟就好。 w := newWorker(m, startup, pod, c) //將 key 與對應的 worker 設定到 map 中 m.workers[key] = w //啟動 worker go w.run() } //如果 pod spec container 有設定 ReadinessProbe 的話 if c.ReadinessProbe != nil { //將 key probeType 設定成 readiness key.probeType = readiness //透過 map 檢查是否曾經處理過相同的 key ，如果有表示已經處理過了。不再處理。 if _, ok := m.workers[key]; ok { klog.ErrorS(nil, \u0026#34;Readiness probe already exists for container\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;containerName\u0026#34;, c.Name) return } //建立新的 worker並且傳入 pod 資訊、 container 資訊以及 manager 本身，後續會看到 worker 到底是什麼這邊先了解有這個步驟就好。 w := newWorker(m, readiness, pod, c) //將 key 與對應的 worker 設定到 map 中 m.workers[key] = w //啟動 worker go w.run() } //如果 pod spec container 有設定 LivenessProbe 的話 if c.LivenessProbe != nil { //將 key probeType 設定成 readiness key.probeType = liveness //透過 map 檢查是否曾經處理過相同的 key ，如果有表示已經處理過了。不再處理。 if _, ok := m.workers[key]; ok { klog.ErrorS(nil, \u0026#34;Liveness probe already exists for container\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;containerName\u0026#34;, c.Name) return } //建立新的 worker並且傳入 pod 資訊、 container 資訊以及 manager 本身，後續會看到 worker 到底是什麼這邊先了解有這個步驟就好。 w := newWorker(m, liveness, pod, c) //將 key 與對應的 worker 設定到 map 中 m.workers[key] = w //啟動 worker go w.run() } } } RemovePod 1 2 3 4 5 6 7 8 // 當 kubelet 接收到有哪些 pod 要從這個節點移除，會觸發 HandlePodRemoves function ，並且傳入有哪些 pod 要從節點移除。 func (kl *Kubelet) HandlePodRemoves(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { kl.probeManager.RemovePod(pod) } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 //當 pod 被移除的時候會觸發 func (m *manager) RemovePod(pod *v1.Pod) { //確保 map 一致性上鎖 m.workerLock.RLock() defer m.workerLock.RUnlock() //以 pod uid 組合成 probeKey 物件 key := probeKey{podUID: pod.UID} //遞迴 pod spec container 欄位 for _, c := range pod.Spec.Containers { //設定 probeKey containerName 為 container name key.containerName = c.Name //因為需要找到 map 裡面 key 對應 worker，現在 probeKey 物件已經有 podUID containerName 現在還缺少 probeType //我們需要用 for 迴圈跑 readiness, liveness, startup 組合 probeKey 物件物件 //再透過組合出來的 probeKey 從 map 中找找看有沒有對應的 worker for _, probeType := range [...]probeType{readiness, liveness, startup} { //設定 probeType 為 readiness 或是 liveness 或是 startup key.probeType = probeType //再透過組合出來的 probeKey 從 map 中找找看有沒有對應的 worker，接著關閉 worker， 後續會看到 worker 到底是什麼這邊先了解有這個步驟就好。 if worker, ok := m.workers[key]; ok { worker.stop() } } } } UpdatePodStatus 根據 container 運行狀態、probe 結果，針對每個 container 進行適當狀態修改。\n1 2 3 4 5 6 7 8 9 10 11 12 // 給定 pod 狀態，為 pod 建立最終的 API pod 狀態。這裡的狀態可以想像成 pod spec 最後的 pod status 欄位。 func (kl *Kubelet) generateAPIPodStatus(pod *v1.Pod, podStatus *kubecontainer.PodStatus) v1.PodStatus { ... spec := \u0026amp;pod.Spec ... //透過pod status kl.probeManager.UpdatePodStatus(pod.UID, podStatus) ... return *s } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 func (m *manager) UpdatePodStatus(podUID types.UID, podStatus *v1.PodStatus) { //透過傳入的 pod status 遞迴所有的 ContainerStatuses for i, c := range podStatus.ContainerStatuses { var started bool //看看 container 是否已經開始執行，已經開始執行的話需要判斷 startup probe 是否成功。 if c.State.Running == nil { //如果還沒開始執行設定為 false started = false // 傳入的 container id 透過 kubecontainer.ParseContainerID function 轉成 containerId 物件 // 透過 startupManager get function 傳入 containerId 物件得到 startup 探測結果。 } else if result, ok := m.startupManager.Get(kubecontainer.ParseContainerID(c.ContainerID)); ok { // 如果 startup 探測成功的話就設定 started 為 true started = result == results.Success } else { // 透過 pod id 、container name 與 startup 從 getWorker 拿到 worker _, exists := m.getWorker(podUID, c.Name, startup) //如果找不到 worker ，就當作探測成功，因為沒有 startup worker started = !exists } //依照幾種情況來修正 container 狀態 //State.Runnin 判斷 container 是否啟動 //startupManager.Get 探測 container startup 狀態 //getWorker 判斷是否有 worker ，若沒有worker 表示沒有 startup probe //依照上述情況設定設定 Container 的 Statuses podStatus.ContainerStatuses[i].Started = \u0026amp;started // 若是確認 container 已經啟動 if started { var ready bool // 再次確認 container 是否有啟動 if c.State.Running == nil { ready = false // 如果 container 有啟動繼續透過 傳入的 container id 透過 kubecontainer.ParseContainerID function 轉成 containerId 物件 // readinessManager.Get 傳入 containerId 物件得到 readiness 探測結果。 } else if result, ok := m.readinessManager.Get(kubecontainer.ParseContainerID(c.ContainerID)); ok { // 如果 readiness 探測成功的話就設定 started 為 true ready = result == results.Success } else { // 透過 pod id 、container name 與 readiness 從 getWorker 拿到 readiness worker w, exists := m.getWorker(podUID, c.Name, readiness) //如果找不到 worker ，就當作探測 readinessProbe 成功 ready = !exists // no readinessProbe -\u0026gt; always ready //如果有找到我們需要進一步判斷 worker 狀態 if exists { // 手動觸發探測下次就可以得知結果 select { case w.manualTriggerCh \u0026lt;- struct{}{}: default: // Non-blocking. klog.InfoS(\u0026#34;Failed to trigger a manual run\u0026#34;, \u0026#34;probe\u0026#34;, w.probeType.String()) } } } //依照幾種情況來修正 container 狀態 //State.Runnin 判斷 container 是否啟動 //readinessManager.Get 探測 container readiness 狀態 //getWorker 判斷是否有 worker //依照上述情況設定設定 Container 的 Statuses podStatus.ContainerStatuses[i].Ready = ready } } // 如果 init container 為成功退出或就把 init container status ready 設定為成功。 for i, c := range podStatus.InitContainerStatuses { //預設 ready 為 false var ready bool //如果 init container 狀態為 Terminated 並且退出狀態碼為 0 ，就把 ready 設定為 true if c.State.Terminated != nil \u0026amp;\u0026amp; c.State.Terminated.ExitCode == 0 { ready = true } // init container status ready 設定為成功。 podStatus.InitContainerStatuses[i].Ready = ready } } CleanupPods 執行一系列清理工作，包括終止 pod worker、殺死不需要的 pod 以及刪除 orphaned 的volume/pod directories。\n1 2 3 4 5 6 7 8 9 10 // NOTE: This function is executed by the main sync loop, so it // should not contain any blocking calls. func (kl *Kubelet) HandlePodCleanups() error { ... //這裡有點複雜我們先當作 desiredPods 就是那些已經不存在的 pod 就好 //Stop the workers for no-longer existing pods. kl.probeManager.CleanupPods(desiredPods) ... return nil } 1 2 3 4 5 6 7 8 9 10 11 func (m *manager) CleanupPods(desiredPods map[types.UID]sets.Empty) { //確保 map 一致性上鎖 m.workerLock.RLock() defer m.workerLock.RUnlock() //遞迴 manager 所有 worker 找到 預期要消失的 pod uid ，透過 worker stop 結束 probe 作業。 for key, worker := range m.workers { if _, ok := desiredPods[key.podUID]; !ok { worker.stop() } } } support 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 //透過 getworker 我們可以傳入 pod uid 、 container name 以及 probeType 獲得 probe worker func (m *manager) getWorker(podUID types.UID, containerName string, probeType probeType) (*worker, bool) { //確保 map 一致性上鎖 m.workerLock.RLock() defer m.workerLock.RUnlock() //透過 pod uid 以及 container name 還有 probetype 組合成 probeKey //再從 map 找到對應的 worker worker, ok := m.workers[probeKey{podUID, containerName, probeType}] return worker, ok } // 當不再需要某一個 probe worker 時候我們可以呼叫 removeWorker 傳入 pod uid 、 container name 以及 probeType 刪除 probe worker。 func (m *manager) removeWorker(podUID types.UID, containerName string, probeType probeType) { //確保 map 一致性上鎖 m.workerLock.Lock() defer m.workerLock.Unlock() //透過 pod uid 以及 container name 還有 probetype 組合成 probeKey //再從 map 刪除對應的 worker delete(m.workers, probeKey{podUID, containerName, probeType}) } // workerCount 返回 probe worker 的總數，測試用。 func (m *manager) workerCount() int { //確保 map 一致性上鎖 m.workerLock.RLock() defer m.workerLock.RUnlock() //看看目前現在有多少的 worker return len(m.workers) } 整理一下 還記得之前為了找到在 proberesults.Manager 定義的卻沒被用到的這三個 function 嗎？我們現在來看看都在哪裡用上了吧！\n1 2 3 4 5 6 // 透過 container id 從 實作者身上得到 result 結果 Get(kubecontainer.ContainerID) (Result, bool) // 透過 container id 設定 pod 探測的結果。實作者需要把結果儲存起來。 Set(kubecontainer.ContainerID, Result, *v1.Pod) // 透過 container id 移除時實作者身上的對應的資料 Remove(kubecontainer.ContainerID) Get 透過 container id 從 實作者身上得到 result 結果\nUpdatePodStatus\n1 2 3 4 5 6 7 8 9 func (m *manager) UpdatePodStatus(podUID types.UID, podStatus *v1.PodStatus) { //透過傳入的 pod status 遞迴所有的 ContainerStatuses for i, c := range podStatus.ContainerStatuses { ... } else if result, ok := m.startupManager.Get(kubecontainer.ParseContainerID(c.ContainerID)); ok { // 如果 startup 探測成功的話就設定 started 為 true started = result == results.Success } 其他的好像都還沒用上，沒關係等等我們還會看到！\n接著來談談 prober.Manager 的實作者 manager 一直有用到的 probe worker 到底是什麼勒～\nworker 這裡的 worker 做的事情就是定期地去執行 probe 的工作，一個 worker 只會只能一種工作 startup 、 readliness 或是 liveness。\nstruct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 type worker struct { // 停止 worker 的 channel stopCh chan struct{} // 手動觸發 probe 的 channel manualTriggerCh chan struct{} // pod spec pod *v1.Pod // container spec container v1.Container // container probe spec spec *v1.Probe // worker 目前執行什麼種類的 probe（liveness, readiness or startup） probeType probeType // 一開始 worker 處於什麼狀態（Unknown、Success、Failure） initialValue results.Result // 用來儲存 worker probe 後的結果 resultsManager results.Manager //worker 會用到上層 manager 一些方法 probeManager *manager // worker 處理的 container id containerID kubecontainer.ContainerID // worker 的最後一次探測結果。 lastResult results.Result // worker probe 連續返回多少次相同的結果。 resultRun int // 如果有設定，worker在 probe 探測時會略過本次探測。 onHold bool // promethus metric 紀錄 probe metric 用 proberResultsSuccessfulMetricLabels metrics.Labels proberResultsFailedMetricLabels metrics.Labels proberResultsUnknownMetricLabels metrics.Labels } new function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 // 建立 work 去檢測 container 狀態 func newWorker( m *manager, probeType probeType, pod *v1.Pod, container v1.Container) *worker { //初始化 worker w := \u0026amp;worker{ //stop channel 呼叫的時候不會有 block （其實還是會拉xD），用來停止worker用 stopCh: make(chan struct{}, 1), //manualTriggerCh 呼叫的時候不會有 block （其實還是會拉xD），用來觸發 worker probe 用 manualTriggerCh: make(chan struct{}, 1), //傳入 pod spec pod: pod, //要 prob 的 container container: container, //要 prob 的型態，有可能是liveness, readiness or startup probeType: probeType, //傳入 manager 因為 worker 會操作 manger 物件 probeManager: m, } //判定 probeType switch probeType { //如果是 readiness //設定 worker spec 為 container 的 readiness 區塊 //設定 worker 的 resultsManager 為外部 manger 的 readinessManager ，用以做 readiness probe //設定初始狀態為 Failure ，因為 readiness 為 faill 的話就不會把 pod 加到服務（ service ）上。 case readiness: w.spec = container.ReadinessProbe w.resultsManager = m.readinessManager w.initialValue = results.Failure //如果是 liveness //設定 worker spec 為 container 的 LivenessProbe 區塊 //設定 worker 的 resultsManager 為外部 manger 的 livenessManager ，用以做 livenessManager probe //設定初始狀態為Success ，因為 liveness 為 Success 的話一開始就不會把 pod 刪掉/重啟。 case liveness: w.spec = container.LivenessProbe w.resultsManager = m.livenessManager w.initialValue = results.Success //如果是 startup //設定 worker spec 為 container 的 startup 區塊 //設定 worker 的 resultsManager 為外部 manger 的 startupManager ，用以做 startup probe //設定初始狀態為Unknown case startup: w.spec = container.StartupProbe w.resultsManager = m.startupManager w.initialValue = results.Unknown } //設定 promethus metric 結構 basicMetricLabels := metrics.Labels{ //worker 在 prob type \u0026#34;probe_type\u0026#34;: w.probeType.String(), //worker prob 的 container 對象 \u0026#34;container\u0026#34;: w.container.Name, //worker prob 的 pod 對象 \u0026#34;pod\u0026#34;: w.pod.Name, //worker prob 的 namespace \u0026#34;namespace\u0026#34;: w.pod.Namespace, //worker prob 的 pod id 對象 \u0026#34;pod_uid\u0026#34;: string(w.pod.UID), } //透過 deepCopyPrometheusLabels 把 basicMetricLabels 複製一份並且\t//設定 worker proberResultsSuccessfulMetricLabels //proberResultsSuccessfulMetricLabels 這邊 metric label 設定成 probeResultSuccessful w.proberResultsSuccessfulMetricLabels = deepCopyPrometheusLabels(basicMetricLabels) w.proberResultsSuccessfulMetricLabels[\u0026#34;result\u0026#34;] = probeResultSuccessful //透過 deepCopyPrometheusLabels 把 basicMetricLabels 複製一份並且\t//設定 worker proberResultsFailedMetricLabels //proberResultsFailedMetricLabels 這邊 metric label 設定成 probeResultSuccessful w.proberResultsFailedMetricLabels = deepCopyPrometheusLabels(basicMetricLabels) w.proberResultsFailedMetricLabels[\u0026#34;result\u0026#34;] = probeResultFailed //透過 deepCopyPrometheusLabels 把 basicMetricLabels 複製一份並且\t//設定 worker proberResultsUnknownMetricLabels //proberResultsUnknownMetricLabels 這邊 metric label 設定成 probeResultSuccessful w.proberResultsUnknownMetricLabels = deepCopyPrometheusLabels(basicMetricLabels) w.proberResultsUnknownMetricLabels[\u0026#34;result\u0026#34;] = probeResultUnknown //回傳worker return w } run worker 開始定時執行 probe 作業\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 // run periodically probes the container. func (w *worker) run() { //透過 pod spec 設定 probe 的間隔時間 probeTickerPeriod := time.Duration(w.spec.PeriodSeconds) * time.Second //我猜的...不確定為什麼會這樣設計，如果有知道的大大希望能不吝嗇告知 //依照現在的時間點減去 kubelet 的啟動時間，得到 kubelet 存活的時間。 //如果 PeriodSeconds 大於 kubelet 存活的時間的話，讓worker 睡一下等等在 probe 。 //我猜可能是因為kubelet 還沒完全準備好（吧？） if probeTickerPeriod \u0026gt; time.Since(w.probeManager.start) { time.Sleep(time.Duration(rand.Float64() * float64(probeTickerPeriod))) } //設定 ticker 多久要觸發一次 probeTicker := time.NewTicker(probeTickerPeriod) //clean up function defer func() { // 關閉ticker probeTicker.Stop() //如果 container id 還在的話就移除 resultsManager 跟這個 container 有關的資料 if !w.containerID.IsEmpty() { w.resultsManager.Remove(w.containerID) } //移除map中紀錄的 worker 資訊 w.probeManager.removeWorker(w.pod.UID, w.container.Name, w.probeType) //metric 不在記錄這個 worker 所發生的 metric ProberResults.Delete(w.proberResultsSuccessfulMetricLabels) ProberResults.Delete(w.proberResultsFailedMetricLabels) ProberResults.Delete(w.proberResultsUnknownMetricLabels) }() probeLoop: //每次 prob 完成後如果 probe 結果為 true ，需要等待 probeTicker ，或是 manualTriggerCh 再進行下一次的 probe trigger //如果是收到 stopCh 或是 probe 結果為 flase 則關閉 worker 。 for w.doProbe() { // Wait for next probe tick. select { case \u0026lt;-w.stopCh: break probeLoop case \u0026lt;-probeTicker.C: case \u0026lt;-w.manualTriggerCh: // continue } } } //用來關閉 worker，設計成 Non-blocking的，簡單來看就是往 stop channel 送 stop 訊號。 func (w *worker) stop() { select { case w.stopCh \u0026lt;- struct{}{}: default: // Non-blocking. } } // probe container 並且回傳 probe 結果。如果 probe 過程中有錯會回傳 false 呼叫者需要關閉 worker。 func (w *worker) doProbe() (keepGoing bool) { //無腦回復狀態 panic 。 defer func() { recover() }() // runtime.HandleCrash 紀錄一下而已。 defer runtime.HandleCrash(func(_ interface{}) { keepGoing = true }) //透過 statusManager 跟著 UID 拿到 pod status（statusManager 在這裡不太重要，知道可以拿到 pod status 就好了） status, ok := w.probeManager.statusManager.GetPodStatus(w.pod.UID) if !ok { // Pod 尚未創建，或者已被刪除。 klog.V(3).InfoS(\u0026#34;No status for pod\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(w.pod)) return true } // 如果 pod 處於 PodFailed 跟 PodSucceeded 狀態，這個 worker 就可以關閉了 if status.Phase == v1.PodFailed || status.Phase == v1.PodSucceeded { klog.V(3).InfoS(\u0026#34;Pod is terminated, exiting probe worker\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(w.pod), \u0026#34;phase\u0026#34;, status.Phase) return false } //遞迴 container status ，判斷所有的 container status 對應到輸入的 ccontainer name 回傳 status 狀態 //如果找不到對應的 container status 就等待下一輪 c, ok := podutil.GetContainerStatus(status.ContainerStatuses, w.container.Name) if !ok || len(c.ContainerID) == 0 { // 容器尚未創建，或者已被刪除。 klog.V(3).InfoS(\u0026#34;Probe target container not found\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(w.pod), \u0026#34;containerName\u0026#34;, w.container.Name) return true // Wait for more information. } //判斷 worker 負責的 container id 是不是跟 status 的 container 可以對上，如果對不上這種狀況可能發生在 container 被刪除或是 container 改變了。 if w.containerID.String() != c.ContainerID { //如果 container id 不是空的 if !w.containerID.IsEmpty() { //從 resultsManager 刪除關於 worker container 的結果 w.resultsManager.Remove(w.containerID) } //設定新的 container id w.containerID = kubecontainer.ParseContainerID(c.ContainerID) //設定 resultsManager 要接收新的 worker container id ，並且給他初始化的 value 與 pod spec （這裡要注意根據 probe 形式不同他們初始化得數值也不一樣 例如readiness :faill liveness :Success startup :Unknown） w.resultsManager.Set(w.containerID, w.initialValue, w.pod) // 因為有新的 container 我們繼續 prob 流程 w.onHold = false } //判斷是否要繼續 probe 流程 if w.onHold { // Worker is on hold until there is a new container. return true } //判斷 container 的狀態是否正在Running，如果不是正在 Running 有可能在做waiting 、有可能在 Terminated 。 if c.State.Running == nil { klog.V(3).InfoS(\u0026#34;Non-running container probed\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(w.pod), \u0026#34;containerName\u0026#34;, w.container.Name) //如果 container id 不是空的 if !w.containerID.IsEmpty() { //從 resultsManager 刪除關於 worker container 的結果 w.resultsManager.Set(w.containerID, results.Failure, w.pod) } // 如果 RestartPolicy 為 不重新啟動，則中止 worker。 return c.State.Terminated == nil || w.pod.Spec.RestartPolicy != v1.RestartPolicyNever } // 這邊我們要先了解一點！！！非長重要如果不了解建議先看之前我這篇文章[學習Kubernetes Garbage Collection機制](https://blog.jjmengze.website/posts/kubernetes/kubernetes-garbage-collection/) // 簡單來說，pod 被刪掉有可能先出現 DeletionTimestamp 的狀態 // 在這個狀態之下 pod 會在 BackGround 狀態被回收 // 可以把它想像成處於 Deletion 狀態的 pod ，且有設定 probe liveness 或是 startup，透過 resultsManager 設定成 probe 成功，不然會把 container 刪掉(重啟)。 // 最後停止 worker ，因為 pod 已經要被刪掉了 worker 就沒用處囉。 if w.pod.ObjectMeta.DeletionTimestamp != nil \u0026amp;\u0026amp; (w.probeType == liveness || w.probeType == startup) { klog.V(3).InfoS(\u0026#34;Pod deletion requested, setting probe result to success\u0026#34;, \u0026#34;probeType\u0026#34;, w.probeType, \u0026#34;pod\u0026#34;, klog.KObj(w.pod), \u0026#34;containerName\u0026#34;, w.container.Name) if w.probeType == startup { klog.InfoS(\u0026#34;Pod deletion requested before container has fully started\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(w.pod), \u0026#34;containerName\u0026#34;, w.container.Name) } // Set a last result to ensure quiet shutdown. w.resultsManager.Set(w.containerID, results.Success, w.pod) // Stop probing at this point. return false } // 判斷 probe 的初始 Delay 時間是否到了，如果還沒到就需要等下一次觸發 if int32(time.Since(c.State.Running.StartedAt.Time).Seconds()) \u0026lt; w.spec.InitialDelaySeconds { return true } //判斷 (過去) startup probe 是否已經成功，如果已經成功就可以關閉 startup worker，其他種類的 woker 保留。 //如果是還沒 startup probe 失敗 其他 probe 都不用談直接退回去重新等待下一次觸發。 if c.Started != nil \u0026amp;\u0026amp; *c.Started { // Stop probing for startup once container has started. if w.probeType == startup { return false } } else { // Disable other probes until container has started. if w.probeType != startup { return true } } //實際執行各種 probe 的地方，如果 probe 有 error 直接停止 worker //這裡依賴之前注入的 probeManager 的 probe 實作，藉由我們丟入的 probe 型態決定要怎麼執行 probe 。 result, err := w.probeManager.prober.probe(w.probeType, w.pod, status, w.container, w.containerID) if err != nil { // Prober error, throw away the result. return true } //如果 probe 沒有 error 透過 ProberResults.With 去觸發 metric 以提供後續監控服務 switch result { case results.Success: ProberResults.With(w.proberResultsSuccessfulMetricLabels).Inc() case results.Failure: ProberResults.With(w.proberResultsFailedMetricLabels).Inc() default: ProberResults.With(w.proberResultsUnknownMetricLabels).Inc() } //用來判斷同一個 probe 結果，並且計算執行 probe 次數用 if w.lastResult == result { w.resultRun++ } else { w.lastResult = result w.resultRun = 1 } //如果 probe 錯誤或是成功 低於閥值就直接安排下一次的 probe if (result == results.Failure \u0026amp;\u0026amp; w.resultRun \u0026lt; int(w.spec.FailureThreshold)) || (result == results.Success \u0026amp;\u0026amp; w.resultRun \u0026lt; int(w.spec.SuccessThreshold)) { // Success or failure is below threshold - leave the probe state unchanged. return true } //透過 resultsManager 設定哪個 container 的 prob 結果是什麼，給外面的人做事（重啟、刪除之類的） w.resultsManager.Set(w.containerID, result, w.pod) //如果 worker 型態為 （ liveness 或是 startup ）並且本次 prob 結果為失敗。 container 會重啟所以需要把 resultRun 重置 //並且設定 onHold=true ，因為 container 重啟了。前面要重新獲取 container id 不需要。 if (w.probeType == liveness || w.probeType == startup) \u0026amp;\u0026amp; result == results.Failure { w.onHold = true w.resultRun = 0 } //worker繼續 執行 return true } 整理一下 還記得之前為了找到在 proberesults.Manager 定義的卻沒被用到的這三個 function 嗎？我們現在來看看是不是都用上了！\n1 2 3 4 5 6 // 透過 container id 從 實作者身上得到 result 結果 Get(kubecontainer.ContainerID) (Result, bool) // 透過 container id 設定 pod 探測的結果。實作者需要把結果儲存起來。 Set(kubecontainer.ContainerID, Result, *v1.Pod) // 透過 container id 移除時實作者身上的對應的資料 Remove(kubecontainer.ContainerID) Set Worker - run\n透過 container id 設定 pod 探測的結果。實作者需要把結果儲存起來。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func (w *worker) doProbe() (keepGoing bool) { ... if w.containerID.String() != c.ContainerID { if !w.containerID.IsEmpty() { w.resultsManager.Remove(w.containerID) } w.containerID = kubecontainer.ParseContainerID(c.ContainerID) w.resultsManager.Set(w.containerID, w.initialValue, w.pod) // We\u0026#39;ve got a new container; resume probing. w.onHold = false } result, err := w.probeManager.prober.probe(w.probeType, w.pod, status, w.container, w.containerID) ... w.resultsManager.Set(w.containerID, result, w.pod) ... Remove Worker - run\n透過 container id 移除時實作者身上的對應的資料\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func (w *worker) run() { ... probeTicker := time.NewTicker(probeTickerPeriod) defer func() { // Clean up. probeTicker.Stop() if !w.containerID.IsEmpty() { w.resultsManager.Remove(w.containerID) } w.probeManager.removeWorker(w.pod.UID, w.container.Name, w.probeType) ... }() ... } func (w *worker) doProbe() (keepGoing bool) { ... if w.containerID.String() != c.ContainerID { if !w.containerID.IsEmpty() { w.resultsManager.Remove(w.containerID) } w.containerID = kubecontainer.ParseContainerID(c.ContainerID) w.resultsManager.Set(w.containerID, w.initialValue, w.pod) // We\u0026#39;ve got a new container; resume probing. w.onHold = false } ... 小結 總結一下 kubernetes kubelet 如何做 probe 的這幾篇文章，我先從最常使用到的 tcp socket、 exec 以及 http get 三種不同 probe 的基本操作相關文章分別是Kubernetes kubelet 探測 pod 的生命症狀 Http Get，Kubernetes kubelet 探測 pod 的生命症狀 tcp socket以及Kubernetes kubelet 探測 pod 的生命症狀 Exec，可以從這三篇文章了解 kubernetes probe 底層是如何實作這三種 probe 的，由於文章中沒有探討 probe 的物件是如何生成的。\n因此在接下來的Kubernetes kubelet 探測 pod 的生命症狀探針得如何出生-1的文章中，探討了 probe 到底怎麼誕生的。首先我們先觀察 proberesults.Manager 經過分析之後得知原來是用來儲存 container liveness 、 readiness 以及 startup 的 probe 結果。\n在 kubelet syncLoopIteration 的生命中週期中嘗試獲取 proberesults.Manager 的結果，再依照結果執行不同的動作。\n當我們分析完 proberesults.Manager 發現有許多方法沒被呼叫，因此我們順藤摸瓜找到 prober.Manager 主要都本篇文章分析的對象。經過本篇文章包丁解牛後發現原來 prober.Manager 是用來管理 container 是否要加入 probe 以及什麼時候要移除 probe 。\n其中我們發現了 prober.Manager 的實作對象透過 worker 的方式，將 container 身上的 probe 任務分配到不同的 worker 上執行，使得管理與實作的職責分離。\n以上為 kubernetes kubelet 如何做 probe 的簡易分析，文章中若有出現錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":6,"section":"posts","tags":["kubernetes","source-code"],"title":"Kubernetes kubelet 探測 pod 的生命症狀探針得如何出生-2","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/kubelet/probe/kubernetes-kubelet-probe-obj2/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n本篇文章基於Kubernetes kubelet 探測 pod 的生命症狀 Http Get，Kubernetes kubelet 探測 pod 的生命症狀 tcp socket以及Kubernetes kubelet 探測 pod 的生命症狀 Exec繼續往上蓋的違建（Ｘ），如果對於前幾個章節有興趣的小夥伴可以先到前三個章節了解一下 kubernetes 中的 kubelet 是如何分別透過三種手段去完成監測的。\n前幾個章節內容比較著重於 probe 是如何做檢測 container 的關於物件本身是如何產生的比較沒有著墨，因此本篇文章會將重點聚焦在 probe 物件是如何產生的以及相關的調用鍊。\nprobe 在哪裏絕對難不倒你 我們可以先從 kubelet 的資料結構中找到 probeManager 、 livenessManager 、 readinessManager 、 startupManager Kubernetes 。 kubelet 主要是靠以上這幾種 manager 去管理 probe 的相關物件。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 // Kubelet is the main kubelet implementation. type Kubelet struct //用來控制哪些 pod 要加入到 probe manager // Handles container probing. probeManager prober.Manager // 用來探測 container liveness 、 readiness 、 startup prob 結果儲存 // Manages container health check results. livenessManager proberesults.Manager readinessManager proberesults.Manager startupManager proberesults.Manager ... } 看完了 kubelet 其中跟 probe 相關的資料結構後我們接著來看看這些資料結構是如何生成與作用的。\n等等上面那個 prober.Manager 跟 proberesults.Manager 到底是什麼？別急我們後續會看到！\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...)(*Kubelet, error) { ... //在這裡透過 proberesults.NewManager() 生成 並且設定 kubelet 的 livenessManager readinessManager startupManager，用來儲存 container 探測 liveness 、 readiness 、 startup prob 的結果 //後面會繼續解析 proberesults.NewManager 得實作 klet.livenessManager = proberesults.NewManager() klet.readinessManager = proberesults.NewManager() klet.startupManager = proberesults.NewManager() ... // kubelet 的 probemanager 則是需要組合上面提到的三種 manager 以及 runner 與 recorder 透過 prober.NewManager 新增對應的物件，用以用來控制哪些 pod 要加入到 prob manager。 //後面會繼續解析 prober.NewManager 得實作 klet.probeManager = prober.NewManager( klet.statusManager, klet.livenessManager, klet.readinessManager, klet.startupManager, klet.runner, kubeDeps.Recorder) ... } 了解完 livenessManager readinessManager startupManager 以及 probeManager 的物件是從哪裡生成的後，我們就要來看看這四個物件作用在什麼地方，以及作用在哪裡。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 //kubelet 會透過 syncLoopIteration 去\u0026#34;得到\u0026#34; container 的狀態並且作出相應的行為， //至於怎麼跑到 syncLoopIteration 這一段的，我之後再做一篇整理，以減少文章的複雜xD func (kl *Kubelet) syncLoopIteration(configCh \u0026lt;-chan kubetypes.PodUpdate, handler SyncHandler, syncCh \u0026lt;-chan time.Time, housekeepingCh \u0026lt;-chan time.Time, plegCh \u0026lt;-chan *pleg.PodLifecycleEvent) bool { select { ... //接收 livenessManager channel 傳來的訊號，判斷是否探測成功。若是失敗透過 handleProbeSync function 處理。 case update := \u0026lt;-kl.livenessManager.Updates(): if update.Result == proberesults.Failure { handleProbeSync(kl, update, handler, \u0026#34;liveness\u0026#34;, \u0026#34;unhealthy\u0026#34;) } ... //接收 readinessManager channel 傳來的訊號，判斷是否探測成功。若是失敗透過 statusManager SetContainerReadiness function 與 handleProbeSync function 處理。 case update := \u0026lt;-kl.readinessManager.Updates(): ready := update.Result == proberesults.Success kl.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready) handleProbeSync(kl, update, handler, \u0026#34;readiness\u0026#34;, map[bool]string{true: \u0026#34;ready\u0026#34;, false: \u0026#34;\u0026#34;}[ready]) ... //接收 startupManager channel 傳來的訊號，判斷是否探測成功。若是失敗透過 statusManager SetContainerReadiness function 與 handleProbeSync function 處理。 case update := \u0026lt;-kl.startupManager.Updates(): started := update.Result == proberesults.Success kl.statusManager.SetContainerStartup(update.PodUID, update.ContainerID, started) handleProbeSync(kl, update, handler, \u0026#34;startup\u0026#34;, map[bool]string{true: \u0026#34;started\u0026#34;, false: \u0026#34;unhealthy\u0026#34;}[started]) ... } proberesults.Manager 我們首先來看 剛剛出現在 kubelet struct 裡面 livenessManager 、 readinessManager 以及 startupManager 共同的型態 proberesults.Manager 到底是什麼吧！\nInterface source code\n1 2 3 4 5 6 7 8 9 10 11 12 // Manager interface 定義了一些行為，讓實作的物件可以透過 container id 將 pod 的結果儲存起來，並且透過 channel 獲取探測的結果。 type Manager interface { // 透過 container id 從 實作者身上得到 result 結果 Get(kubecontainer.ContainerID) (Result, bool) // 透過 container id 設定 pod 探測的結果。實作者需要把結果儲存起來。 Set(kubecontainer.ContainerID, Result, *v1.Pod) // 透過 container id 移除時實作者身上的對應的資料 Remove(kubecontainer.ContainerID) // 透過 channel 接受 pod 探測的結果。 // NOTE: The current implementation only supports a single updates channel. Updates() \u0026lt;-chan Update } 從以上的程式碼我們可以看到 prober.Manager 是一個 interface 他定義了實作物件必須要可以透過 container id 將 pod 的結果儲存起來，並且透過 channel 獲取探測的結果。\nStruct 我們接著來了解實作 prober.Manager interface 的物件 manager\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // 實作 manager interface 的物件 type manager struct { // 用來保證 map 的一致性的鎖 sync.RWMutex // 使用 map 儲存 container ID 與 probe Result cache map[kubecontainer.ContainerID]Result // 透過 channel 回傳探針執行結果，透過 Update 物件包起來。 updates chan Update } // 透過 channel 要回傳給 kubelet 的 probe 結果之資料結構以 update 物件將結果包起來。 type Update struct { //哪一個 container id 的 探測結果 ContainerID kubecontainer.ContainerID //探測結果 Result Result //哪一個 pod id 的探測結果 PodUID types.UID } // 探測得結果，以 int 表示 type Result int const ( // golang 處理這種 enum 通常會 iota - 1 表示 Unknown 狀態，避免零值產生的錯誤 Unknown Result = iota - 1 // prob 成功回傳以 0 做為代表 Success // prob 成功回傳以 1 做為代表 Failure ) //編譯時期確保 manager 物件有實作 manger interface var _ Manager = \u0026amp;manager{} 好，看完了 manager struct 結構以及相關的物件長怎麼樣後，我們還需要了解這個物件是怎麼生成的。\nNew function 1 2 3 4 5 6 7 8 // Manager 的 new function 回傳一個空的 manger 物件 func NewManager() Manager { //回傳初始化過後的 manager 物件 return \u0026amp;manager{ cache: make(map[kubecontainer.ContainerID]Result), updates: make(chan Update, 20), } } impliment manger 物件實作了 prober.Manager interface ，我們來看每一個 function 實作的內容吧。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 //傳入 container 從 map 中取的儲存的 prob result func (m *manager) Get(id kubecontainer.ContainerID) (Result, bool) { //防止競爭加入 rw 鎖 m.RLock() defer m.RUnlock() //從 map 中透過 container id 取得對應的 probe 結果 result, found := m.cache[id] return result, found } //設定哪一個 container id 的 prob 探測結果(由外部傳入 probe 結果)，manager 單純作為 cache 用。 func (m *manager) Set(id kubecontainer.ContainerID, result Result, pod *v1.Pod) { //判斷本次 prob 結果是否與上次相符，若是有不一樣的地方存入 map 。並且包裝成 update 物件傳入 update channel if m.setInternal(id, result) { m.updates \u0026lt;- Update{id, result, pod.UID} } } // 判斷本次 probe 結果是否與上次相符，若是有不一樣的地方存入 map ，並且回傳 true。告知使用者本次 prob 的結果跟上次不一樣。 func (m *manager) setInternal(id kubecontainer.ContainerID, result Result) bool { //防止競爭加入 rw 鎖 m.Lock() defer m.Unlock() //需要判定本次處理 result 的跟上次 map 中儲存的處理結果是否一致。 //若是一致就不進行任何動作，並且回傳 false //若是不一致就需要將本次處理的結過存到 map 中，並且回傳 true prev, exists := m.cache[id] if !exists || prev != result { m.cache[id] = result return true } return false } //透過 container id 將 map 中對應的資料刪除 func (m *manager) Remove(id kubecontainer.ContainerID) { //防止競爭加入 rw 鎖 m.Lock() defer m.Unlock() //刪除 map 對應資料 delete(m.cache, id) } //回傳update channel func (m *manager) Updates() \u0026lt;-chan Update { return m.updates } 到此簡單的分析完實作了 proberesults.Manager interface 的 manger 物件，接著我們要來看看 manger 物件 在 kubelet 中怎麼被使用的的。\n等等！不是還有一個沒有分析嗎？probeManager 他的型態不都是 prober.Manager 嗎？怎麼不一起說說呢？這個部分我認為先了解livenessManager readinessManager startupManager 怎麼用之後我們再來看\nproberesults.Manager 觸發的時機 這邊雖然我們開始在 syncLoopIteration 小節已經看過，我想想再提一次，原來 livenessManager readinessManager startupManager 三個物件在 syncLoopIteration 時候會用到。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func (kl *Kubelet) syncLoopIteration(configCh \u0026lt;-chan kubetypes.PodUpdate, handler SyncHandler, syncCh \u0026lt;-chan time.Time, housekeepingCh \u0026lt;-chan time.Time, plegCh \u0026lt;-chan *pleg.PodLifecycleEvent) bool { select { ... //接收 livenessManager channel 傳來的訊號，判斷是否探測成功。若是失敗透過 handleProbeSync function 處理。 case update := \u0026lt;-kl.livenessManager.Updates(): if update.Result == proberesults.Failure { handleProbeSync(kl, update, handler, \u0026#34;liveness\u0026#34;, \u0026#34;unhealthy\u0026#34;) } ... //接收 readinessManager channel 傳來的訊號，判斷是否探測成功。若是失敗透過 statusManager SetContainerReadiness function 與 handleProbeSync function 處理。 case update := \u0026lt;-kl.readinessManager.Updates(): ready := update.Result == proberesults.Success kl.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready) handleProbeSync(kl, update, handler, \u0026#34;readiness\u0026#34;, map[bool]string{true: \u0026#34;ready\u0026#34;, false: \u0026#34;\u0026#34;}[ready]) ... //接收 startupManager channel 傳來的訊號，判斷是否探測成功。若是失敗透過 statusManager SetContainerReadiness function 與 handleProbeSync function 處理。 case update := \u0026lt;-kl.startupManager.Updates(): started := update.Result == proberesults.Success kl.statusManager.SetContainerStartup(update.PodUID, update.ContainerID, started) handleProbeSync(kl, update, handler, \u0026#34;startup\u0026#34;, map[bool]string{true: \u0026#34;started\u0026#34;, false: \u0026#34;unhealthy\u0026#34;}[started]) ... } 只有 Updates() function 被用到這樣嗎？剛剛看到 interface 定義了一堆東西，怎麼都沒用到呢？\n還記得 probeManager 怎麼生成的嗎？\n在 prob 在哪裏絕對難不倒你 章節有看過 probeManager 是由其他三種 manager 組合再一起生成的，那魔鬼一定就藏在這裡面。\n1 2 3 4 5 6 7 8 9 10 11 ... // kubelet 的 probemanager 則是需要組合上面提到的三種 manager 以及 runner 與 recorder 透過 prober.NewManager 新增對應的物件，用以用來控制哪些 pod 要加入到 prob manager。 //後面會繼續解析 prober.NewManager 實作 klet.probeManager = prober.NewManager( klet.statusManager, klet.livenessManager, klet.readinessManager, klet.startupManager, klet.runner, kubeDeps.Recorder) ... 所以說其他 function 會在 probeManager 裡面被調用囉？是沒錯！所以我們需要進一步的來分析 probeManager 是什麼。\n小結 前幾個章節內容比較著重於 probe 是如何做檢測 container 的關於物件本身是如何產生的比較沒有著墨，因此本篇文章會將重點聚焦在 probe 物件是如何產生的以及相關的調用鍊。\n由上文分析所得知 kubernetes 的 kubelet 資料結構中包含了 livenessManager、readinessManager 以及 startupManager 他們的型態皆為 proberesults.Manager 主要負責儲存 liveness 、 readiness 以及 startup 的 probe 結果。\n目前只看到 kubelet 在 syncLoopIteration 的階段會透過 proberesults.Manager的 Updates function 取出 probe 的變化，並且依照便會進行不同行為。\n這裡我保留了一個伏筆在下篇文章做揭曉，proberesults.Manager 的其他 function 去哪了？\nkubelet 資料結構中型態為 probeManager 的 probeManager 到底是什麼跟 livenessManager、readinessManager 以及 startupManager 又有什麼關西呢？\n","description":"","id":7,"section":"posts","tags":["kubernetes","source-code"],"title":"Kubernetes kubelet 探測 pod 的生命症狀探針得如何出生-1","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/kubelet/probe/kubernetes-kubelet-probe-obj/"},{"content":" 先來說說本篇文章的背景故事，鋪墊一下為什麼會想寫這篇文章。不論是同事還是朋友常常會會遇到應用服務（ Application Service ）套上快取（ Cache ）會出現一致性問題的問題，也就是資料庫( Database server) 跟 Cache 上面的資料不一致，本篇文章會簡單的討論一下為什麼會出現這樣的問題以及要如何去盡量防範這種不一致的情況。\n在這裡我想先說我的看法，如果有不一樣的想法的朋友歡迎提出討論。\n我認為套上 Cache 幾乎 沒辦法保證強一致性。\n可能會有朋友說有很多方式可以解決啊。\n在存取期間加入悲觀鎖，在併發寫的時候加鎖、讀取資料的時候不寫入 cache 。 透過分散式 transaction 3PC、TCC。 或是封裝 CAS 樂觀鎖，更新 cache 透過 lua 腳本去執行原子更新。 這個問題可以回到經典的 CAP 理論，我個人認為套用 Cache 的場景偏向於 CAP 中的 AP 。\nCAP 理論，表示在一個分散式系統中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分區容錯性），理論上三者不可得兼。\n但可以透過一些手段達成弱一致性，或是最終一致性。這邊非常建議各位朋友閱讀 Jack 大大在鐵人賽撰寫的文章分散式系統 - 在分散的世界中保持一致 ，可以獲的更多啟發。\n我們的應用服務有可能是以下這種架構\nApplication Service | | | Cache Server | | | Database server 這邊我們要先想一下，我們常用的 Cahca Pattern，常見的是以下三種，這三種模式在本篇文章中會簡易的說明。\nCache-Aside Write-behind Read/Write through Cache-Aside 最常見的 cache 架構，注意點為，在寫請求的情況會先更新 db 在刪除 cache。\nwrite 流程如圖所示：\nApplication Service 收到寫資料請求 將資料寫入 db 刪除 cache 資料 read 流程如圖所示：\nApplication Service 收到讀資料請求 先讀 cache 看看有沒有資料(does cache hit ?) cache hit 直接回傳資料 cahce 未命中，從 db 讀取讀取資料 把 db 讀取出來的資料寫入 cache 回傳資料 Write-behind 比較少看到這種情 cache 設計，主要是對資料遺失不是特別在意才會採用的方案，注意點為， Application Service 內部在上一層 local cache ，主要是先更新 local cache 在批次的向 DB 寫入。\nwrite 流程如圖所示：\nApplication Service 收到寫資料請求 先更新 local cache 更新外部 cache 再有一個請求進來\nApplication Service 收到寫資料請求\n先更新 local cache\n更新外部 cache\n隔一段時間後批次將cache資料寫入 db\n將第一次請求得資料寫入 db\n將第二次請求得資料寫入 db\nread 流程如圖所示：\nApplication Service 收到讀資料請求 Application Service 會先檢查 local cache 是否存在對應的資料 若是存在於 local cache 直接回傳對應資料 若是不存在 local cache 將往到外部 cache 查詢資料 若是外部 cahce 有資料直接會傳 若是不存在於外部 cache 的話將會從 db 撈資料 從 DB 撈出來的資料先寫回 local cache 最後再寫入外部 cache Read/Write through 大多數人會稱為三層架構（還是只有我這麼說xDD），注意點為， Application Service 內部在上一層 local cache 在寫請求的情況會先更新 local cache 再 db 最後才是更新 cache。\n可以理解為凡事以 local cache 為優先，再逐步更新 db 以及外部 cache 。\n通常會在 local cache 上 ttl\nwrite 流程如圖所示：\nApplication Service 收到寫資料請求 Application Service 將資料寫入 local cache 將資料往 Db 送 最後更新外部 cache read 流程如圖所示：\nApplication Service 收到度資料請求 Application Service 會先檢查 local cache 是否存在對應的資料 若是存在於 local cache 直接回傳對應資料 若是不存在 local cache 將往後到外部 cache 查詢資料 若是外部 cahce 有資料直接會傳 若是不存在於外部 cache 的話將會從 db 撈資料 從 DB 撈出來的資料先寫回 local cache 最後再寫入外部 cache 你可能會說：『對，我們大多數的應用程式都是長這樣子。』，那一致性問題會出現在哪呢？\n思考Ａ-\u0026gt;更新資料時要先 update cache 還是 delete cache?? 這個題目我們用 cache Aside 來觀察，從上面的寫的流程來看為 先更新資料庫 接著刪除 cache 。\n至於為什麼要這麼設計？\nexample 我們來看一個有如果是先 update cache 的簡單的例子吧。\nＡ 送了一個寫入請求給 Application Service Application Service 向 DB 寫入 A 的資料 B 送了一個寫入請求給 Application Service Application Service 向 DB 寫入 B 的資料\n（按照順序來說應是先更新 Ａ 的資料到 cache 中） 因為網路問題，Ａ的資料寫入 cache 時就 timeout 了 Ｂ的資料寫入 cache A timeout 重新寫入 cache 請問這時候使用者去讀 cache 會讀到資料 Ａ 還是資料 Ｂ 呢？\n很顯然是資料Ａ，也就是過時的資料！ solution 解法也相當簡單，只要把 update cache 的部分轉換成 delete cache 就行了，也就是標準的 cache Aside。\n你可能會思考這樣子就真的 ok 嗎？ 正如我開頭所說的，加上cache 非常 難達到所謂的強一制性。 為什麼？ 有許多狀況我們可以思考 表準的 cache Aside 是先更新 db 在刪除 cache。如果說有一種情況為，刪除 cache 失敗呢 \u0026lt;下面會看到例子\u0026gt;? 思考Ｂ-\u0026gt;更新資料時要先操作 DB 還是先操作 cache ?? 這個題目我們用 cache Aside 來觀察，從上面的寫的流程來看為 先更新資料庫 接著刪除cache。\n為什麼不是先刪除cache 接著才更新 DB ?\nexample Ａ 送了一個寫入請求給 Application Service Application Service 向 從 Cache 刪除 A 資料 Ａ\u0026rsquo; 時刻送了一個讀請求給 Application Service 因為網路問題，Ａ\u0026rsquo; 讀的資料先執行到了，導致以下狀況發生 所以 Ａ\u0026rsquo; 時刻讀 cache miss 進一步到 DB 拉資料 寫入到 cache Application Service 向 DB 寫入 A 的資料 請問這之後使用者去讀 cache 會讀到資料應該還是讀到 A\u0026rsquo; 時刻的資料，還是 A 時刻的資料？\n很顯然是資料Ａ，也就是過時的資料！ solution 解法也相當簡單，只要先操作 DB 的在操作 cache 就行了，也就是標準的 cache Aside。\n你可能會思考這樣子就真的 ok 嗎？ 正如我開頭所說的，加上cache 非常 難達到所謂的強一制性。 為什麼？ 有許多狀況我們可以思考 表準的 cache Aside 是先更新 db 在刪除 cache。如果說有一種情況為，刪除 cache 失敗呢仍然更新 DB 了呢 ? 可能有聽過的延遲雙刪除策略 可能各位朋友也有聽過一種做法叫做 延遲雙刪除策略 ，這邊就簡單的說明一下什麼是延遲雙刪除策略。\n簡單來說就是更新資料的時候，先刪除 cache 相隔一段時間後再次刪除 cache。\n我們想一下，如果有請求 Ａ 寫入、請求 Ａ 更新為 A\u0026rsquo; 以及讀取最新的資料，順序應該為下列所示。\n寫入 A 到 DB 刪除 cache 寫入 A\u0026rsquo; 到 DB 刪除 cache 想要從 cache 讀取 A\u0026rsquo; (cache miss) 從 DB 讀取 A' 寫入 A\u0026rsquo; 到 cache 如果這中間發生了一些延遲，可能行為會長得像以下情形。\n寫入 A 到 DB 刪除 cache 寫入 A\u0026rsquo; 到 DB 從 cache 讀取 A (cache miss)read 從 DB 讀取 Aread 刪除 cache 寫入 A 到 cacheread 這裡就發生了，cache 與DB 不一致，這時候可以透過雙刪策略去盡量別免這件事情的發生。\n我們先來看一下流程圖\n流程如圖所示：\nApplication Service 收到寫資料請求 刪除 cache 資料 將資料寫入 db 刪除 cache 資料 了這樣做的好處是，在業務邏輯允許的情況下，會讓 cache 與 DB 不同步的狀況減少，用雙刪除保證一件事情\u0026hellip;.中間拿走舊資料的時間有限，第二次刪除我就會把就漏的地方控制xD。\n刪除 cache write 1 寫入 A 到 DBwrite 1 刪除 cache write 1 刪除 cache[write 2] 寫入 A\u0026rsquo; 到 DB[write 2] 從 cache 讀取 A (cache miss)read 從 DB 讀取 Aread 寫入 A 到 cachewrite 2 刪除 cachewrite 2 到這裡其實還有問題，刪除有可能失敗。可能導致使用者一直以為 cache 上的是最新資料，這時候該怎麼辦？\n還可以用刪除重試策略 在上一小節中提到延遲雙刪除策略，只要是 api 操作就有可能遇到 timeout 、 Packet loss 等問題，那這時候可能會採取刪除重試策略，白話文可以翻譯成刪到成功為止。\n流程如圖所示：\nApplication Service 收到寫資料請求 刪除 cache 資料 如果失敗的話 Application Service 會把要 delete cache 的物件丟到 queue 裡面，用來確認之後刪除的對象 Queue會嘗試 delete cache 的物件(直到成功、或是某一遺忘種策略) 將資料寫入 db 刪除 cache 資料 如果失敗的話 Application Service 會把要 delete cache 的物件丟到 queue 裡面，用來確認之後刪除的對象 Queue會嘗試 delete cache 的物件(直到成功、或是某一遺忘種策略) 如果會覺得這種方式對於對於 code 不友善（我就是不想寫扣垃xD）的話，還有什麼方式呢？\nbindlog 刪除策略 在上一小節中提到我就是不想寫扣該怎麼辦，那可以試試看採用 biglog 刪除策略 ，只要將 DB 的 log訊息 bind 到 MQ 上面，由 MQ 的 ACK 機制確認 cache 有沒有正確處理訊息。\n流程如圖所示：\nApplication Service 收到寫資料請求 刪除 cache 資料 將資料寫入 db 由於 DB 的 log 與 MQ 進行綁定， DB 的某些操做會 enqueue MQ MQ 裡面的資料在 dequeue 時會自動向 cache 進行操作，並且以 ack 來確認是否操作成功（不成功的話不能 dequeue 或是執行 reenqueue ） Reference https://aws.amazon.com/builders-library/caching-challenges-and-strategies https://github.com/PegasusWang/system-design-primer#cache https://medium.com/@mena.meseha/3-major-problems-and-solutions-in-the-cache-world-155ecae41d4f http://lanlingzi.cn/post/technical/2018/0624_cache_design/ https://www.zhihu.com/question/39114188 https://docs.rs/xfetch/1.0.0/xfetch/ https://medium.com/coinmonks/tao-facebooks-distributed-database-for-social-graph-c2b45f5346ea https://www.mailgun.com/blog/golangs-superior-cache-solution-memcached-redis/ http://lanlingzi.cn/post/technical/2018/0624_cache_design/ ","description":"","id":8,"section":"posts","tags":["cache"],"title":"我的 cache 沒感覺了","uri":"https://blog.jjmengze.website/zh-tw/posts/cache/common/cache/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n本篇文章基於Kubernetes kubelet 探測 pod 的生命症狀 Http Get以及Kubernetes kubelet 探測 pod 的生命症狀 Exec繼續往上蓋的違建（Ｘ），大部分的內容都差不多，如有看過前一篇的並且建立基礎觀念的朋友可以直接滑到最下面看 kubernetes kubelet 如何透過 tcp socket 探測 pod 的生命症狀。\n使用 kubernetes 在建置自己的服務時，我們通常會透過 kubernetes 所提供的探針（probes） 來探測 pod 的特定服務是否正常運作。 probes 主要用來進行自我修復的功能，例如今天某一隻 process 因為業務邏輯或是程式寫錯造成死鎖的問題，我們就能透過 probes 來重新啟動 pod 來恢復程式的運行。或是假設今天 process 啟動到真正可以提供外部存取提供服務，所花費的時間需要比較長的時候我們也會透過 kubernetes 所提供的探針（probes） 來探測服務是不是可以提供外部使用。\n綜上所述 probes 分成兩種\nliveness\n主要用來判斷 pod 是否正常運作，如果探測失敗的話 kubelet 會把 pod 殺掉再重新建置。 readiness\n主要用來判斷 pod 是否可以提供給其他服務存取，如果探測失敗的話 kubelet 會把 pod 從 service backend 移除，這樣的話其他服務就無法從 service 存取到該服務。 今天主要跟大家分享是的 kubernetes 怎麼透過 liveness probes 的 tcp socket 去探測 pod 的生命狀態。\nprobes source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 // probe probes the container. func (pb *prober) probe(probeType probeType, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (results.Result, error) { var probeSpec *v1.Probe //首先判斷這次要執行探針的是哪一種類別，分別有readiness、liveness、startup switch probeType { //如果判斷是readiness就需要載入 container spec ReadinessProbe 寫的要求 case readiness: probeSpec = container.ReadinessProbe //如果判斷是liveness就需要載入 container spec LivenessProbe 寫的要求 case liveness: probeSpec = container.LivenessProbe //如果判斷是startup就需要載入 container spec StartupProbe 寫的要求 case startup: probeSpec = container.StartupProbe //不是上述這三種的話 kubernetes 目前無法處理。 default: return results.Failure, fmt.Errorf(\u0026#34;unknown probe type: %q\u0026#34;, probeType) } ctrName := fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, format.Pod(pod), container.Name) //如果 pod 裡面沒有定義 probe 的話就當作探測成功 if probeSpec == nil { klog.Warningf(\u0026#34;%s probe for %s is nil\u0026#34;, probeType, ctrName) return results.Success, nil } //傳入探針型態，探針規格，pod狀態，pod spec，以及要探測哪一個 container，以及重試次次數。 //接著會依照探測結果進行不同策略 result, output, err := pb.runProbeWithRetries(probeType, probeSpec, pod, status, container, containerID, maxProbeRetries) //如果 err 不是 nil 或是 result 不是 Success 同時不是 Warning，就要進行 log 處理 if err != nil || (result != probe.Success \u0026amp;\u0026amp; result != probe.Warning) { // Probe failed in one way or another. //簡單來說就是紀錄哪個 pod 哪個 container 發生了探針探測結果 ContainerUnhealthy，以及印一下 log 。 if err != nil { klog.V(1).Infof(\u0026#34;%s probe for %q errored: %v\u0026#34;, probeType, ctrName, err) pb.recordContainerEvent(pod, \u0026amp;container, v1.EventTypeWarning, events.ContainerUnhealthy, \u0026#34;%s probe errored: %v\u0026#34;, probeType, err) } else { // result != probe.Success klog.V(1).Infof(\u0026#34;%s probe for %q failed (%v): %s\u0026#34;, probeType, ctrName, result, output) pb.recordContainerEvent(pod, \u0026amp;container, v1.EventTypeWarning, events.ContainerUnhealthy, \u0026#34;%s probe failed: %s\u0026#34;, probeType, output) } return results.Failure, err } //如果 result 是 Warning ，簡單來說就是紀錄哪個 pod 哪個 container 發生了探針探測結果 warning，以及印一下 log 。 if result == probe.Warning { pb.recordContainerEvent(pod, \u0026amp;container, v1.EventTypeWarning, events.ContainerProbeWarning, \u0026#34;%s probe warning: %s\u0026#34;, probeType, output) klog.V(3).Infof(\u0026#34;%s probe for %q succeeded with a warning: %s\u0026#34;, probeType, ctrName, output) } //不然就是成功，發個 log 沒什麼其他的用途。 else { klog.V(3).Infof(\u0026#34;%s probe for %q succeeded\u0026#34;, probeType, ctrName) } //回傳一下探測結果 return results.Success, nil } runProbeWithRetries 主要傳入探針型態，探針規格，pod狀態，pod spec，以及要探測哪一個 container，接著透過 runProbe function 去執行探測。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // runProbeWithRetries tries to probe the container in a finite loop, it returns the last result // if it never succeeds. func (pb *prober) runProbeWithRetries(probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID, retries int) (probe.Result, string, error) { //探針錯誤訊息 var err error //探針結果 var result probe.Result //探針結果 var output string //若是失敗需要探測的總次數 for i := 0; i \u0026lt; retries; i++ { //開始探測，帶入探針型態，探針規格，pod狀態，pod spec，以及要探測哪一個 container。 result, output, err = pb.runProbe(probeType, p, pod, status, container, containerID) //如果探測成功直接回傳 if err == nil { return result, output, nil } } //如果探測失敗達到重試次數 return result, output, err } runProbe function 主要是執行探針探測的動作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 func (pb *prober) runProbe(probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (probe.Result, string, error) { //設定探測多久會 timeout timeout := time.Duration(p.TimeoutSeconds) * time.Second //如果 pod 有設定 exec 的話，就會透過 pb.exec.Probe 進行探測，上一篇主要在探討這一塊[Kubernetes kubelet 探測 pod 的生命症狀 Exec]([https://blog.jjmengze.website/posts/kubernetes/source-code/kubelet/prob/kubernetes-kubelet-http-get/](https://blog.jjmengze.website/posts/kubernetes/source-code/kubelet/prob/kubernetes-kubelet-exec/)) if p.Exec != nil { //先打個要執行 exec 的 log klog.V(4).Infof(\u0026#34;Exec-Probe Pod: %v, Container: %v, Command: %v\u0026#34;, pod.Name, container.Name, p.Exec.Command) //組合要執行的 command 與 container 環境變數 command := kubecontainer.ExpandContainerCommandOnlyStatic(p.Exec.Command, container.Env) //回傳執行結果 return pb.exec.Probe(pb.newExecInContainer(container, containerID, command, timeout)) } //如果 pod 有設定 HTTPGet 的話，就會透過 pb.HTTPGet.Probe 進行探測,上上篇[Kubernetes kubelet 探測 pod 的生命症狀 Http Get](https://blog.jjmengze.website/posts/kubernetes/source-code/kubelet/prob/kubernetes-kubelet-http-get/)主要在討論這一塊。 if p.HTTPGet != nil { //先把 HTTPGet.Scheme 轉成小寫，一般來說就是 http 或是 https scheme := strings.ToLower(string(p.HTTPGet.Scheme)) //取出目標 host 位置 host := p.HTTPGet.Host //如果目標 host 位置為空，預設用 pod 本身的 ip if host == \u0026#34;\u0026#34; { host = status.PodIP } //取出 pod 裡面指定 prob 的 port 號，有可能有人寫成 port: \u0026#34;http\u0026#34;或是寫成 port: 80 又或是 port : \u0026#34;80\u0026#34; //因此不能做簡單的提取 port, err := extractPort(p.HTTPGet.Port, container) if err != nil { return probe.Unknown, \u0026#34;\u0026#34;, err } //取出目標探測目標位置的路徑 path := p.HTTPGet.Path klog.V(4).Infof(\u0026#34;HTTP-Probe Host: %v://%v, Port: %v, Path: %v\u0026#34;, scheme, host, port, path) //把 scheme 、 host 、 port 、 path 組成 url 物件 url := formatURL(scheme, host, port, path) //填充這次要探測的 http header headers := buildHeader(p.HTTPGet.HTTPHeaders) klog.V(4).Infof(\u0026#34;HTTP-Probe Headers: %v\u0026#34;, headers) //本次要探測的型態，依照不同的探測型態去進行探測。 switch probeType { //若為 liveness 就透過 liveness Probe function 去檢測 case liveness: return pb.livenessHTTP.Probe(url, headers, timeout) //若為 startupHTTP 就透過 startupHTTP Probe function 去檢測 case startup: return pb.startupHTTP.Probe(url, headers, timeout) //若為 readinessHTTP 就透過 readinessHTTP Probe function 去檢測 default: return pb.readinessHTTP.Probe(url, headers, timeout) } } //如果有 pod 定義 tcp socket 的話，就會透過 pb.tcp.Probe 進行探測，本篇主要探討的部分。 if p.TCPSocket != nil { //取出 pod 裡面指定 prob 的 port 號，有可能有人寫成 port: \u0026#34;http\u0026#34;或是寫成 port: 80 又或是 port : \u0026#34;80\u0026#34; //因此不能做簡單的提取 port, err := extractPort(p.TCPSocket.Port, container) if err != nil { return probe.Unknown, \u0026#34;\u0026#34;, err } //取出目標 host 的位置 host := p.TCPSocket.Host //如果目標 host 位置為空，預設用 pod 本身的 ip if host == \u0026#34;\u0026#34; { host = status.PodIP } klog.V(4).Infof(\u0026#34;TCP-Probe Host: %v, Port: %v, Timeout: %v\u0026#34;, host, port, timeout) //實際執行 tcp prob 的部分這等等會看到如何處理。 return pb.tcp.Probe(host, port, timeout) } klog.Warningf(\u0026#34;Failed to find probe builder for container: %v\u0026#34;, container) //不屬於以上三種的 kubernetes 目前不支援呦，所以會還傳結果probe.Unknown，以及不支援 probe 的錯誤。 return probe.Unknown, \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;missing probe handler for %s:%s\u0026#34;, format.Pod(pod), container.Name) } 針對上述用到的 function 進行一些補充～\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 //仔細觀察參數的話，第一個輸入的 param 型態為 intstr.IntOrString，這個型態是什麼東西呢？ //依照文件的註解為IntOrString是可以含有 int32 或 string 的類型。在 JSON / YAML marshalling and unmarshalling 時使用，簡單來說使用者可以傳入 string 或是 int 的型態進來。 func extractPort(param intstr.IntOrString, container v1.Container) (int, error) { port := -1 var err error //第一步我們需要先去解析傳入的 port 是什麼型態來做對應的解析。 switch param.Type { //如果是 INT 的話，就把 port 以 int 的方式對出 case intstr.Int: port = param.IntValue() //如果是 INT 的話，就把 port 以 int 的方式對出 case intstr.String: //通過名稱查找 container 中的 Port。 if port, err = findPortByName(container, param.StrVal); err != nil { // 覺得註解很有趣，保留下來，最後一搏，嘗試將 string 轉成 int 有可能使用者定義 port : \u0026#34;8080\u0026#34;，試試看這樣可不可以轉成功 // Last ditch effort - maybe it was an int stored as string? if port, err = strconv.Atoi(param.StrVal); err != nil { return port, err } } // Type 無法處理 default: return port, fmt.Errorf(\u0026#34;intOrString had no kind: %+v\u0026#34;, param) } // port 在 0 ~ 65536 這個區間內才有效 if port \u0026gt; 0 \u0026amp;\u0026amp; port \u0026lt; 65536 { return port, nil } //回傳解析的 port 為多少 return port, fmt.Errorf(\u0026#34;invalid port number: %v\u0026#34;, port) } //上面有看到透過 param.IntValue() 把 intstr.IntOrString 為 int type 的轉換成 int 是透過這個方法 func (intstr *IntOrString) IntValue() int { //應該不會跑到這個方法，外面已經判斷過了，可能多一層做保障？ if intstr.Type == String { i, _ := strconv.Atoi(intstr.StrVal) return i } return int(intstr.IntVal) } // 通過名稱查找 container 中的 Port。 func findPortByName(container v1.Container, portName string) (int, error) { //透過傳入的 container port 透過迴圈找尋 port 名稱對應到的實際 port 號，以 int 的方式回傳。 for _, port := range container.Ports { if port.Name == portName { return int(port.ContainerPort), nil } } return 0, fmt.Errorf(\u0026#34;port %s not found\u0026#34;, portName) } // formatURL 格式化 args 中的 URL。 func formatURL(scheme string, host string, port int, path string) *url.URL { // 透過 url package 的 parse function 將 url 去解析。 u, err := url.Parse(path) //不知道這個錯誤什麼時候會出現...先保留註解，求大大幫看xD // Something is busted with the path, but it\u0026#39;s too late to reject it. Pass it along as is. if err != nil { u = \u0026amp;url.URL{ Path: path, } } // url 加上 scheme u.Scheme = scheme // url host 加上 host:port u.Host = net.JoinHostPort(host, strconv.Itoa(port)) return u } //把 pod spec prob header 加入到 prob 的請求中。 func buildHeader(headerList []v1.HTTPHeader) http.Header { //建立一個 head slice headers := make(http.Header) //把 pod spec prob header 透過 for rnge 的方式加入到 prob 的請求中。 for _, header := range headerList { headers[header.Name] = append(headers[header.Name], header.Value) } return headers } tcp socket kubernetes worker 上的 kubelet 會定期發送一個 command exec request 給 pod 內的 container ，如果 coomand exec status code 回傳成功 0 ，判斷目前 container 是否正常運作運作，若是不在這個 status code 範圍就會把 pod 刪掉。\npods/probe/tcp-liveness-readiness.yaml 範例是擷取自 kubernetes 官方網站，撰寫一個 yaml 檔送給 kubernetes 告訴 kubernetes 幫忙啟動一個 pod 並且建立一個 livenessProbe ， livenessProbe 會透過 tcp socket 方法去判斷 container 的 8080 port 是否正常。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 我們就以這個範例 kubelet livenessProbe 的 tcp socket 底層是如何實現的吧，先從觸發點來看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func (pb *prober) runProbe(probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (probe.Result, string, error) { ... //如果 pod 有設定 exec 的話，就會透過 pb.exec.Probe 進行探測 if p.Exec != nil { ... //如果有 pod 定義 tcp socket 的話，就會透過 pb.tcp.Probe 進行探測，本篇主要探討的部分。 if p.TCPSocket != nil { //取出 pod 裡面指定 prob 的 port 號，有可能有人寫成 port: \u0026#34;http\u0026#34;或是寫成 port: 80 又或是 port : \u0026#34;80\u0026#34; //因此不能做簡單的提取 port, err := extractPort(p.TCPSocket.Port, container) if err != nil { return probe.Unknown, \u0026#34;\u0026#34;, err } //取出目標 host 的位置 host := p.TCPSocket.Host //如果目標 host 位置為空，預設用 pod 本身的 ip if host == \u0026#34;\u0026#34; { host = status.PodIP } klog.V(4).Infof(\u0026#34;TCP-Probe Host: %v, Port: %v, Timeout: %v\u0026#34;, host, port, timeout) //實際執行 tcp prob 的部分這等等會看到如何處理。 return pb.tcp.Probe(host, port, timeout) } ... 如果探針型態為 TCPSocket 的話，就會透過（非常非常非常外面注入的）tcp 物件去處理探針，至於怎麼注入的\u0026hellip;之後再找時間整理xD\n我們來看怎麼一下透過 pb.tcp.Probe(host, port, timeout) 這個 function 做到 tcp socket 探測吧。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // Probe returns a ProbeRunner capable of running an TCP check. func (pr tcpProber) Probe(host string, port int, timeout time.Duration) (probe.Result, string, error) //組合 host 位址與 port 號，例如 hostip:port，傳入給 DoTCPProbe 處理 return DoTCPProbe(net.JoinHostPort(host, strconv.Itoa(port)), timeout) } // DoTCPProbe checks that a TCP socket to the address can be opened. // If the socket can be opened, it returns Success // If the socket fails to open, it returns Failure. // This is exported because some other packages may want to do direct TCP probes. func DoTCPProbe(addr string, timeout time.Duration) (probe.Result, string, error) { //透過 go 內建的 net package 透過 DialTimeout 建立 tcp 連線 conn, err := net.DialTimeout(\u0026#34;tcp\u0026#34;, addr, timeout) //如果建立連線失敗或是 time out 就噴錯，並且回傳錯誤訊息。 if err != nil { // Convert errors to failures to handle timeouts. return probe.Failure, err.Error(), nil } //建立連線成功後，就代表探測成功就可以關閉連線了 err = conn.Close() //若是關閉失敗，會印出做錯誤紀錄。 if err != nil { klog.Errorf(\u0026#34;Unexpected error closing TCP probe socket: %v (%#v)\u0026#34;, err, err) } //回傳探測成功 return probe.Success, \u0026#34;\u0026#34;, nil } 由於 tcp socket 的篇幅有點短，附上 tcp socket 的測試給大家參考xD\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 func TestTcpHealthChecker(t *testing.T) { // 透過 go 內建的 http test package 建立 http server ，以及設定好 handler 要做什麼。 server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) })) // function 結束後關閉 server defer server.Close() //把 hostip:port，切割成 hostip 與 port tHost, tPortStr, err := net.SplitHostPort(server.Listener.Addr().String()) if err != nil { t.Errorf(\u0026#34;unexpected error: %v\u0026#34;, err) } // port 是 string 型態轉換成 int 型態 tPort, err := strconv.Atoi(tPortStr) if err != nil { t.Errorf(\u0026#34;unexpected error: %v\u0026#34;, err) } // table driven //設定兩種測試情境 //第一種為測試成功。 //第二種為測試失敗 tests := []struct { host string port int expectedStatus probe.Result expectedError error }{ // A connection is made and probing would succeed {tHost, tPort, probe.Success, nil}, // No connection can be made and probing would fail {tHost, -1, probe.Failure, nil}, } //建立 tcp prob prober := New() //for 迴圈遞迴 table deiven 的測試情境 for i, tt := range tests { // 直接呼叫 Probe function 給定目標主機 port號 與 timeout 時間 status, _, err := prober. Probe(tt.host, tt.port, 1*time.Second) //觀察結果是否是符合測試案例 if status != tt.expectedStatus { t.Errorf(\u0026#34;#%d: expected status=%v, get=%v\u0026#34;, i, tt.expectedStatus, status) } if err != tt.expectedError { t.Errorf(\u0026#34;#%d: expected error=%v, get=%v\u0026#34;, i, tt.expectedError, err) } } } 小結 以上為 kubelet 探測 pod 的生命症狀 - tcp socket 簡易分析，簡單來說 kubernetes worker node 上的 kubelet process 會有一隻 worker 的 thread 建立一個探針，該 worker 會把 pod prob spec 解析出來並建立對應的探針，本篇以 prob 為 tcp socket 為例。\n我們看到了 net.DialTimeout 執行的結果沒有 error 的話就當作當作成功，其他結果都回報 Failure 。\n文章中若有出現錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":9,"section":"posts","tags":["kubernetes","source-code"],"title":"Kubernetes kubelet 探測 pod 的生命症狀 tcp socket","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/kubelet/probe/kubernetes-kubelet-tcp-socket/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n本篇文章基於Kubernetes kubelet 探測 pod 的生命症狀 Http Get繼續往上蓋的違建（Ｘ），大部分的內容都差不多，如有看過前一篇的並且建立基礎觀念的朋友可以直接滑到最下面看 kubernetes kubelet 如何透過 exec 探測 pod 的生命症狀。\n使用 kubernetes 在建置自己的服務時，我們通常會透過 kubernetes 所提供的探針（probes） 來探測 pod 的特定服務是否正常運作。 probes 主要用來進行自我修復的功能，例如今天某一隻 process 因為業務邏輯或是程式寫錯造成死鎖的問題，我們就能透過 probes 來重新啟動 pod 來恢復程式的運行。或是假設今天 process 啟動到真正可以提供外部存取提供服務，所花費的時間需要比較長的時候我們也會透過 kubernetes 所提供的探針（probes） 來探測服務是不是可以提供外部使用。\n綜上所述 probes 分成兩種\nliveness\n主要用來判斷 pod 是否正常運作，如果探測失敗的話 kubelet 會把 pod 殺掉再重新建置。 readiness\n主要用來判斷 pod 是否可以提供給其他服務存取，如果探測失敗的話 kubelet 會把 pod 從 service backend 移除，這樣的話其他服務就無法從 service 存取到該服務。 今天主要跟大家分享是的 kubernetes 怎麼透過 liveness probes 的 exec 去探測 pod 的生命狀態。\nprobes source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 // probe probes the container. func (pb *prober) probe(probeType probeType, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (results.Result, error) { var probeSpec *v1.Probe //首先判斷這次要執行探真的是哪一種類別，分別有readiness、liveness、startup switch probeType { //如果判斷是readiness就需要載入 container spec ReadinessProbe 寫的要求 case readiness: probeSpec = container.ReadinessProbe //如果判斷是liveness就需要載入 container spec LivenessProbe 寫的要求 case liveness: probeSpec = container.LivenessProbe //如果判斷是startup就需要載入 container spec StartupProbe 寫的要求 case startup: probeSpec = container.StartupProbe //不是上述這三種的話 kubernetes 目前無法處理。 default: return results.Failure, fmt.Errorf(\u0026#34;unknown probe type: %q\u0026#34;, probeType) } ctrName := fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, format.Pod(pod), container.Name) //如果 pod 裡面沒有定義 probe 的話就當作探測成功 if probeSpec == nil { klog.Warningf(\u0026#34;%s probe for %s is nil\u0026#34;, probeType, ctrName) return results.Success, nil } //傳入探針型態，探針規格，pod狀態，pod spec，以及要探測哪一個 container，以及重試次次數。 //接著會依照探測結果進行不同策略 result, output, err := pb.runProbeWithRetries(probeType, probeSpec, pod, status, container, containerID, maxProbeRetries) //如果 err 不是 nil 或是 result 不是 Success 同時不是 Warning，就要進行 log 處理 if err != nil || (result != probe.Success \u0026amp;\u0026amp; result != probe.Warning) { // Probe failed in one way or another. //簡單來說就是紀錄哪個 pod 哪個 container 發生了探針探測結果 ContainerUnhealthy，以及印一下 log 。 if err != nil { klog.V(1).Infof(\u0026#34;%s probe for %q errored: %v\u0026#34;, probeType, ctrName, err) pb.recordContainerEvent(pod, \u0026amp;container, v1.EventTypeWarning, events.ContainerUnhealthy, \u0026#34;%s probe errored: %v\u0026#34;, probeType, err) } else { // result != probe.Success klog.V(1).Infof(\u0026#34;%s probe for %q failed (%v): %s\u0026#34;, probeType, ctrName, result, output) pb.recordContainerEvent(pod, \u0026amp;container, v1.EventTypeWarning, events.ContainerUnhealthy, \u0026#34;%s probe failed: %s\u0026#34;, probeType, output) } return results.Failure, err } //如果 result 是 Warning ，簡單來說就是紀錄哪個 pod 哪個 container 發生了探針探測結果 warning，以及印一下 log 。 if result == probe.Warning { pb.recordContainerEvent(pod, \u0026amp;container, v1.EventTypeWarning, events.ContainerProbeWarning, \u0026#34;%s probe warning: %s\u0026#34;, probeType, output) klog.V(3).Infof(\u0026#34;%s probe for %q succeeded with a warning: %s\u0026#34;, probeType, ctrName, output) } //不然就是成功，發個 log 沒什麼其他的用途。 else { klog.V(3).Infof(\u0026#34;%s probe for %q succeeded\u0026#34;, probeType, ctrName) } //回傳一下探測結果 return results.Success, nil } runProbeWithRetries 主要傳入探針型態，探針規格，pod狀態，pod spec，以及要探測哪一個 container，接著透過 runProbe function 去執行探測。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // runProbeWithRetries tries to probe the container in a finite loop, it returns the last result // if it never succeeds. func (pb *prober) runProbeWithRetries(probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID, retries int) (probe.Result, string, error) { //探針錯誤訊息 var err error //探針結果 var result probe.Result //探針結果 var output string //若是失敗需要探測的總次數 for i := 0; i \u0026lt; retries; i++ { //開始探測，帶入探針型態，探針規格，pod狀態，pod spec，以及要探測哪一個 container。 result, output, err = pb.runProbe(probeType, p, pod, status, container, containerID) //如果探測成功直接回傳 if err == nil { return result, output, nil } } //如果探測失敗達到重試次數 return result, output, err } runProbe function 主要是執行探針探測的動作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 func (pb *prober) runProbe(probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (probe.Result, string, error) { //設定太測多久會 timeout timeout := time.Duration(p.TimeoutSeconds) * time.Second //如果 pod 有設定 exec 的話，就會透過 pb.exec.Probe 進行探測，今天主要討論這一塊。 if p.Exec != nil { //先打個要執行 exec 的 log klog.V(4).Infof(\u0026#34;Exec-Probe Pod: %v, Container: %v, Command: %v\u0026#34;, pod.Name, container.Name, p.Exec.Command) //實際執行 exec prob 本篇主要討論的對象 command := kubecontainer.ExpandContainerCommandOnlyStatic(p.Exec.Command, container.Env) //回傳執行結果 return pb.exec.Probe(pb.newExecInContainer(container, containerID, command, timeout)) } //如果 pod 有設定 HTTPGet 的話，就會透過 pb.HTTPGet.Probe 進行探測,上一篇[Kubernetes kubelet 探測 pod 的生命症狀 Http Get](https://blog.jjmengze.website/posts/kubernetes/source-code/kubelet/prob/kubernetes-kubelet-http-get/)主要在討論這一塊。 if p.HTTPGet != nil { //先把 HTTPGet.Scheme 轉成小寫，一般來說就是 http 或是 https scheme := strings.ToLower(string(p.HTTPGet.Scheme)) //取出目標 host 位置 host := p.HTTPGet.Host //如果目標 host 位置為空，預設用 pod 本身的 ip if host == \u0026#34;\u0026#34; { host = status.PodIP } //取出 pod 裡面指定 prob 的 port 號，有可能有人寫成 port: \u0026#34;http\u0026#34;或是寫成 port: 80 又或是 port : \u0026#34;80\u0026#34; //因此不能做簡單的提取 port, err := extractPort(p.HTTPGet.Port, container) if err != nil { return probe.Unknown, \u0026#34;\u0026#34;, err } //取出目標探測目標位置的路徑 path := p.HTTPGet.Path klog.V(4).Infof(\u0026#34;HTTP-Probe Host: %v://%v, Port: %v, Path: %v\u0026#34;, scheme, host, port, path) //把 scheme 、 host 、 port 、 path 組成 url 物件 url := formatURL(scheme, host, port, path) //填充這次要探測的 http header headers := buildHeader(p.HTTPGet.HTTPHeaders) klog.V(4).Infof(\u0026#34;HTTP-Probe Headers: %v\u0026#34;, headers) //本次要探測的型態，依照不同的探測型態去進行探測。 switch probeType { //若為 liveness 就透過 liveness Probe function 去檢測 case liveness: return pb.livenessHTTP.Probe(url, headers, timeout) //若為 startupHTTP 就透過 startupHTTP Probe function 去檢測 case startup: return pb.startupHTTP.Probe(url, headers, timeout) //若為 readinessHTTP 就透過 readinessHTTP Probe function 去檢測 default: return pb.readinessHTTP.Probe(url, headers, timeout) } } //如果有 pod 定義 tcp socket 的話，就會透過 pb.tcp.Probe 進行探測，這一塊未來會再討論。 if p.TCPSocket != nil { //取出 pod 裡面指定 prob 的 port 號，有可能有人寫成 port: \u0026#34;http\u0026#34;或是寫成 port: 80 又或是 port : \u0026#34;80\u0026#34; //因此不能做簡單的提取 port, err := extractPort(p.TCPSocket.Port, container) if err != nil { return probe.Unknown, \u0026#34;\u0026#34;, err } //取出目標 host 的位置 host := p.TCPSocket.Host //如果目標 host 位置為空，預設用 pod 本身的 ip if host == \u0026#34;\u0026#34; { host = status.PodIP } klog.V(4).Infof(\u0026#34;TCP-Probe Host: %v, Port: %v, Timeout: %v\u0026#34;, host, port, timeout) //實際執行 tcp prob 的部分這一塊未來會再討論。 return pb.tcp.Probe(host, port, timeout) } klog.Warningf(\u0026#34;Failed to find probe builder for container: %v\u0026#34;, container) //不屬於以上三種的 kubernetes 目前不支援呦，所以會還傳結果probe.Unknown，以及不支援 probe 的錯誤。 return probe.Unknown, \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;missing probe handler for %s:%s\u0026#34;, format.Pod(pod), container.Name) } 針對上述用到的 function 進行一些補充～\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 //仔細觀察參數的話，第一個輸入的 param 型態為 intstr.IntOrString，這個型態是什麼東西呢？ //依照文件的註解為IntOrString是可以含有 int32 或 string 的類型。在 JSON / YAML marshalling and unmarshalling 時使用，簡單來說使用者可以傳入 string 或是 int 的型態進來。 func extractPort(param intstr.IntOrString, container v1.Container) (int, error) { port := -1 var err error //第一步我們需要先去解析傳入的 port 是什麼型態來做對應的解析。 switch param.Type { //如果是 INT 的話，就把 port 以 int 的方式對出 case intstr.Int: port = param.IntValue() //如果是 INT 的話，就把 port 以 int 的方式對出 case intstr.String: //通過名稱查找 container 中的 Port。 if port, err = findPortByName(container, param.StrVal); err != nil { // 覺得註解很有趣，保留下來，最後一搏，嘗試將 string 轉成 int 有可能使用者定義 port : \u0026#34;8080\u0026#34;，試試看這樣可不可以轉成功 // Last ditch effort - maybe it was an int stored as string? if port, err = strconv.Atoi(param.StrVal); err != nil { return port, err } } // Type 無法處理 default: return port, fmt.Errorf(\u0026#34;intOrString had no kind: %+v\u0026#34;, param) } // port 在 0 ~ 65536 這個區間內才有效 if port \u0026gt; 0 \u0026amp;\u0026amp; port \u0026lt; 65536 { return port, nil } //回傳解析的 port 為多少 return port, fmt.Errorf(\u0026#34;invalid port number: %v\u0026#34;, port) } //上面有看到透過 param.IntValue() 把 intstr.IntOrString 為 int type 的轉換成 int 是透過這個方法 func (intstr *IntOrString) IntValue() int { //應該不會跑到這個方法，外面已經判斷過了，可能多一層做保障？ if intstr.Type == String { i, _ := strconv.Atoi(intstr.StrVal) return i } return int(intstr.IntVal) } // 通過名稱查找 container 中的 Port。 func findPortByName(container v1.Container, portName string) (int, error) { //透過傳入的 container port 透過迴圈找尋 port 名稱對應到的實際 port 號，以 int 的方式回傳。 for _, port := range container.Ports { if port.Name == portName { return int(port.ContainerPort), nil } } return 0, fmt.Errorf(\u0026#34;port %s not found\u0026#34;, portName) } // formatURL 格式化 args 中的 URL。 func formatURL(scheme string, host string, port int, path string) *url.URL { // 透過 url package 的 parse function 將 url 去解析。 u, err := url.Parse(path) //不知道這個錯誤什麼時候會出現...先保留註解，求大大幫看xD // Something is busted with the path, but it\u0026#39;s too late to reject it. Pass it along as is. if err != nil { u = \u0026amp;url.URL{ Path: path, } } // url 加上 scheme u.Scheme = scheme // url host 加上 host:port u.Host = net.JoinHostPort(host, strconv.Itoa(port)) return u } //把 pod spec prob header 加入到 prob 的請求中。 func buildHeader(headerList []v1.HTTPHeader) http.Header { //建立一個 head slice headers := make(http.Header) //把 pod spec prob header 透過 for rnge 的方式加入到 prob 的請求中。 for _, header := range headerList { headers[header.Name] = append(headers[header.Name], header.Value) } return headers } exec kubernetes worker 上的 kubelet 會定期發送一個 HTTP request 給 pod 內的 container ，如果 HTTP status code 回傳成功（400\u0026gt; code \u0026gt;= 200)，判斷目前 container 是否正常運作運作，若是不在這個 status code 範圍就會把 pod 刪掉。\npods/probe/exec-liveness.yaml 範例是擷取自 kubernetes 官方網站，撰寫一個 yaml 檔送給 kubernetes 告訴 kubernetes 幫忙啟動一個 pod 並且建立一個 livenessProbe ， livenessProbe 會透過 exec 方法去判斷 cat /tmp/healthy 執行指令是否執行成功。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 我們就以這個範例 kubelet livenessProbe 的 exec 底層是如何實現的吧，先從觸發點來看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (pb *prober) runProbe(probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (probe.Result, string, error) { ... //如果 pod 有設定 exec 的話，就會透過 pb.exec.Probe 進行探測 if p.Exec != nil { //印一下 log 看要執行甚麼指令，執行指令的 pod 與 container 是哪一個 klog.V(4).InfoS(\u0026#34;Exec-Probe runProbe\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;containerName\u0026#34;, container.Name, \u0026#34;execCommand\u0026#34;, p.Exec.Command) //把 command 與 container 的環境變數進行整理 command := kubecontainer.ExpandContainerCommandOnlyStatic(p.Exec.Command, container.Env) //整理完的 command 與 container 環境變數透過帶入 container id 去告知 pb.exec.Probe function 哪個 container 要執行什麼指令。 return pb.exec.Probe(pb.newExecInContainer(container, containerID, command, timeout)) } //如果有設定 http get 的話，上一篇主要在討論這一塊 if p.HTTPGet != nil { ... 如果探針型態為 exec 的話，就會透過（非常非常非常外面注入的）exec 物件去處理探針，至於怎麼注入的\u0026hellip;之後再找時間整理xD\n我們來看一下怎模透過 ExpandContainerCommandOnlyStatic 這個 function 整理 env 環境變數與 cmd 指令吧～\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 // ExpandContainerCommandOnlyStatic substitutes only static environment variable values from the // container environment definitions. This does *not* include valueFrom substitutions. // TODO: callers should use ExpandContainerCommandAndArgs with a fully resolved list of environment. func ExpandContainerCommandOnlyStatic(containerCommand []string, envs []v1.EnvVar) (command []string) { //v1EnvVarsToMap function 將 container []v1.EnvVar 環境變數轉換成 map[string]string 形式 //接著把 map[string]string 透過 expansion.MappingFuncFor 封裝成一個function mapping := expansion.MappingFuncFor(v1EnvVarsToMap(envs)) //先判斷 containerCommand 是否為零，若是不為 0 的話需要進行整理 if len(containerCommand) != 0 { //透過迴圈把數入的 command 透過 expansion.Expand 一個一個處理 for _, cmd := range containerCommand { command = append(command, expansion.Expand(cmd, mapping)) } } return command } //v1EnvVarsToMap 將 container []v1.EnvVar 環境變數轉換成 map[string]string 形式 func v1EnvVarsToMap(envs []v1.EnvVar) map[string]string { //建立map[string]string ，並且遞迴 envs 將環境變數以存入map。 result := map[string]string{} for _, env := range envs { result[env.Name] = env.Value } return result } //傳入 map[string]string 形式的環境變數回傳一個 function ，這個 function 會檢查傳入的 string //是否有包含環境變數的 key ，如果有的話直接回傳包含的環境變數，若是沒有需要透過 syntaxWrap function 封裝一層 func MappingFuncFor(context ...map[string]string) func(string) string { return func(input string) string { //透過 for 迴圈把環境變數全部跑一次 for _, vars := range context { //檢查 map 是否有對應 key 拿出對應的 value 回傳 val, ok := vars[input] if ok { return val } } return syntaxWrap(input) } } const ( operator = \u0026#39;$\u0026#39; referenceOpener = \u0026#39;(\u0026#39; referenceCloser = \u0026#39;)\u0026#39; ) // syntaxWrap function 回傳 shell 語法所需要的字串，例如 cat /etc/resolv.conf，會透過這個方法封裝成$(cat /etc/resolv.conf) func syntaxWrap(input string) string { return string(operator) + string(referenceOpener) + input + string(referenceCloser) } // 透過 Expand function 把 cmd 所含的環境變提取出來 // 我這邊假設三種情境（有點複雜只能透過這樣，呈現給大家，請見諒） // 我們都假設環境變數都存在 //情境A input : ls -al $(VAR_A) //情境B input : kubectl apply -f $(VAR_A) -f $(VAR_B) //情境C input : kubectl apply -f $(VAR_A)-1 func Expand(input string, mapping func(string) string) string { //先建立一個bytes.Buffer var buf bytes.Buffer //checkpoint從0開始 checkpoint := 0 //開始遞迴 input 情境 // Ａ 的 input ls -al $(VAR_A) // B input : kubectl apply -f $(VAR_A) -f $(VAR_B) // C input : kubectl apply -f $(VAR_A)-1 for cursor := 0; cursor \u0026lt; len(input); cursor++ { //case A input : ls -al $(VAR_A) //前面的ls -al 都不會處理，直到遇到第一個 $ 才會開始處理，這時候 cursor 是 7 也就是指到 input 的 $ 位置上。 if input[cursor] == operator \u0026amp;\u0026amp; cursor+1 \u0026lt; len(input) { // 從 check point 到 cursor 之間的數值寫到 buf 中 //case A input : ls -al $(VAR_A) // 這時 check point 為 0 到 cursor 7 之間的數值寫到 buf 中，也就是 \u0026#34;ls -al \u0026#34; 會寫到 buf buf.WriteString(input[checkpoint:cursor]) // Attempt to read the variable name as defined by the // syntax from the input string //用以判斷輸入的字串是否有包含環境變數，如果有環境變數的話回傳 //advance 是為了環境變數之後還有其他指令，下次可以從該index開始找環境變數 //case A input : ls -al $(VAR_A) //read 為 環境變數例如 VAR_A //isVar 為 true 表示為環境變數 //advance 為 從 (VAR_A) 的 ( 算到 ) +1 的 index (index 從0 開始算) ，case A 例子為 7 。 read, isVar, advance := tryReadVariableName(input[cursor+1:]) //如果有包含變數的話需要從環境變數中找到對應的value，並且寫入buffer if isVar { //從外部傳來的 mapping function 找到對應的環境變數，如果忘記的朋友可以上去複習一下 MappingFuncFor //case A input : ls -al $(VAR_A) //在這個情境下我們 透過 mapping function 找 VAR_A 對應的環境變數，我們這裡假設為example.yaml buf.WriteString(mapping(read)) } else { // 如果不包含環境變數的話接寫入 buffer buf.WriteString(read) } //把 cursor 指標往前移到剛剛判斷是否為環境變數的地方，因為要從剛剛判斷環境變數的地方繼續跑 //case A input : ls -al $(VAR_A) //在這個情境下 cursor 會移動到 14 的位置上 cursor += advance //check point 往前移動 //case A input : ls -al $(VAR_A) //在這個情境下 cursor 會移動到 15 的位置上 checkpoint = cursor + 1 } } //把 buffer 與剩下的 cmd 全部倒入並且回傳 //case A input : ls -al $(VAR_A) //在這個情境下會回傳剛剛寫入到 buffer 的東東要全部倒出來 ls -al example.yaml return buf.String() + input[checkpoint:] } //雖然離得有點遠這一段是在 runProbe function 中被呼叫的，忘記的可以回去複習。 //回傳一個實作 exec.Cmd interface 的物件，這邊實作的對象是 execInContainer struct，並且帶入run function 為 pb.runner.RunInContainer，等等下面會來看 //exec.Cmd interface 以及實作 execInContainer 的部分 func (pb *prober) newExecInContainer(container v1.Container, containerID kubecontainer.ContainerID, cmd []string, timeout time.Duration) exec.Cmd { // exec.Cmd interface 被 execInContainer 實作，我們等等來看 execInContainer 底層的實作。 // execInContainer 帶入 run function ，這裡的 function 採用的是 pb.runner.RunInContainer(containerID, cmd, timeout) return \u0026amp;execInContainer{run: func() ([]byte, error) { return pb.runner.RunInContainer(containerID, cmd, timeout) }} } // RunInContainer synchronously executes the command in the container, and returns the output. func (m *kubeGenericRuntimeManager) RunInContainer(id kubecontainer.ContainerID, cmd []string, timeout time.Duration) ([]byte, error) { stdout, stderr, err := m.runtimeService.ExecSync(id.ID, cmd, timeout) // NOTE(tallclair): This does not correctly interleave stdout \u0026amp; stderr, but should be sufficient // for logging purposes. A combined output option will need to be added to the ExecSyncRequest // if more precise output ordering is ever required. return append(stdout, stderr...), err } source code\nCmd 是一個 interface，它提供了一個與 os/exec 中的 Cmd 非常相似的 API。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 type Cmd interface { // Run runs the command to the completion. Run() error // CombinedOutput runs the command and returns its combined standard output // and standard error. This follows the pattern of package os/exec. CombinedOutput() ([]byte, error) // Output runs the command and returns standard output, but not standard err Output() ([]byte, error) SetDir(dir string) SetStdin(in io.Reader) SetStdout(out io.Writer) SetStderr(out io.Writer) SetEnv(env []string) // StdoutPipe and StderrPipe for getting the process\u0026#39; Stdout and Stderr as // Readers StdoutPipe() (io.ReadCloser, error) StderrPipe() (io.ReadCloser, error) // Start and Wait are for running a process non-blocking Start() error Wait() error // Stops the command by sending SIGTERM. It is not guaranteed the // process will stop before this function returns. If the process is not // responding, an internal timer function will send a SIGKILL to force // terminate after 10 seconds. Stop() } 我們來看一下實作 Cmd interface 的 結構體，這個 struct 就是上面 exec prob 呼叫的，由於 interface 有許多 signature 以下範例我們只看 prob exec 會用到的。\n1 2 3 4 5 type execInContainer struct { // run function 就是在 container 中執行命令。執行結束會回傳 stdout 和 stderr 。如果執行 function 發生錯誤，透過 error 回傳。 run func() ([]byte, error) writer io.Writer } 前面講了很多相關的東西只為了組出實作 exec.cmd interface 的物件，execProber 的 prob function 用，我們就來看看底層是怎麼處理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 func (pr execProber) Probe(e exec.Cmd) (probe.Result, string, error) { //建立一個 bytes buffer var dataBuffer bytes.Buffer writer := ioutils.LimitWriter(\u0026amp;dataBuffer, maxReadLength) //設定cmd的 stderr 以及 stdout 為 LimitWriter 的 writer e.SetStderr(writer) e.SetStdout(writer) //開始執行cmd err := e.Start() //判斷 cmd 執行錯誤，若是錯誤為 nil 就執行 cmd wait ，雖然 wait 沒有做任何事情。（不清楚為什麼要這樣設計，跪求大大提點） if err == nil { err = e.Wait() } //從 buffer 中取回 byte data := dataBuffer.Bytes() //印 log 看執行會傳什麼 response klog.V(4).Infof(\u0026#34;Exec probe response: %q\u0026#34;, string(data)) //如果 cmd 執行錯誤的話 if err != nil { //先把 error 轉成 exec.ExitError exit, ok := err.(exec.ExitError) //如果轉換換成功的話 ，進一步判斷ExitStatus是否為正常退出 //也就是 ExitStatus = 0 ，如果 ExitStatus 不為 0 就當作失敗。 if ok { if exit.ExitStatus() == 0 { return probe.Success, string(data), nil } return probe.Failure, string(data), nil } //轉換失敗當做未知錯誤來處理 return probe.Unknown, \u0026#34;\u0026#34;, err } //如果 cmd 執行沒有錯誤的話 直接回傳 success 處理 return probe.Success, string(data), nil } 小結 以上為 kubelet 探測 pod 的生命症狀 - exec 簡易分析，簡單來說 kubernetes worker node 上的 kubelet process 會有一隻 worker 的 thread 建立一個探針，該 worker 會把 pod prob spec 解析出來並建立對應的探針，本篇以 prob 為 exec 為例。\n我們看到了 exec 執行的結果回傳為 0 的話就當作當作成功，其他結果都回報 Failure ，下一章節將會針對 kubelet 如何透過 TCP Prob 探測 pod 的生命症狀。\n文章中若有出現錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":10,"section":"posts","tags":["kubernetes","source-code"],"title":"Kubernetes kubelet 探測 pod 的生命症狀 Exec","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/kubelet/probe/kubernetes-kubelet-exec/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n使用 kubernetes 在建置自己的服務時，我們通常會透過 kubernetes 所提供的探針（probes） 來探測 pod 的特定服務是否正常運作。 probes 主要用來進行自我修復的功能，例如今天某一隻 process 因為業務邏輯或是程式寫錯造成死鎖的問題，我們就能透過 probes 來重新啟動 pod 來恢復程式的運行。或是假設今天 process 啟動到真正可以提供外部存取提供服務，所花費的時間需要比較長的時候我們也會透過 kubernetes 所提供的探針（probes） 來探測服務是不是可以提供外部使用。\n綜上所述 probes 分成兩種\nliveness\n主要用來判斷 pod 是否正常運作，如果探測失敗的話 kubelet 會把 pod 殺掉再重新建置。 readiness\n主要用來判斷 pod 是否可以提供給其他服務存取，如果探測失敗的話 kubelet 會把 pod 從 service backend 移除，這樣的話其他服務就無法從 service 存取到該服務。 今天主要跟大家分享是的 kubernetes 怎麼透過 Http Get 去探測 pod 的生命狀態。\nprobes source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 // probe probes the container. func (pb *prober) probe(probeType probeType, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (results.Result, error) { var probeSpec *v1.Probe //首先判斷這次要執行探真的是哪一種類別，分別有readiness、liveness、startup switch probeType { //如果判斷是readiness就需要載入 container spec ReadinessProbe 寫的要求 case readiness: probeSpec = container.ReadinessProbe //如果判斷是liveness就需要載入 container spec LivenessProbe 寫的要求 case liveness: probeSpec = container.LivenessProbe //如果判斷是startup就需要載入 container spec StartupProbe 寫的要求 case startup: probeSpec = container.StartupProbe //不是上述這三種的話 kubernetes 目前無法處理。 default: return results.Failure, fmt.Errorf(\u0026#34;unknown probe type: %q\u0026#34;, probeType) } ctrName := fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, format.Pod(pod), container.Name) //如果 pod 裡面沒有定義 probe 的話就當作探測成功 if probeSpec == nil { klog.Warningf(\u0026#34;%s probe for %s is nil\u0026#34;, probeType, ctrName) return results.Success, nil } //傳入探針型態，探針規格，pod狀態，pod spec，以及要探測哪一個 container，以及重試次次數。 //接著會依照探測結果進行不同策略 result, output, err := pb.runProbeWithRetries(probeType, probeSpec, pod, status, container, containerID, maxProbeRetries) //如果 err 不是 nil 或是 result 不是 Success 也不是 Warning if err != nil || (result != probe.Success \u0026amp;\u0026amp; result != probe.Warning) { // Probe failed in one way or another. //簡單來說就是紀錄哪個 pod 哪個 container 發生了探針探測結果 ContainerUnhealthy，以及印一下 log 。 if err != nil { klog.V(1).Infof(\u0026#34;%s probe for %q errored: %v\u0026#34;, probeType, ctrName, err) pb.recordContainerEvent(pod, \u0026amp;container, v1.EventTypeWarning, events.ContainerUnhealthy, \u0026#34;%s probe errored: %v\u0026#34;, probeType, err) } else { // result != probe.Success klog.V(1).Infof(\u0026#34;%s probe for %q failed (%v): %s\u0026#34;, probeType, ctrName, result, output) pb.recordContainerEvent(pod, \u0026amp;container, v1.EventTypeWarning, events.ContainerUnhealthy, \u0026#34;%s probe failed: %s\u0026#34;, probeType, output) } return results.Failure, err } //如果 result 是 Warning ，簡單來說就是紀錄哪個 pod 哪個 container 發生了探針探測結果 warning，以及印一下 log 。 if result == probe.Warning { pb.recordContainerEvent(pod, \u0026amp;container, v1.EventTypeWarning, events.ContainerProbeWarning, \u0026#34;%s probe warning: %s\u0026#34;, probeType, output) klog.V(3).Infof(\u0026#34;%s probe for %q succeeded with a warning: %s\u0026#34;, probeType, ctrName, output) } //不然就是成功，發個 log 沒什麼其他的用途。 else { klog.V(3).Infof(\u0026#34;%s probe for %q succeeded\u0026#34;, probeType, ctrName) } //回傳一下探測結果 return results.Success, nil } runProbeWithRetries 主要傳入探針型態，探針規格，pod狀態，pod spec，以及要探測哪一個 container，接著透過 runProbe function 去執行探測。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // runProbeWithRetries tries to probe the container in a finite loop, it returns the last result // if it never succeeds. func (pb *prober) runProbeWithRetries(probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID, retries int) (probe.Result, string, error) { //探針錯誤訊息 var err error //探針結果 var result probe.Result //探針結果 var output string //若是失敗需要探測的總次數 for i := 0; i \u0026lt; retries; i++ { //開始探測，帶入探針型態，探針規格，pod狀態，pod spec，以及要探測哪一個 container。 result, output, err = pb.runProbe(probeType, p, pod, status, container, containerID) //如果探測成功直接回傳 if err == nil { return result, output, nil } } //如果探測失敗達到重試次數 return result, output, err } runProbe function 主要是執行探針探測的動作。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 func (pb *prober) runProbe(probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (probe.Result, string, error) { //設定太測多久會 timeout timeout := time.Duration(p.TimeoutSeconds) * time.Second //如果 pod 有設定 exec 的話，就會透過 pb.exec.Probe 進行探測，這一塊未來會再討論。 if p.Exec != nil { //先打個要執行 exec 的 log klog.V(4).Infof(\u0026#34;Exec-Probe Pod: %v, Container: %v, Command: %v\u0026#34;, pod.Name, container.Name, p.Exec.Command) //實際執行 exec prob 的部分這一塊未來會再討論。 command := kubecontainer.ExpandContainerCommandOnlyStatic(p.Exec.Command, container.Env) //回傳執行結果 return pb.exec.Probe(pb.newExecInContainer(container, containerID, command, timeout)) } //如果 pod 有設定 HTTPGet 的話，就會透過 pb.HTTPGet.Probe 進行探測，今天主要討論這一塊。 if p.HTTPGet != nil { //先把 HTTPGet.Scheme 轉成小寫，一般來說就是 http 或是 https scheme := strings.ToLower(string(p.HTTPGet.Scheme)) //取出目標 host 位置 host := p.HTTPGet.Host //如果目標 host 位置為空，預設用 pod 本身的 ip if host == \u0026#34;\u0026#34; { host = status.PodIP } //取出 pod 裡面指定 prob 的 port 號，有可能有人寫成 port: \u0026#34;http\u0026#34;或是寫成 port: 80 又或是 port : \u0026#34;80\u0026#34; //因此不能做簡單的提取 port, err := extractPort(p.HTTPGet.Port, container) if err != nil { return probe.Unknown, \u0026#34;\u0026#34;, err } //取出目標探測目標位置的路徑 path := p.HTTPGet.Path klog.V(4).Infof(\u0026#34;HTTP-Probe Host: %v://%v, Port: %v, Path: %v\u0026#34;, scheme, host, port, path) //把 scheme 、 host 、 port 、 path 組成 url 物件 url := formatURL(scheme, host, port, path) //填充這次要探測的 http header headers := buildHeader(p.HTTPGet.HTTPHeaders) klog.V(4).Infof(\u0026#34;HTTP-Probe Headers: %v\u0026#34;, headers) //本次要探測的型態，依照不同的探測型態去進行探測。 switch probeType { //若為 liveness 就透過 liveness Probe function 去檢測 case liveness: return pb.livenessHTTP.Probe(url, headers, timeout) //若為 startupHTTP 就透過 startupHTTP Probe function 去檢測 case startup: return pb.startupHTTP.Probe(url, headers, timeout) //若為 readinessHTTP 就透過 readinessHTTP Probe function 去檢測 default: return pb.readinessHTTP.Probe(url, headers, timeout) } } //如果有 pod 定義 tc //p socket 的話，就會透過 pb.tcp.Probe 進行探測，這一塊未來會再討論。 if p.TCPSocket != nil { //取出 pod 裡面指定 prob 的 port 號，有可能有人寫成 port: \u0026#34;http\u0026#34;或是寫成 port: 80 又或是 port : \u0026#34;80\u0026#34; //因此不能做簡單的提取 port, err := extractPort(p.TCPSocket.Port, container) if err != nil { return probe.Unknown, \u0026#34;\u0026#34;, err } //取出目標 host 的位置 host := p.TCPSocket.Host //如果目標 host 位置為空，預設用 pod 本身的 ip if host == \u0026#34;\u0026#34; { host = status.PodIP } klog.V(4).Infof(\u0026#34;TCP-Probe Host: %v, Port: %v, Timeout: %v\u0026#34;, host, port, timeout) //實際執行 tcp prob 的部分這一塊未來會再討論。 return pb.tcp.Probe(host, port, timeout) } klog.Warningf(\u0026#34;Failed to find probe builder for container: %v\u0026#34;, container) //不屬於以上三種的 kubernetes 目前不支援呦，所以會還傳結果probe.Unknown，以及不支援 probe 的錯誤。 return probe.Unknown, \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;missing probe handler for %s:%s\u0026#34;, format.Pod(pod), container.Name) } 針對上述用到的 function 進行一些補充～\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 //仔細觀察參數的話，第一個輸入的 param 型態為 intstr.IntOrString，這個型態是什麼東西呢？ //依照文件的註解為IntOrString是可以含有 int32 或 string 的類型。在 JSON / YAML marshalling and unmarshalling 時使用，簡單來說使用者可以傳入 string 或是 int 的型態進來。 func extractPort(param intstr.IntOrString, container v1.Container) (int, error) { port := -1 var err error //第一步我們需要先去解析傳入的 port 是什麼型態來做對應的解析。 switch param.Type { //如果是 INT 的話，就把 port 以 int 的方式對出 case intstr.Int: port = param.IntValue() //如果是 INT 的話，就把 port 以 int 的方式對出 case intstr.String: //通過名稱查找 container 中的 Port。 if port, err = findPortByName(container, param.StrVal); err != nil { // 覺得註解很有趣，保留下來，最後一搏，嘗試將 string 轉成 int 有可能使用者定義 port : \u0026#34;8080\u0026#34;，試試看這樣可不可以轉成功 // Last ditch effort - maybe it was an int stored as string? if port, err = strconv.Atoi(param.StrVal); err != nil { return port, err } } // Type 無法處理 default: return port, fmt.Errorf(\u0026#34;intOrString had no kind: %+v\u0026#34;, param) } // port 在 0 ~ 65536 這個區間內才有效 if port \u0026gt; 0 \u0026amp;\u0026amp; port \u0026lt; 65536 { return port, nil } //回傳解析的 port 為多少 return port, fmt.Errorf(\u0026#34;invalid port number: %v\u0026#34;, port) } //上面有看到透過 param.IntValue() 把 intstr.IntOrString 為 int type 的轉換成 int 是透過這個方法 [source code](vendor/k8s.io/apimachinery/pkg/util/intstr/intstr.go) func (intstr *IntOrString) IntValue() int { //應該不會跑到這個方法，外面已經判斷過了，可能多一層做保障？ if intstr.Type == String { i, _ := strconv.Atoi(intstr.StrVal) return i } return int(intstr.IntVal) } // 通過名稱查找 container 中的 Port。 [source code](pkg/kubelet/prober/prober.go) func findPortByName(container v1.Container, portName string) (int, error) { //透過傳入的 container port 透過迴圈找尋 port 名稱對應到的實際 port 號，以 int 的方式回傳。 for _, port := range container.Ports { if port.Name == portName { return int(port.ContainerPort), nil } } return 0, fmt.Errorf(\u0026#34;port %s not found\u0026#34;, portName) } // formatURL 格式化 args 中的 URL。 [source code](pkg/kubelet/prober/prober.go) func formatURL(scheme string, host string, port int, path string) *url.URL { // 透過 url package 的 parse function 將 url 去解析。 u, err := url.Parse(path) //不知道這個錯誤什麼時候會出現...先保留註解，求大大幫看xD // Something is busted with the path, but it\u0026#39;s too late to reject it. Pass it along as is. if err != nil { u = \u0026amp;url.URL{ Path: path, } } // url 加上 scheme u.Scheme = scheme // url host 加上 host:port u.Host = net.JoinHostPort(host, strconv.Itoa(port)) return u } //把 pod spec prob header 加入到 prob 的請求中。 [source code](pkg/kubelet/prober/prober.go) func buildHeader(headerList []v1.HTTPHeader) http.Header { //建立一個 head slice headers := make(http.Header) //把 pod spec prob header 透過 for rnge 的方式加入到 prob 的請求中。 for _, header := range headerList { headers[header.Name] = append(headers[header.Name], header.Value) } return headers } HTTP request kubernetes worker 上的 kubelet 會定期發送一個 HTTP request 給 pod 內的 container ，如果 HTTP status code 回傳成功（400\u0026gt; code \u0026gt;= 200)，判斷目前 container 是否正常運作運作，若是不在這個 status code 範圍就會把 pod 刪掉。\npods/probe/http-liveness.yaml\n範例是擷取自 kubernetes 官方網站，撰寫一個 yaml 檔送給 kubernetes 告訴 kubernetes 幫忙啟動一個 pod 並且建立一個 livenessProbe ， livenessProbe 會透過 http get 方法去判斷 pod:8080/healthz 並且 header 加上 http header key:Custom-Header value:Awesome。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: - name: liveness image: k8s.gcr.io/liveness args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 我們就以這個範例 kubelet livenessProbe 的 httpGet 底層是如何實現的吧，先從觸發點來看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (pb *prober) runProbe(probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (probe.Result, string, error) { ... //如果 pod 有設定 HTTPGet 的話，就會透過 pb.HTTPGet.Probe 進行探測 if p.HTTPGet != nil { //先把 HTTPGet.Scheme 轉成小寫，一般來說就是 http 或是 https scheme := strings.ToLower(string(p.HTTPGet.Scheme)) ... //本次要探測的型態，依照不同的探測型態去進行探測。 switch probeType { //若為 liveness 就透過 liveness Probe function 去檢測 case liveness: return pb.livenessHTTP.Probe(url, headers, timeout) case startup: ... 如果 probeType 是 liveness ，就會透過（非常非常非常外面注入的）livenessHTTP 物件去處理探針，至於怎麼注入的\u0026hellip;之後再找時間整理xD\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 // Probe returns a ProbeRunner capable of running an HTTP check. func (pr httpProber) Probe(url *url.URL, headers http.Header, timeout time.Duration) (probe.Result, string, error) { //會從外部帶入要 prob 的 url 物件， header timeout 等參數。 //先組成一個 http client ，Transport 這邊不是很重要先不要管他。 client := \u0026amp;http.Client{ Timeout: timeout, Transport: pr.transport, //設定轉發策略 CheckRedirect: redirectChecker(pr.followNonLocalRedirects), } //執行 prob 探測，傳入 http client 以及 要探測的 url 物件以及 header。 return DoHTTPProbe(url, headers, client) } func redirectChecker(followNonLocalRedirects bool) func(*http.Request, []*http.Request) error { //使用預設的 Redirects ，預設十次 if followNonLocalRedirects { return nil } return func(req *http.Request, via []*http.Request) error { // 轉發得目標不等於原來要發送的目標，也是直接噴錯。 if req.URL.Hostname() != via[0].URL.Hostname() { return http.ErrUseLastResponse } // Redirect \u0026gt;=10 就不轉發了，直接噴錯。 if len(via) \u0026gt;= 10 { return errors.New(\u0026#34;stopped after 10 redirects\u0026#34;) } return nil } } // GetHTTPInterface 用於發出 HTTP 請求的 interface ，回傳 response 跟 error ，這個 interface http client 有實作，可以能是未來可以抽換用的吧？ type GetHTTPInterface interface { Do(req *http.Request) (*http.Response, error) } // DoHTTPProbe checks if a GET request to the url succeeds. // If the HTTP response code is successful (i.e. 400 \u0026gt; code \u0026gt;= 200), it returns Success. // If the HTTP response code is unsuccessful or HTTP communication fails, it returns Failure. // This is exported because some other packages may want to do direct HTTP probes. func DoHTTPProbe(url *url.URL, headers http.Header, client GetHTTPInterface) (probe.Result, string, error) { //透過 http NewRequest 建立 get 方法，目標為 url 物件的網址 req, err := http.NewRequest(\u0026#34;GET\u0026#34;, url.String(), nil) // 建立 http requeset 失敗，報錯回傳 if err != nil { // Convert errors into failures to catch timeouts. return probe.Failure, err.Error(), nil } //如果 header 沒有 User-Agent 的話，就主動幫他加入 header 與 User-Agent 的 key 以及 value 為 kube-probe/\u0026lt;Major version\u0026gt;.\u0026lt;Minor version\u0026gt; if _, ok := headers[\u0026#34;User-Agent\u0026#34;]; !ok { if headers == nil { headers = http.Header{} } // explicitly set User-Agent so it\u0026#39;s not set to default Go value v := version.Get() headers.Set(\u0026#34;User-Agent\u0026#34;, fmt.Sprintf(\u0026#34;kube-probe/%s.%s\u0026#34;, v.Major, v.Minor)) } //將 Header 加入 request req.Header = headers //如果 header 有 Host 的話就把 header 的 host 的數值加入到 requeset 的 host if headers.Get(\u0026#34;Host\u0026#34;) != \u0026#34;\u0026#34; { req.Host = headers.Get(\u0026#34;Host\u0026#34;) } //透過外面注入進來的 http client 執行 requeset 的請求 res, err := client.Do(req) //請求失敗，報錯回傳 if err != nil { // Convert errors into failures to catch timeouts. return probe.Failure, err.Error(), nil } // defer 關閉 io 的讀取 defer res.Body.Close() //讀取 request body ，並且限制 body 長度 b, err := utilio.ReadAtMost(res.Body, maxRespBodyLength) //如果有錯誤的話就直接報錯，並且判斷是否超過 body 長度限制 if err != nil { if err == utilio.ErrLimitReached { klog.V(4).Infof(\u0026#34;Non fatal body truncation for %s, Response: %v\u0026#34;, url.String(), *res) } else { return probe.Failure, \u0026#34;\u0026#34;, err } } //讀出來的 body byte 轉成 string body := string(b) //判斷 StatusCode ，若是 StatusCode 介於 200 ~ 400 之間就當作成功，但是...StatusCode 為重新導向的話 （300） ，就回報 warring 。 if res.StatusCode \u0026gt;= http.StatusOK \u0026amp;\u0026amp; res.StatusCode \u0026lt; http.StatusBadRequest { //StatusCode 為重新導向的話 （300） ，就回報 warring 。 if res.StatusCode \u0026gt;= http.StatusMultipleChoices { // Redirect klog.V(4).Infof(\u0026#34;Probe terminated redirects for %s, Response: %v\u0026#34;, url.String(), *res) return probe.Warning, body, nil } //StatusCode 不為 300 的 200 ~ 400 其他狀況回報成功 klog.V(4).Infof(\u0026#34;Probe succeeded for %s, Response: %v\u0026#34;, url.String(), *res) return probe.Success, body, nil } //其他不是 200 ~ 400 的狀況登回報錯誤 klog.V(4).Infof(\u0026#34;Probe failed for %s with request headers %v, response body: %v\u0026#34;, url.String(), headers, body) return probe.Failure, fmt.Sprintf(\u0026#34;HTTP probe failed with statuscode: %d\u0026#34;, res.StatusCode), nil } // ReadAtMost 可以從 Reader 中讀取 byte 如果 body 大於 limit 的話就報錯～ func ReadAtMost(r io.Reader, limit int64) ([]byte, error) { limitedReader := \u0026amp;io.LimitedReader{R: r, N: limit} data, err := ioutil.ReadAll(limitedReader) if err != nil { return data, err } if limitedReader.N \u0026lt;= 0 { return data, ErrLimitReached } return data, nil } 小結 以上為 kubelet 探測 pod 的生命症狀 Http Get 簡易分析，簡單來說 kubernetes worker node 上的 kubelet process 會有一隻 worker 的 thread 建立一個探針，該 worker 會把 pod prob spec 解析出來並建立對應的探針（這一部份在後續會揭露），本篇以 prob 為 http get 為例。\n我們看到了 http status code 介於 200 ~ 400 之間就當作成功，此外 StatusCode 為重新導向的話 （300） ，就回報 warring ，下一章節將會針對 kubelet 如何透過 exec Prob 探測 pod 的生命症狀。\n文章中若有出現錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":11,"section":"posts","tags":["kubernetes","source-code"],"title":"Kubernetes kubelet 探測 pod 的生命症狀 Http Get","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/kubelet/probe/kubernetes-kubelet-http-get/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n本篇文章跟前一章非常相似，如果還沒有看過Kubernetes kubelet 怎麼抓住你的 configmap可以去參考看看！\n本章節的內容同樣需要先了解 Kubernetes kubelet cacheBasedManager 現在才知道以及Kubernetes kubelet cacheBasedManager 好喜歡 objectcache 的原因，會比較容易了解本章節的重點呦！\nmanger interface Kubernetes 先定義出取得 secret 資料的行為，有以下三點。\n當 pod spec 內寫到需要 secret 的時候如何註冊需要的 secret 。 曾經 pod spce 內有需要 xxx secret 現在不用的時候，需要反註冊 secret 。 能透過 secret 的 namespace 與 name 取得對應的資料。\nsource. code 1 2 3 4 5 6 7 8 9 10 11 12 // Manager manages Kubernetes secrets. This includes retrieving // secrets or registering/unregistering them via Pods. type Manager interface { GetSecret(namespace, name string) (*v1.Secret, error) RegisterPod(pod *v1.Pod) UnregisterPod(pod *v1.Pod) } Secret manager secretManager 這個物件實作了 manger interface，物件裡面的屬性也相當的簡單只有一個我們在前兩章節介紹過的 Kubernetes kubelet configmap \u0026amp; secret 與 cacheBasedManager 激情四射 ，也就是說secret Manager 極度依賴 manager.Manager 的實作。\nsource code\n1 2 3 type secretManager struct { manager manager.Manager\t//用以處理 pod 註冊 configmap/secret ，建立 reflector 等相關操作。 } [color=#f41dc9]TIPS:\n如果還不是很了解的 manager.Manager 的實作方式，強烈建議複習一下Kubernetes kubelet configmap \u0026amp; secret 與 cacheBasedManager 激情四射。\nnew function 了解完secretManager的資料結構後，我們接著要來看看在初始化這個物件時需要什麼東西吧！\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 // NewWatchingSecretManager creates a manager that keeps a cache of all secrets // necessary for registered pods. // It implements the following logic: // - whenever a pod is created or updated, we start individual watches for all // referenced objects that aren\u0026#39;t referenced from other registered pods // - every GetObject() returns a value from local cache propagated via watches func NewWatchingSecretManager(kubeClient clientset.Interface) Manager { //建立一個 lister watcher function ，主要用來監聽 kubernetes secret 的變化 //list 的條件為透過 client go 的 corev1 secret list＆watch 相關的資訊。 //實作上依賴注入的 kubernetes client interface ，這個 interface 功能非常多有機會之後再來看 listSecret := func(namespace string, opts metav1.ListOptions) (runtime.Object, error) { return kubeClient.CoreV1().Secrets(namespace).List(context.TODO(), opts) } watchSecret := func(namespace string, opts metav1.ListOptions) (watch.Interface, error) { return kubeClient.CoreV1().Secrets(namespace).Watch(context.TODO(), opts) } //還記得之前在挖掘 kubernetes controller operator 的過程嗎？ 需要知道要觀測對象的型態 //在這裡就是 secret newSecret := func() runtime.Object { return \u0026amp;v1.Secret{} } //secret 其中有個欄位是 Immutable ，當 reflector 觀察到 secret 會透過這個 function // 檢查 secret 是不是 Immutable 的狀態，若是為 Immutable 的狀態就會停止該物件的 reflector isImmutable := func(object runtime.Object) bool { if secret, ok := object.(*v1.Secret); ok { return secret.Immutable != nil \u0026amp;\u0026amp; *secret.Immutable } return false } gr := corev1.Resource(\u0026#34;secret\u0026#34;) //回傳 secret Manger 的物件，其中 manager 的實作為 WatchBasedManager return \u0026amp;secretManager{ manager: manager.NewWatchBasedManager(listSecret, watchSecret, newSecret, isImmutable, gr, getSecretNames), } } RegisterPod/UnregisterPod configMapManager 這裡就是無腦的把任務交給 manager 的 RegisterPod function 或是 UnregisterPod function ，裡面的實作簡單的提一下細節可以回去複習Kubernetes kubelet configmap \u0026amp; secret 與 cacheBasedManager 激情四射。\nsource code\nRegisterPod function pod spec 內用到 secret 的地方會透過 cacheBasedManager 去解析並且生成對應的 reflector UnregisterPod function pod spec 內用到 secret 的地方會透過 cacheBasedManager 去解析並且刪除對應的 reflector 1 2 3 4 5 6 7 func (s *secretManager) RegisterPod(pod *v1.Pod) { s.manager.RegisterPod(pod) } func (s *secretManager) UnregisterPod(pod *v1.Pod) { s.manager.UnregisterPod(pod) } 一定有人問 kubelet 如何取得 secret 資料，中間過程非常非常非常的長，整個調用鍊生命週期也有點小雜亂，之後有時間會來整理一下 kubelet 的調用鍊，本小節只簡單的帶出 kubelet 的 secret manager 如何註/反註冊 pod spec 中用到的 secret 欄位。\n底下這個 function 會觸發的時機點就先想像成, kubelet 會先把這一段時間有變化的 pod 都透過這個 function 丟進來。前面怎麼走到這一步的先不要管他\u0026hellip;太複雜了會迷失方向xD\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // updatePodsInternal replaces the given pods in the current state of the // manager, updating the various indices. The caller is assumed to hold the // lock. func (pm *basicManager) updatePodsInternal(pods ...*v1.Pod) { //遞迴所有變化的 pod for _, pod := range pods { ... //一般來說...前面已經會把 secretManager 設定好 if pm.secretManager != nil { // 如果 pod 處於 Terminated 狀態就需要返註冊 pod status if isPodInTerminatedState(pod) { pm.secretManager.UnregisterPod(pod) } else { // 如果 pod 處於其他狀態就需要註冊 pod status，開始解析用到的 secret 建立對應的 reflector 等等 pm.secretManager.RegisterPod(pod) } } ... GetSecret 只剩下一個主要的 GetSecret function，只要傳入 namespace 以及要取得的 secret name 就能得到對應的 secret 物件，這邊的情境略為複雜需要舉幾個例子來說明，我們先來看實作的部分。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (s *secretManager) GetSecret(namespace, name string) (*v1.Secret, error) { //主要是透過前幾章說的[Kubernetes kubelet configmap \u0026amp; secret 與 cacheBasedManager 激情四射](https://blog.jjmengze.website/posts/kubernetes/source-code/kubelet/configmapsecret/kubernetes-kubelet-cachebasedmanager/) //去取得 secret 物件的資料，這邊不了解的話可以去複習一下相關連結。 object, err := c.manager.GetObject(namespace, name) if err != nil { return nil, err } //透過 cacheBasedManager 取得的物件需要轉成對應的型態例如 secret if secret, ok := object.(*v1.Secret); ok { return secret, nil } return nil, fmt.Errorf(\u0026#34;unexpected object type: %v\u0026#34;, object) } 總共有三種方法可以將 kubernetes 中設定好的 secret 掛載到 pod container 中，這三種方法分別是。\nenv valueFrom secretRef envFrom configMapRef volumes secret 撰寫 yaml 給 kubernetes 很簡單那實際上 kubernetes 幫我們做了什麼呢？\nenv - valueFrom - secret 範例是擷取自 kubernetes 官方網站，撰寫一個 yaml 檔送給 kubernetes 告訴 kubernetes 幫忙啟動一個 pod 並且建立一個 secret ，將所有 secret 定義為 pod 的環境變數。 secret 中的 key 成為 Pod 中的環境變數名稱，value 則為環境變數的數值。\nUse-Case: As container environment variables\n1 2 3 4 5 6 7 8 apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: USER_NAME: YWRtaW4= PASSWORD: MWYyZDFlMmU2N2Rm 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata: name: secret-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34; ] envFrom: - secretRef: name: mysecret restartPolicy: Never 話不多說我們先來看 kubelet 怎麼從 api server 經過層層關卡抓到 pod yaml 裡面寫的東西，再取得對應的 secret 。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 func (kl *Kubelet) makeEnvironmentVariables(pod *v1.Pod, container *v1.Container, podIP string, podIPs []string) ([]kubecontainer.EnvVar, error) { .. //儲存最後要變成 pod 的環境變數 var result []kubecontainer.EnvVar //簡單來說就是如果 pod yaml 有啟用 EnableServiceLinks 的話就需要把同一個 namespaces 所有的 service //對應的名稱、IP 與 port 以環境變數的方式除存在 serviceENV 中 serviceEnv, err := kl.getServiceEnvVarMap(pod.Namespace, *pod.Spec.EnableServiceLinks) var ( configMaps = make(map[string]*v1.ConfigMap) secrets = make(map[string]*v1.Secret) tmpEnv = make(map[string]string) ) ... // Env will override EnvFrom variables. // Process EnvFrom first then allow Env to replace existing values. //這裡會遞迴 yaml 裡面 container envform 的欄位 for _, envFrom := range container.EnvFrom { //接著要來判斷 envform 裡面有沒有 SecretRef 這個欄位了 switch { //如果有找到 SecretRef 這個欄位的話 case envFrom.SecretRef != nil: //把 SecretRef 欄位內的資料結構拿出來為 s s := envFrom.SecretRef // 把 SecretRef 欄位內的名稱拿出來 ，這裡就是 secret 的名稱 name := s.Name // 檢查 secret map 是否有處理過同樣名字的物件過 secret, ok := secrets[name] //如果 map 找不到東西表示....我們要自己向 api server 要看相關的 secret if !ok { //這裡用的又是 kubernetes client interface ，沒有這個 interface 什麼都做不了 //所以這裡有錯的話就可以回家洗洗睡了xD if kl.kubeClient == nil { return result, fmt.Errorf(\u0026#34;couldn\u0026#39;t get secret %v/%v, no kubeClient defined\u0026#34;, pod.Namespace, name) } //簡單來說就是有定義 SecretEnvSource.Optional = true，等等會看到要用來做什麼的 optional := s.Optional != nil \u0026amp;\u0026amp; *s.Optional //這邊就是用 secret manager 的 GetConfigMap function 去取得 secret //所要求的參數也不多只要 namespace 與 secret 的 name 就能取的對應的資料 secret, err = kl.secretManager.GetSecret(pod.Namespace, name) //如果在找的過程有出現錯的話，並且有定義 SecretEnvSource.Optional = true 那就不會直接噴 error if err != nil { if errors.IsNotFound(err) \u0026amp;\u0026amp; optional { // ignore error when marked optional continue } return result, err } //接著以 secret name 作為 key 以及 secret 物件的內容作為 value 存在 map 中 //以供後續重複使用 configMaps[name] = configMap } //secret 中如果 key value 有不符合環境變數的資料會存在這個 slice 中 invalidKeys := []string{} //剛剛我們透過 secretManager.GetConfigMap 取的對應的 secret //我們的最終目的是取的 secret 對應的 key value 資料 //這裡就用 for 迴圈遞迴 secret 資料的每一列 for k, v := range secret.Data { if len(envFrom.Prefix) \u0026gt; 0 { k = envFrom.Prefix + k } //IsEnvVarName 判斷 configmap 資料的 key value 其中的 key 是否為有效的環境變變數名稱。 if errMsgs := utilvalidation.IsEnvVarName(k); len(errMsgs) != 0 { //secret 中如果 key value 有不符合環境變數的資料會存在這個 slice 中 invalidKeys = append(invalidKeys, k) continue } //把沒問題的資料存在暫存的環境變數 map tmpEnv[k] = v } //將有問題的 configmap 資料做整理加入一些 log 資訊讓使用者更好閱讀 if len(invalidKeys) \u0026gt; 0 { //不知道為什麼要 sort xDDD sort.Strings(invalidKeys) kl.recorder.Eventf(pod, v1.EventTypeWarning, \u0026#34;InvalidEnvironmentVariableNames\u0026#34;, \u0026#34;Keys [%s] from the EnvFrom secret %s/%s were skipped since they are considered invalid environment variable names.\u0026#34;, strings.Join(invalidKeys, \u0026#34;, \u0026#34;), pod.Namespace, name) } //環境變數的正規表達式 const envVarNameFmt = \u0026#34;[-._a-zA-Z][-._a-zA-Z0-9]*\u0026#34; var envVarNameRegexp = regexp.MustCompile(\u0026#34;^\u0026#34; + envVarNameFmt + \u0026#34;$\u0026#34;) //IsEnvVarName 測試 secret 資料的 key value 其中的 key 是否為有效的環境變變數名稱。 func IsEnvVarName(value string) []string { var errs []string if !envVarNameRegexp.MatchString(value) { errs = append(errs, RegexError(envVarNameFmtErrMsg, envVarNameFmt, \u0026#34;my.env-name\u0026#34;, \u0026#34;MY_ENV.NAME\u0026#34;, \u0026#34;MyEnvName1\u0026#34;)) } errs = append(errs, hasChDirPrefix(value)...) return errs } env - valueFrom - configMapKeyRef 範例是擷取自 kubernetes 官方網站，撰寫一個 yaml 檔送給 kubernetes 告訴 kubernetes 幫忙啟動一個 pod 並且建立一個 configmap ，將 ConfigMap 中定義的 special.how 數值作為 Pod 中的 SPECIAL_LEVEL_KEY 環境變數。\nconfigmap/multiple key-value pairs.yaml 1 2 3 4 5 6 7 8 apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: SPECIAL_LEVEL: very SPECIAL_TYPE: charm pods/pod-configmap-envFrom.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34; ] env: # Define the environment variable - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY name: special-config # Specify the key associated with the value key: special.how restartPolicy: Never 話不多說我們先來看 kubelet 怎麼從 api server 經過層層關卡抓到 pod yaml 裡面寫的東西，再取得對應的 configmap ，這邊的code 是接續著env - valueFrom - configMapRef 那一小節的 code。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 func (kl *Kubelet) makeEnvironmentVariables(pod *v1.Pod, container *v1.Container, podIP string, podIPs []string) ([]kubecontainer.EnvVar, error) { ... for _, envFrom := range container.EnvFrom switch { //如果有找到 ConfigMapRef 這個欄位的話 case envFrom.ConfigMapRef != nil: ... //接續env - valueFrom - configMapRef 小節 //遞迴 container 的Env 欄位 for _, envVar := range container.Env { runtimeVal := envVar.Value if runtimeVal != \u0026#34;\u0026#34; { ... //若是有定義 ValueFrom 欄位的話 } else if envVar.ValueFrom != nil { // Step 1b: resolve alternate env var sources //需要判斷是從 from 哪一種資源，本篇文章只關心 configmap switch { case envVar.ValueFrom.FieldRef != nil: ... case envVar.ValueFrom.ResourceFieldRef != nil: ... //如果 valueFrom 是引用 configmap 的話 case envVar.ValueFrom.ConfigMapKeyRef != nil: //先取出 ConfigMapKeyRef 為 cm cm := envVar.ValueFrom.ConfigMapKeyRef //取出 configmap 的 name name := cm.Name //取出對應的 key key := cm.Key //簡單來說就是有定義 ConfigMapEnvSource.Optional = true，等等會看到要用來做什麼的 optional := cm.Optional != nil \u0026amp;\u0026amp; *cm.Optional // 檢查 configmap map 是否有處理過同樣名字的物件過，如果有就可以直接拿出來用 configMap, ok := configMaps[name] //如果 map 找不到東西表示....我們要自己向 api server 要看相關的 configmap if !ok { //這裡用的又是 kubernetes client interface ，沒有這個 interface 什麼都做不了 //其實...這一段流程也沒拿來幹嘛...xD if kl.kubeClient == nil { return result, fmt.Errorf(\u0026#34;couldn\u0026#39;t get configMap %v/%v, no kubeClient defined\u0026#34;, pod.Namespace, name) } //這邊就是用 configmap manager 的 GetConfigMap function 去取得 configmap //所要求的參數也不多只要 namespace 與 configmap 的 name 就能取的對應的資料 configMap, err = kl.configMapManager.GetConfigMap(pod.Namespace, name) //如果在找的過程有出現錯的話，並且有定義 ConfigMapEnvSource.Optional = true 那就不會直接噴 error if err != nil { if errors.IsNotFound(err) \u0026amp;\u0026amp; optional { // ignore error when marked optional continue } return result, err } //接著以 configmap name 作為 key 以及 configmap 物件的內容作為 value 存在 map 中 //以供後續重複使用 configMaps[name] = configMap } //取出 configmap data 對應的 key 當作環境變數 runtimeVal, ok = configMap.Data[key] //如果拿不到對應的 key 且 ConfigMapEnvSource.Optional = true 那就不會直接噴 error if !ok { if optional { continue } return result, fmt.Errorf(\u0026#34;couldn\u0026#39;t find key %v in ConfigMap %v/%v\u0026#34;, key, pod.Namespace, name) } case envVar.ValueFrom.SecretKeyRef != nil: ... } } //還記得在env - valueFrom - configMapRef 這一小節，有提到過 serviceEnv 嗎？ //幫大家快速複習一下 //簡單來說就是如果 pod yaml 有啟用 EnableServiceLinks 的話就需要把同一個 namespaces 所有的 service //對應的名稱、IP 與 port 以環境變數的方式存在 serviceENV 中 //我們舉一個例子 kubernetes 通常會幫你在 default namespace 的 pod 加上一些環境變數 //例如：KUBERNETES_PORT_443_TCP_PROTO=tcp //這一行環境變數就是 serviceEnv 裡面的資料，資料來源可能透過kubelet 或是 api server //對應到 code 就是在env - valueFrom - configMapRef 這一小節的 //serviceEnv, err := kl.getServiceEnvVarMap(pod.Namespace, *pod.Spec.EnableServiceLinks) //如果使用者就是想覆蓋這個環境變數該怎麼辦？ //其實就直接在 yaml 直接設定一下就覆蓋過去了xD也就是下面這一行code而已拉 delete(serviceEnv, envVar.Name) //設定環境變數名稱為 pod spec 中定義的 name value 為從 configmap 或是其他資源取的數值 tmpEnv[envVar.Name] = runtimeVal } source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 func (kl *Kubelet) makeEnvironmentVariables(pod *v1.Pod, container *v1.Container, podIP string, podIPs []string) ([]kubecontainer.EnvVar, error) ... var result []kubecontainer.EnvVar serviceEnv, err := kl.getServiceEnvVarMap(pod.Namespace, *pod.Spec.EnableServiceLinks) ... var ( configMaps = make(map[string]*v1.ConfigMap) secrets = make(map[string]*v1.Secret) tmpEnv = make(map[string]string) ) ... // 將剛剛所產生出的環境變數暫存檔加入到 result 這個 slice ，未來將作為 container 的環境變數 for k, v := range tmpEnv { result = append(result, kubecontainer.EnvVar{Name: k, Value: v}) } // 加入 service env 環境變數 //簡單來說就是如果 pod yaml 有啟用 EnableServiceLinks 的話就需要把同一個 namespaces 所有的 service //對應的名稱、IP 與 port 以環境變數的方式除存在 serviceENV 中 for k, v := range serviceEnv { //如果 service env 的 key 暫存的環境變數不暫存這個 key 的話 //加入 service env 的環境變數到 result 的 slice 中 if _, present := tmpEnv[k]; !present { result = append(result, kubecontainer.EnvVar{Name: k, Value: v}) } } return result, nil } 小結 上面我們了解了 kubernetes 的 Manager 怎麼實作並且如何取得對應的 secret 資料了，基本上是依賴透過前兩章內容Kubernetes kubelet cacheBasedManager 現在才知道以及Kubernetes kubelet cacheBasedManager 好喜歡 objectcache 的原因。\n透過 cacheBasedManager 註冊 pod 的時候 ，將有使用到 secret 的欄位提取出來建立對應的 reflector ，並且將 reflector 對應到 objectStore 的 objectCacheItem 中。 objectStore 主要用來儲存 reflector 觀測到的物件狀態（新增的情況）\n如果有 nginxA 以及 nginxB 兩個 pod 同時都有 xxx-secret 該怎麼辦 xxx-secret 對應 xxx-objectCacheItem ， xxx-objectCacheItem 要記錄有兩個人 reference 到。 透過 cacheBasedManager 反註冊 pod 的時候 ，將上一次 pod 引用到 secret 用到的欄位取出來 ，並且將對應的 reflector 到 objectStore 的 objectCacheItem 進行移除。\nobjectStore 主要用來儲存 reflector 觀測到的物件狀態（刪除的情況） 如果 nginx B 不再關注這個 xxx-secret 要如何處理？ xxx-secret 對應 xxx-objectCacheItem ， xxx-objectCacheItem 要記錄現在有只有一個人 reference 到。 GetSecret 我們透過 namespace/name 向 cacheBasedManager 的 objectStore 取得對應的資料 objectStore 會拿 namespace/name 作為 key 找到對應到 objectCacheItem objectCacheItem 內有 reflector 可以把資料吐出來 以上為 secret manager 的簡易分析，如果對詳細的步驟有疑慮的話建議先到前兩章節去了解底層是如何處理的！!\n文章中若有出現錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":12,"section":"posts","tags":["kubernetes","source-code"],"title":" Kubernetes kubelet 怎麼抓住你的 secret","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/kubelet/configmapsecret/kubernetes-kubelet-secret-overlay/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n經過了前幾章節的鋪成今天終於可以跟大家聊聊非常常用在 kubernetes 的 configmap ， configmap 的底層到底是怎麼運作的。本章節的內容建議需要先了解 Kubernetes kubelet cacheBasedManager 現在才知道以及Kubernetes kubelet cacheBasedManager 好喜歡 objectcache 的原因，會比較容易了解本章節的重點！\nmanger interface Kubernetes 先定義出取得 configmap 資料的行為，有以下三點。\n當 pod spec 內寫到需要 configmap 的時候如何註冊需要的 configmap 。 曾經 pod spce 內有需要 xxx configmap 現在不用的時候，需要反註冊 configmap 。 能透過 configmap 的 namespace 與 name 取得對應的資料。 1 2 3 4 5 6 7 8 9 10 11 // Manager interface provides methods for Kubelet to manage ConfigMap. type Manager interface { GetConfigMap(namespace, name string) (*v1.ConfigMap, error) RegisterPod(pod *v1.Pod) UnregisterPod(pod *v1.Pod) } configmap manager configMapManager 這個物件實作了 manger interface，物件裡面的屬性也相當的簡單只有一個我們在前兩章節介紹過的 Kubernetes kubelet configmap \u0026amp; secret 與 cacheBasedManager 激情四射 ，也就是說configmap manager 極度依賴 manager.Manager 的實作。\n1 2 3 type configMapManager struct { manager manager.Manager\t//用以處理 pod 註冊 configmap/secret ，建立 reflector 等相關操作。 } [color=#f41dc9]TIPS:\n如果還不是很了解的 manager.Manager 的實作方式，強烈建議複習一下Kubernetes kubelet configmap \u0026amp; secret 與 cacheBasedManager 激情四射。\nnew function 了解完configMapManager的資料結構後，我們接著要來看看在初始化這個物件時需要什麼東西吧！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func NewWatchingConfigMapManager(kubeClient clientset.Interface) Manager //建立一個 lister watcher function ，主要用來監聽 kubernetes configmap 的變化 //list 的條件為透過 client go 的 corev1 configmap list＆watch 相關的資訊。 //實作上依賴注入的 kubernetes client interface ，這個 interface 功能非常多有機會之後再來看 listConfigMap := func(namespace string, opts metav1.ListOptions) (runtime.Object, error) { return kubeClient.CoreV1().ConfigMaps(namespace).List(context.TODO(), opts) } watchConfigMap := func(namespace string, opts metav1.ListOptions) (watch.Interface, error) { return kubeClient.CoreV1().ConfigMaps(namespace).Watch(context.TODO(), opts) } //還記得之前在挖掘 kubernetes controller operator 的過程嗎？ 需要知道要觀測對象的型態 //在這裡就是 configmap newConfigMap := func() runtime.Object { return \u0026amp;v1.ConfigMap{} } //configmap 其中有個欄位是 Immutable ，當 reflector 觀察到 configmap 會透過這個 function // 檢查 configmap 是不是 Immutable 的狀態，若是為 Immutable 的狀態就會停止該物件的 reflector isImmutable := func(object runtime.Object) bool { if configMap, ok := object.(*v1.ConfigMap); ok { return configMap.Immutable != nil \u0026amp;\u0026amp; *configMap.Immutable } return false } gr := corev1.Resource(\u0026#34;configmap\u0026#34;) //回傳 configMap Manger 的物件，其中 manager 的實作為 WatchBasedManager return \u0026amp;configMapManager{ manager: manager.NewWatchBasedManager(listConfigMap, watchConfigMap, newConfigMap, isImmutable, gr, getConfigMapNames), } } RegisterPod/UnregisterPod configMapManager 這裡就是無腦的把任務交給 manager 的 RegisterPod function 或是 UnregisterPod function ，裡面的實作簡單的提一下細節可以回去複習Kubernetes kubelet configmap \u0026amp; secret 與 cacheBasedManager 激情四射。\nsource code\nRegisterPod function pod spec 內用到 configmap 的地方會透過 cacheBasedManager 去解析並且生成對應的 reflector UnregisterPod function pod spec 內用到 configmap 的地方會透過 cacheBasedManager 去解析並且刪除對應的 reflector 1 2 3 4 5 6 7 func (c *configMapManager) RegisterPod(pod *v1.Pod) { c.manager.RegisterPod(pod) } func (c *configMapManager) UnregisterPod(pod *v1.Pod) { c.manager.UnregisterPod(pod) } 一定有人問 kubelet 如何取得 configmap 資料，中間過程非常非常非常的長，整個調用鍊生命週期也有點小雜亂，之後有時間會來整理一下 kubelet 的調用鍊，本小節只簡單的帶出 kubelet 的 configmap manager 如何註/反註冊 pod spec 中用到的 configmap 欄位。\n底下這個 function 會觸發的時機點就先想像成, kubelet 會先把這一段時間有變化的 pod 都透過這個 function 丟進來。前面怎麼走到這一步的先不要管他\u0026hellip;太複雜了會迷失方向xD\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // updatePodsInternal replaces the given pods in the current state of the // manager, updating the various indices. The caller is assumed to hold the // lock. func (pm *basicManager) updatePodsInternal(pods ...*v1.Pod) { //遞迴所有變化的 pod for _, pod := range pods { ... //一般來說...前面已經會把 configMapManager 設定好 if pm.configMapManager != nil { // 如果 pod 處於 Terminated 狀態就需要返註冊 pod status if isPodInTerminatedState(pod) { pm.configMapManager.UnregisterPod(pod) } else { // 如果 pod 處於其他狀態就需要註冊 pod status，開始解析用到的 configmap 建立對應的 reflector 等等 pm.configMapManager.RegisterPod(pod) } } ... GetConfigMap 只剩下一個主要的 GetConfigMap function，只要傳入 namespace 以及要取得的 configmap name 就能得到對應的 configmap 物件，這邊的情境略為複雜需要舉幾個例子來說明，我們先來看實作的部分。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (c *configMapManager) GetConfigMap(namespace, name string) (*v1.ConfigMap, error) { //主要是透過前幾章說的[Kubernetes kubelet configmap \u0026amp; secret 與 cacheBasedManager 激情四射](https://blog.jjmengze.website/posts/kubernetes/source-code/kubelet/configmapsecret/kubernetes-kubelet-cachebasedmanager/) //去取得 configmap 物件的資料，這邊不了解的話可以去複習一下相關連結。 object, err := c.manager.GetObject(namespace, name) if err != nil { return nil, err } //透過 cacheBasedManager 取得的物件需要轉成對應的型態例如 configmap if configmap, ok := object.(*v1.ConfigMap); ok { return configmap, nil } return nil, fmt.Errorf(\u0026#34;unexpected object type: %v\u0026#34;, object) } 總共有三種方法可以將 kubernetes 中設定好的 configmap 掛載到 pod container 中，這三種方法分別是。\nenv valueFrom configMapKeyRef envFrom configMapRef volumes configMap 撰寫 yaml 給 kubernetes 很簡單那實際上 kubernetes 幫我們做了什麼呢？\nenv - valueFrom - configMapRef 範例是擷取自 kubernetes 官方網站，撰寫一個 yaml 檔送給 kubernetes 告訴 kubernetes 幫忙啟動一個 pod 並且建立一個 configmap ，將所有 ConfigMap 的 data 定義為 pod 的環境變數。 ConfigMap中的 key 成為 Pod 中的環境變數名稱，value 則為環境變數的數值。\nconfigmap/multiple key-value pairs.yaml 1 2 3 4 5 6 7 8 apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: SPECIAL_LEVEL: very SPECIAL_TYPE: charm pods/pod-single-configmap-env-variable.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34; ] envFrom: - configMapRef: name: special-config restartPolicy: Never 話不多說我們先來看 kubelet 怎麼從 api server 經過層層關卡抓到 pod yaml 裡面寫的東西，再取得對應的 configmap 。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 func (kl *Kubelet) makeEnvironmentVariables(pod *v1.Pod, container *v1.Container, podIP string, podIPs []string) ([]kubecontainer.EnvVar, error) { //儲存最後要變成 pod 的環境變數 var result []kubecontainer.EnvVar //簡單來說就是如果 pod yaml 有啟用 EnableServiceLinks 的話就需要把同一個 namespaces 所有的 service //對應的名稱、IP 與 port 以環境變數的方式除存在 serviceENV 中 serviceEnv, err := kl.getServiceEnvVarMap(pod.Namespace, *pod.Spec.EnableServiceLinks) var ( configMaps = make(map[string]*v1.ConfigMap) secrets = make(map[string]*v1.Secret) tmpEnv = make(map[string]string) ) ... // Env will override EnvFrom variables. // Process EnvFrom first then allow Env to replace existing values. //這裡會遞迴 yaml 裡面 container envform 的欄位 for _, envFrom := range container.EnvFrom { //接著要來判斷 envform 裡面有沒有 ConfigMapRef 這個欄位了 switch { //如果有找到 ConfigMapRef 這個欄位的話 case envFrom.ConfigMapRef != nil: //把 ConfigMapRef 欄位內的資料結構拿出來為 cm cm := envFrom.ConfigMapRef // 把 ConfigMapRef 欄位內的名稱拿出來 ，這裡就是 configmap 的名稱 name := cm.Name // 檢查 configmap map 是否有處理過同樣名字的物件過 configMap, ok := configMaps[name] //如果 map 找不到東西表示....我們要自己向 api server 要看相關的 configmap if !ok { //這裡用的又是 kubernetes client interface ，沒有這個 interface 什麼都做不了 //所以這裡有錯的話就可以回家洗洗睡了xD if kl.kubeClient == nil { return result, fmt.Errorf(\u0026#34;couldn\u0026#39;t get configMap %v/%v, no kubeClient defined\u0026#34;, pod.Namespace, name) } //簡單來說就是有定義 ConfigMapEnvSource.Optional = true，等等會看到要用來做什麼的 optional := cm.Optional != nil \u0026amp;\u0026amp; *cm.Optional //這邊就是用 configmap manager 的 GetConfigMap function 去取得 configmap //所要求的參數也不多只要 namespace 與 configmap 的 name 就能取的對應的資料 configMap, err = kl.configMapManager.GetConfigMap(pod.Namespace, name) //如果在找的過程有出現錯的話，並且有定義 ConfigMapEnvSource.Optional = true 那就不會直接噴 error if err != nil { if errors.IsNotFound(err) \u0026amp;\u0026amp; optional { // ignore error when marked optional continue } return result, err } //接著以 configmap name 作為 key 以及 configmap 物件的內容作為 value 存在 map 中 //以供後續重複使用 configMaps[name] = configMap } //configmap 中如果 key value 有不符合環境變數的資料會存在這個 slice 中 invalidKeys := []string{} //剛剛我們透過 configMapManager.GetConfigMap 取的對應的 configmap //我們的最終目的是取的 configmap 對應的 key value 資料 //這裡就用 for 迴圈遞迴 configmap 資料的每一列 for k, v := range configMap.Data { if len(envFrom.Prefix) \u0026gt; 0 { k = envFrom.Prefix + k } //IsEnvVarName 判斷 configmap 資料的 key value 其中的 key 是否為有效的環境變變數名稱。 if errMsgs := utilvalidation.IsEnvVarName(k); len(errMsgs) != 0 { //configmap 中如果 key value 有不符合環境變數的資料會存在這個 slice 中 invalidKeys = append(invalidKeys, k) continue } //把沒問題的資料存在暫存的環境變數 map tmpEnv[k] = v } //將有問題的 configmap 資料做整理加入一些 log 資訊讓使用者更好閱讀 if len(invalidKeys) \u0026gt; 0 { //不知道為什麼要 sort xDDD sort.Strings(invalidKeys) kl.recorder.Eventf(pod, v1.EventTypeWarning, \u0026#34;InvalidEnvironmentVariableNames\u0026#34;, \u0026#34;Keys [%s] from the EnvFrom configMap %s/%s were skipped since they are considered invalid environment variable names.\u0026#34;, strings.Join(invalidKeys, \u0026#34;, \u0026#34;), pod.Namespace, name) } //環境變數的正規表達式 const envVarNameFmt = \u0026#34;[-._a-zA-Z][-._a-zA-Z0-9]*\u0026#34; var envVarNameRegexp = regexp.MustCompile(\u0026#34;^\u0026#34; + envVarNameFmt + \u0026#34;$\u0026#34;) //IsEnvVarName 測試 configmap 資料的 key value 其中的 key 是否為有效的環境變變數名稱。 func IsEnvVarName(value string) []string { var errs []string if !envVarNameRegexp.MatchString(value) { errs = append(errs, RegexError(envVarNameFmtErrMsg, envVarNameFmt, \u0026#34;my.env-name\u0026#34;, \u0026#34;MY_ENV.NAME\u0026#34;, \u0026#34;MyEnvName1\u0026#34;)) } errs = append(errs, hasChDirPrefix(value)...) return errs } env - valueFrom - configMapKeyRef 範例是擷取自 kubernetes 官方網站，撰寫一個 yaml 檔送給 kubernetes 告訴 kubernetes 幫忙啟動一個 pod 並且建立一個 configmap ，將 ConfigMap 中定義的 special.how 數值作為 Pod 中的 SPECIAL_LEVEL_KEY 環境變數。\nconfigmap/multiple key-value pairs.yaml 1 2 3 4 5 6 7 8 apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: SPECIAL_LEVEL: very SPECIAL_TYPE: charm pods/pod-configmap-envFrom.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34; ] env: # Define the environment variable - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY name: special-config # Specify the key associated with the value key: special.how restartPolicy: Never 話不多說我們先來看 kubelet 怎麼從 api server 經過層層關卡抓到 pod yaml 裡面寫的東西，再取得對應的 configmap ，這邊的code 是接續著env - valueFrom - configMapRef 那一小節的 code。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 func (kl *Kubelet) makeEnvironmentVariables(pod *v1.Pod, container *v1.Container, podIP string, podIPs []string) ([]kubecontainer.EnvVar, error) { ... for _, envFrom := range container.EnvFrom switch { //如果有找到 ConfigMapRef 這個欄位的話 case envFrom.ConfigMapRef != nil: ... //接續env - valueFrom - configMapRef 小節 //遞迴 container 的Env 欄位 for _, envVar := range container.Env { runtimeVal := envVar.Value if runtimeVal != \u0026#34;\u0026#34; { ... //若是有定義 ValueFrom 欄位的話 } else if envVar.ValueFrom != nil { // Step 1b: resolve alternate env var sources //需要判斷是從 from 哪一種資源，本篇文章只關心 configmap switch { case envVar.ValueFrom.FieldRef != nil: ... case envVar.ValueFrom.ResourceFieldRef != nil: ... //如果 valueFrom 是引用 configmap 的話 case envVar.ValueFrom.ConfigMapKeyRef != nil: //先取出 ConfigMapKeyRef 為 cm cm := envVar.ValueFrom.ConfigMapKeyRef //取出 configmap 的 name name := cm.Name //取出對應的 key key := cm.Key //簡單來說就是有定義 ConfigMapEnvSource.Optional = true，等等會看到要用來做什麼的 optional := cm.Optional != nil \u0026amp;\u0026amp; *cm.Optional // 檢查 configmap map 是否有處理過同樣名字的物件過，如果有就可以直接拿出來用 configMap, ok := configMaps[name] //如果 map 找不到東西表示....我們要自己向 api server 要看相關的 configmap if !ok { //這裡用的又是 kubernetes client interface ，沒有這個 interface 什麼都做不了 //其實...這一段流程也沒拿來幹嘛...xD if kl.kubeClient == nil { return result, fmt.Errorf(\u0026#34;couldn\u0026#39;t get configMap %v/%v, no kubeClient defined\u0026#34;, pod.Namespace, name) } //這邊就是用 configmap manager 的 GetConfigMap function 去取得 configmap //所要求的參數也不多只要 namespace 與 configmap 的 name 就能取的對應的資料 configMap, err = kl.configMapManager.GetConfigMap(pod.Namespace, name) //如果在找的過程有出現錯的話，並且有定義 ConfigMapEnvSource.Optional = true 那就不會直接噴 error if err != nil { if errors.IsNotFound(err) \u0026amp;\u0026amp; optional { // ignore error when marked optional continue } return result, err } //接著以 configmap name 作為 key 以及 configmap 物件的內容作為 value 存在 map 中 //以供後續重複使用 configMaps[name] = configMap } //取出 configmap data 對應的 key 當作環境變數 runtimeVal, ok = configMap.Data[key] //如果拿不到對應的 key 且 ConfigMapEnvSource.Optional = true 那就不會直接噴 error if !ok { if optional { continue } return result, fmt.Errorf(\u0026#34;couldn\u0026#39;t find key %v in ConfigMap %v/%v\u0026#34;, key, pod.Namespace, name) } case envVar.ValueFrom.SecretKeyRef != nil: ... } } //還記得在env - valueFrom - configMapRef 這一小節，有提到過 serviceEnv 嗎？ //幫大家快速複習一下 //簡單來說就是如果 pod yaml 有啟用 EnableServiceLinks 的話就需要把同一個 namespaces 所有的 service //對應的名稱、IP 與 port 以環境變數的方式存在 serviceENV 中 //我們舉一個例子 kubernetes 通常會幫你在 default namespace 的 pod 加上一些環境變數 //例如：KUBERNETES_PORT_443_TCP_PROTO=tcp //這一行環境變數就是 serviceEnv 裡面的資料，資料來源可能透過kubelet 或是 api server //對應到 code 就是在env - valueFrom - configMapRef 這一小節的 //serviceEnv, err := kl.getServiceEnvVarMap(pod.Namespace, *pod.Spec.EnableServiceLinks) //如果使用者就是想覆蓋這個環境變數該怎麼辦？ //其實就直接在 yaml 直接設定一下就覆蓋過去了xD也就是下面這一行code而已拉 delete(serviceEnv, envVar.Name) //設定環境變數名稱為 pod spec 中定義的 name value 為從 configmap 或是其他資源取的數值 tmpEnv[envVar.Name] = runtimeVal } source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 func (kl *Kubelet) makeEnvironmentVariables(pod *v1.Pod, container *v1.Container, podIP string, podIPs []string) ([]kubecontainer.EnvVar, error) ... var result []kubecontainer.EnvVar serviceEnv, err := kl.getServiceEnvVarMap(pod.Namespace, *pod.Spec.EnableServiceLinks) ... var ( configMaps = make(map[string]*v1.ConfigMap) secrets = make(map[string]*v1.Secret) tmpEnv = make(map[string]string) ) ... // 將剛剛所產生出的環境變數暫存檔加入到 result 這個 slice ，未來將作為 container 的環境變數 for k, v := range tmpEnv { result = append(result, kubecontainer.EnvVar{Name: k, Value: v}) } // 加入 service env 環境變數 //簡單來說就是如果 pod yaml 有啟用 EnableServiceLinks 的話就需要把同一個 namespaces 所有的 service //對應的名稱、IP 與 port 以環境變數的方式除存在 serviceENV 中 for k, v := range serviceEnv { //如果 service env 的 key 暫存的環境變數不暫存這個 key 的話 //加入 service env 的環境變數到 result 的 slice 中 if _, present := tmpEnv[k]; !present { result = append(result, kubecontainer.EnvVar{Name: k, Value: v}) } } return result, nil } 小結 上面我們了解了 kubernetes 的 Manager 怎麼實作並且如何取得對應的 configmap 資料了，基本上是依賴透過前兩章內容Kubernetes kubelet cacheBasedManager 現在才知道以及Kubernetes kubelet cacheBasedManager 好喜歡 objectcache 的原因。\n透過 cacheBasedManager 註冊 pod 的時候 ，將有使用到 configmap 的欄位提取出來建立對應的 reflector ，並且將 reflector 對應到 objectStore 的 objectCacheItem 中。\nobjectStore 主要用來儲存 reflector 觀測到的物件狀態（新增的情況）\n如果有 nginxA 以及 nginxB 兩個 pod 同時都有 xxx-configmap該怎麼辦 xxx-configmap 對應 xxx-objectCacheItem ， xxx-objectCacheItem 要記錄有兩個人 refference 到。 透過 cacheBasedManager 反註冊 pod 的時候 ，將上一次 pod 引用到 configmap 用到的欄位取出來 ，並且將對應的 reflector 到 objectStore 的 objectCacheItem 進行移除。\nobjectStore 主要用來儲存 reflector 觀測到的物件狀態（刪除的情況） 如果 nginx B 不再關注這個 xxx-configmap 要如何處理？ xxx-configmap 對應 xxx-objectCacheItem ， xxx-objectCacheItem 要記錄現在有只有一個人 refference 到。 GetConfigMap 我們透過 namespace/name 向 cacheBasedManager 與 objectStore 取得對應的資料 objectStore 會拿 namespace/name 作為 key 找到對應到 objectCacheItem objectCacheItem 內有 reflector 可以把資料吐出來 以上為 configmap manager 的簡易分析，如果對詳細的步驟有疑慮的話建議先到前兩章節去了解底層是如何處理的！下一章節會繼續解析 kubernetes kubelet 是如何和抓住你的 secret。\n文章中若有出現錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":13,"section":"posts","tags":["kubernetes","source-code"],"title":" Kubernetes kubelet 怎麼抓住你的 configmap","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/kubelet/configmapsecret/kubernetes-kubelet-configmap-overlay/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n在上一篇 Kubernetes kubelet cacheBasedManager 現在才知道 留了一個伏筆，就是 objectStore 到底是什麼？\n在 cacheBasedManager 的 GetObject 、 RegisterPod 以及 UnregisterPod 這三個 function 都有用到，想必 objectStore 一定是個狠角色吧，本篇將承接上一篇文章繼續探討 kubernetes kubelet 怎麼監控 configmap/secret 的底層實作，焦點會聚焦在 objectStore 的實作上。\ncacheBasedManager 我們先來回顧一下 cacheBasedManager 的資料結構定義，可以看到 objectStore 的資料型態是 Store，那這個型態是什麼？\n1 2 3 4 5 6 7 type cacheBasedManager struct { objectStore Store\t//主要用來儲存 reflector 觀測到的物件狀態 getReferencedObjects func(*v1.Pod) sets.String\t//主要用來找到 pod 內所有關聯的欄位，例如 configmap 就有可能出現在 envform configMapKeyRef 、 envform configMapRef 或是 volumes configMap lock sync.Mutex\t//可能有大量的 pod 同時塞入我們必須保證狀態的一致性 registeredPods map[objectKey]*v1.Pod\t//主要用來儲存哪個 pod 已經在觀測名單了 } 是之前講的 Kubernetes Indexers local cache的 store interface 嗎？ 恩\u0026hellip;.這裡並不是之前提到的 local store ，實際的定義我們往下看一下！\ntip: controller/operator 中 reflector 用到的是 cache.store， import 的 package 不一樣呦！\nStore interface 這裡的 Store 並不是給之前我們再討論 controller/operator 時候給 indexer 用的，這裡的 store 主要是給 cache base manager 建立，刪除以及取得 kubernets 物件的 reference 。\n如果對 controller/operator 的 store interface 有興趣可以回去複習一下這一篇Kubernetes Indexers local cache 之美，希望對你有幫助。\n好，回過頭來看 kubelet 中所定義的 store interface 到底是什麼吧！\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 type Store interface { // 使用者將 pod namespace 與 pod name 透過 addreference 實作的物件應該要增加 reference 以及建立對應的 reflector。 AddReference(namespace, name string) // 上一篇我們有談到當沒有任何物件使用 reference 時應該要呼叫 DeleteReference，實作的物件應該要幫我們把對應的 reflector 處理掉 DeleteReference(namespace, name string) // 當 cache base manager 要 kubernetes 物件資料時，會把物件的 namespace 與 name 帶入這個 function ，這裡會幫忙把物件取出。 Get(namespace, name string) (runtime.Object, error) } 單看定義僅能大概了解 store interface 要做的事情，我們還需要深入挖掘底層的實作 let\u0026rsquo;s do it !\nstruct 從 source code 上面的註解來看，可以知道這個 objectCache 是一個透過獨立 watcher 傳播物件的 local cache ，那到底是什麼意思呢？\n這裡先賣個關子先看 code 整理一下思緒會比較清楚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // objectCache is a local cache of objects propagated via individual watches. type objectCache struct { listObject listObjectFunc\t//列出 kubernetes 物件的過濾方法 watchObject watchObjectFunc\t//監控 kubernetes 物件的過濾方法 newObject newObjectFunc\t//reflector 預期要得到的物件型態為何的方法 isImmutable isImmutableFunc\t//如何判斷觀測的 kubernetes 物件是否屬於不可變的 groupResource schema.GroupResource\t//kubernetes 的GVK lock sync.RWMutex\t//讀寫所會用讀寫鎖主要因為讀多寫少（除非一直在加 pod 對應的 reflector） //上一篇有提過，cacheBasedManager 會透過解析用到物件名稱 secret/configmap name 以及 namespace 作為 Objectkey 傳入 //並且生成對應的 reflector items map[objectKey]*objectCacheItem\t//用來儲存 kubelet configmap/secret 各自對應的 reflector } 上面你可以能會有一個疑問， objectCacheItem 到底是什麼？ 總不能我說他是儲存各自對應的 reflector 他就是吧xD\nobjectCacheItem 我們接著來看為什麼 objectCacheItem 為什麼要儲存各自對應的 reflector\n在 objectCache struct 定義了一個 map，Key 為objectKey（resource name 加上 namespac），Value 為 objectCacheItem。 表示xxx-configmap對應xxx-objectCacheItem。 表示yyy-configmap對應yyy-objectCacheItem。 換句話說，也可以看成 表示aaa-secret對應aaa-objectCacheItem。 表示bbb-secret對應aaa-objectCacheItem。 如果有 nginxA 以及 nginxB 兩個 pod 同時都有 xxx-configmap該怎麼辦？ xxx-configmap 對應 xxx-objectCacheItem ， xxx-objectCacheItem 要記錄有兩個人 refference 到。 承上如果 nginx B 不再關注這個 xxx-configmap 要如何處理？ xxx-configmap 對應 xxx-objectCacheItem ， xxx-objectCacheItem 要記錄現在有只有一個人 refference 到。 承上如果 nginx A 也不再關注這個 xxx-configmap 要如何處理？ xxx-configmap 對應 xxx-objectCacheItem ， xxx-objectCacheItem 要記錄現在有沒有人 refference 到。 xxx-objectCacheItem 沒有人 reference 到的話，需要把 reflector 關閉減少資源的浪費該如何 帶著以上的方向我們來看 objectCacheItem 如何實作的吧！\nstruct 1 2 3 4 5 6 7 8 9 // objectCacheItem is a single item stored in objectCache. type objectCacheItem struct { refCount int\t//用來紀錄有多少人引用這個物件，當沒有人引用的時候就可以把 reflector 關掉了 store cache.Store\t//用來儲存 kubernetes 物件的 local stroage（這裡的 stroe 就是 controller/operaoter 會用到的 store 囉） hasSynced func() (bool, error)\t//用來確認 local stroage 有沒有同步 lock sync.Mutex\t//用來防止stop channel重複被呼叫 stopCh chan struct{}\t//用來傳遞關閉 reflector 的 channel } 當 object cache item 的 refCount 歸零，換句話說就是沒有人引用這個 reflector 了，就可以把對應的 reflector 關閉。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (i *objectCacheItem) stop() bool { //避免競爭上鎖 i.lock.Lock() defer i.lock.Unlock() //當使用者呼叫關閉 objectCacheItem.stop 時，第一次會進 default 的 select case 。 //關閉 stop channel ，此時 stop channel 會發出訊號給關聯的 reflector 關閉對 kubernetes resource 的追蹤。 select { case \u0026lt;-i.stopCh: // This means that channel is already closed. return false default: close(i.stopCh) return true } } new function 透過 NewObjectCache function 產出符合 store interface 的物件也就是會建立一個 ObjectCache 物件，這裡會把一些 function 帶入，例如要監控什麼物件他的 list 條件是什麼、他的 watch 條件是什麼以及 GVS 是什麼。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // NewObjectCache returns a new watch-based instance of Store interface. func NewObjectCache( listObject listObjectFunc,\t//使用者會傳入要怎麼列出物件 watchObject watchObjectFunc,\t//使用者會傳入要怎麼監控物件 newObject newObjectFunc,\t//reflector 要用的 object store是哪一個 isImmutable isImmutableFunc,\t//使用者會傳入怎麼判斷這個物件是不是 Immutable groupResource schema.GroupResource) Store {\t//使用者會傳入物件的GVS //最後 new fucntion 回傳實作 store interface 的 objectCache 物件 return \u0026amp;objectCache{ listObject: listObject, watchObject: watchObject, newObject: newObject, isImmutable: isImmutable, groupResource: groupResource, items: make(map[objectKey]*objectCacheItem), } } impliment 看完了 objectCache 物件的參數定義與如何 new 出物件後，接著就可以來了解實作的部分囉！\nAddReference 還記得在 cacheBasedManager 的 RegisterPod 的過程嗎？忘記的話我們簡單的複習一下\n簡單來說就是當 kubernetes assign 一個 pod 到 node 上的時候 ，會透過 cacheBasedManager 幫我們把 pod 內用到 secret/configmap 的地方選出來接著遞迴的呼叫 objectStore.AddReference 幫我們建立對應的 reflector 觀察 kubernetes 物件的變化。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (c *cacheBasedManager) RegisterPod(pod *v1.Pod) { //當 kubernetes assign 一個 pod 到 node 上的時候 //kubelet 會取得 pod 的相關資訊，getReferencedObjects 會將 pod spec 庖丁解牛 //檢查裡面有沒有我們要的欄位，目前追 code 只看到 configmap 與 secret 有用到 names := c.getReferencedObjects(pod) //防止競爭加鎖 c.lock.Lock() defer c.lock.Unlock() //針對 pod 內每一個用到的物件，例如 configmap 、 secret 建立一個對應的 reflector 用以觀察物件的變化 for name := range names { c.objectStore.AddReference(pod.Namespace, name) } cacheBasedManager 主要透過 AddReference 這個 function 來新增 reflector 關聯的數量，若是第一次建立的出現的reflector 就要建立忙建立對應的 reflector ～\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 func (c *objectCache) AddReference(namespace, name string) { //透過使用者傳入的 namespace 以及物件的名稱 封裝為 objectKey 作為唯一值 key := objectKey{namespace: namespace, name: name} //加鎖避免操作時產生競爭 c.lock.Lock() defer c.lock.Unlock() //此時會把剛剛封裝好的 objectKey 作為 key 放入 items map 儲存 //若發現 items map 沒有資料的話需要幫這個 objectKey 建立對應的 newReflector item, exists := c.items[key] if !exists { item = c.newReflector(namespace, name) c.items[key] = item } //最後再讓 item.refCount++ 的原因在於要記錄有多少物件共同使用這個 reflector 。 item.refCount++ } //來看看 newReflector 做了什麼吧 func (c *objectCache) newReflector(namespace, name string) *objectCacheItem { //先建立一個 fieldSelector 用來過濾 kubernetes 物件 fieldSelector := fields.Set{\u0026#34;metadata.name\u0026#34;: name}.AsSelector().String() //用 NewObjectCache 時建立的 list watch function //並且透過剛剛建立的 fieldSelector 過濾 kubernetes 物件 listFunc := func(options metav1.ListOptions) (runtime.Object, error) { options.FieldSelector = fieldSelector return c.listObject(namespace, options) } watchFunc := func(options metav1.ListOptions) (watch.Interface, error) { options.FieldSelector = fieldSelector return c.watchObject(namespace, options) } //這裡的 stroe 之前有介紹過，詳細內容可以參考 https://blog.jjmengze.website/posts/kubernetes/source-code/controller/indexer/kubernetes-indexers-local-cache-1/ //簡單來說就是一個以 index 去儲存的 local storage store := c.newStore() //建立一個 Reflector 詳細的內容也可以參考之前對 controller / Reflector 的分析 https://blog.jjmengze.website/posts/kubernetes/source-code/controller/redlector/reflector-1/ //簡單來就是建立對 kubernetes 物件的監聽，透過剛剛的 list watch function 去過濾 kubernetes 物件 reflector := cache.NewNamedReflector( fmt.Sprintf(\u0026#34;object-%q/%q\u0026#34;, namespace, name), \u0026amp;cache.ListWatch{ListFunc: listFunc, WatchFunc: watchFunc}, c.newObject(), store, 0, ) //建立一個 stop channel 用來終止 Reflector stopCh := make(chan struct{}) //啟動 reflector 去獲取對應的資源 go reflector.Run(stopCh) //最後將上面產出的物件封裝成 objectCacheItem ， object chache item 目前使用的 stoe 為 MetaNamespaceKeyFunc stoe // 目前使用這個 reflector 的物件為 0 個 ，可以透過 stop channel 關閉 reflector 。 return \u0026amp;objectCacheItem{ refCount: 0, store: store, hasSynced: func() (bool, error) { return reflector.LastSyncResourceVersion() != \u0026#34;\u0026#34;, nil }, stopCh: stopCh, } } //這裡的 stroe 之前有介紹過，詳細內容可以參考 https://blog.jjmengze.website/posts/kubernetes/source-code/controller/indexer/kubernetes-indexers-local-cache-1/ //簡單來說就是一個以 index 去儲存的 local storage func (c *objectCache) newStore() cache.Store { //object key 計算方法用預設的 MetaNamespaceKeyFunc return cache.NewStore(cache.MetaNamespaceKeyFunc) } 題外話我滿喜歡，上述 source code 中這一段處理方式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (c *objectCache) newReflector(namespace, name string) *objectCacheItem { ... listFunc := func(options metav1.ListOptions) (runtime.Object, error) { options.FieldSelector = fieldSelector return c.listObject(namespace, options) } reflector := cache.NewNamedReflector( fmt.Sprintf(\u0026#34;object-%q/%q\u0026#34;, namespace, name), \u0026amp;cache.ListWatch{ListFunc: listFunc, WatchFunc: ... 還記得我們在 newObjectCache 的時候有把 list object function 帶入嗎？那一個 list object function 到這裡才派上用， reflector 在用的時候會像是這樣的呼叫練。\n當 reflector 呼叫 list function 會先經由--------\u0026gt;newReflector裡面的listFunc\u0026lt;加了 field selector \u0026gt;進行處理 newReflector裡面的listFunc\u0026lt;加了 field selector \u0026gt;處理完後會再丟給--------\u0026gt; 使用者定義的 list watcher function 做最後的處理。 DeleteReference 透過這個 function 減少 reflector 關聯到的物件數量，當 reflector 沒有跟其他物件有所關聯時需要停止 reflector ~\n我們先來複習一下 cacheBasedManager 是在哪一段呼叫 objectCache 的 DeleteReference function 如何解除關聯的。\ncacheBasedManager 主要透過 UnregisterPod 這個 function 來刪除 reflector 關聯的數量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func (c *cacheBasedManager) UnregisterPod(pod *v1.Pod) { var prev *v1.Pod //以 pod spc 中的 namespace 與 pod name 封裝為 object key key := objectKey{namespace: pod.Namespace, name: pod.Name} //防止競爭加鎖 c.lock.Lock() defer c.lock.Unlock() //透過 pod spec 中的 namespace 與 pod name 作為 key 取得 registeredPods map 中對應的資料 prev = c.registeredPods[key] //透過 pod spec 中的 namespace 與 pod name 作為 key 刪除 registeredPods map 中對應的資料 delete(c.registeredPods, key) //如果有資料的話，要刪除對應的 `Reflector` ，這裡也是用到 getReferencedObjects 去解析 pod spec 的每個欄位 if prev != nil { for name := range c.getReferencedObjects(prev) { c.objectStore.DeleteReference(prev.Namespace, name) } } } cacheBasedManager 主要也是透過 getReferencedObjects 拿到 pod sepc 中有的物件交由 objectCache 刪除關聯的 reflector ，接著來看實際上怎麼刪除的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func (c *objectCache) DeleteReference(namespace, name string) { //透過 namespace 與 name 建立一個 object key 用以後續刪除對應的 reference key := objectKey{namespace: namespace, name: name} //有可能同時多個 thread 在操作所以要上鎖避免競爭 c.lock.Lock() defer c.lock.Unlock() //從 object cache 中的 map 透過 object key 找尋對應的物件 //map 中存的是 objectCacheItem ， object cache item 會記錄當前有多少 reflector if item, ok := c.items[key]; ok { //這個 function 是要刪除 Reference 所以要減少 object cache item 所管理的數量 item.refCount-- //如果 object cache item 已經沒有管理任何 reference 那就需要把最底層的 reflector 停掉 //並且透過 object key 從 object cache 中的 map 移除自己 if item.refCount == 0 { // Stop the underlying reflector. item.stop() delete(c.items, key) } } } Get 一樣我們先來看 cacheBasedManager 是如何使用的，這裡非常簡單直接把想要拿到 kubernetes 物件名稱與 namespace 交給 objectStore 去處理。\n1 2 3 func (c *cacheBasedManager) GetObject(namespace, name string) (runtime.Object, error) { return c.objectStore.Get(namespace, name) } 我們就接著來看 objectStore 是怎麼處理的 Get 的實作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 func (c *objectCache) Get(namespace, name string) (runtime.Object, error) { //透過 namespace 與物件名稱 name 建立一個 object key 用以後續從 map 中查詢對應的 reference key := objectKey{namespace: namespace, name: name} //有可能同時多個 thread 在進行 get 操作所以要上鎖避免競爭 c.lock.RLock() //先把資料讀出來就可以解鎖囉 item, exists := c.items[key] c.lock.RUnlock() //如果透過 object key 找不到對應的資料表示...阿就還沒加過 reference xD if !exists { return nil, fmt.Errorf(\u0026#34;object %q/%q not registered\u0026#34;, namespace, name) } //會立刻嘗試檢查 reflector 是不是已經同步了，關於怎麼確認是不是已經同步可以參考本篇文章 https://blog.jjmengze.website/posts/kubernetes/source-code/controller/deltafifo/kubernetes-delta-fifo-queue/#impliment if err := wait.PollImmediate(10*time.Millisecond, time.Second, item.hasSynced); err != nil { return nil, fmt.Errorf(\u0026#34;failed to sync %s cache: %v\u0026#34;, c.groupResource.String(), err) } //透過 object key 取得 object 怎麼取得的可以參考之前的文章，會從reflector 的 storage 透過 object key 把物件取出。 //這裡有個小 tip 就是我們 reflector 再把物件放入 store 的時候是透過 indexed function 計算後放入 store 的 //所以我們再取出的時候一樣要先透過 indexed function 算出物件的位置。 obj, exists, err := item.store.GetByKey(c.key(namespace, name)) if err != nil { return nil, err } if !exists { return nil, apierrors.NewNotFound(c.groupResource, name) } //因為 storage 儲存的是 interface 什麼東西都可以放進去，我們要先判物件是不是 runtime.Object 型態。 if object, ok := obj.(runtime.Object); ok { // If the returned object is immutable, stop the reflector. // // NOTE: we may potentially not even start the reflector if the object is // already immutable. However, given that: // - we want to also handle the case when object is marked as immutable later // - Secrets and ConfigMaps are periodically fetched by volumemanager anyway // - doing that wouldn\u0026#39;t provide visible scalability/performance gain - we // already have it from here // - doing that would require significant refactoring to reflector // we limit ourselves to just quickly stop the reflector here. //會先檢查有沒有開啟 FeatureGate ImmutableEphemeralVolumes //以及物件是否有明確標注處於 immutable = true ，若是有這兩種情況同時存在 //就停止監控 kubernetes 物件，因為物件已經處於 immutable 狀態 if utilfeature.DefaultFeatureGate.Enabled(features.ImmutableEphemeralVolumes) \u0026amp;\u0026amp; c.isImmutable(object) { if item.stop() { klog.V(4).Infof(\u0026#34;Stopped watching for changes of %q/%q - object is immutable\u0026#34;, namespace, name) } } return object, nil } return nil, fmt.Errorf(\u0026#34;unexpected object type: %v\u0026#34;, obj) } // 因為我們 storage 用的 index function 是 MetaNamespaceKeyFunc 所以我們要從 storage 把 object 拿出來的時候要符合 MetaNamespaceKeyFunc 的格式。 func (c *objectCache) key(namespace, name string) string { if len(namespace) \u0026gt; 0 { return namespace + \u0026#34;/\u0026#34; + name } return name } 回收伏筆 上面再說明 objectCache 時有賣一個關子，從註解來看 objectCache 是一個透過獨立 watcher 傳播物件的 local cache 。\nsource code\n1 2 3 4 5 // objectCache is a local cache of objects propagated via individual watches. type objectCache struct { listObject listObjectFunc\t//列出 kubernetes 物件的過濾方法 watchObject watchObjectFunc\t//監控 kubernetes 物件的過濾方法 ... 這裡終於可以回收為什麼 objectCache 是一個透過獨立 watcher 傳播物件的 local cache 了！\nlistObjectFunc 與 watchObjectFunc\n在執行 newReflector function 其實是把傳入的 listObjectFunc 與 watchObjectFunc 加了 fieldSelector 用來過濾 kubernetes 物件。\n對應到的 code 是以下這一段\nsoure code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func (c *objectCache) newReflector(namespace, name string) *objectCacheItem { //先建立一個 fieldSelector 用來過濾 kubernetes 物件 fieldSelector := fields.Set{\u0026#34;metadata.name\u0026#34;: name}.AsSelector().String() //用 NewObjectCache 時建立的 list watch function //並且透過剛剛建立的 fieldSelector 過濾 kubernetes 物件 listFunc := func(options metav1.ListOptions) (runtime.Object, error) { options.FieldSelector = fieldSelector return c.listObject(namespace, options) } watchFunc := func(options metav1.ListOptions) (watch.Interface, error) { options.FieldSelector = fieldSelector return c.watchObject(namespace, options) } newObjectFunc\n在執行 newReflector function 是把傳入的 newObjectFunc 交給 reflector package 的 NewNamedReflector function 去處理。\n對應到的 code 是以下這一段。\nsoure code 1 2 3 4 5 6 7 8 9 10 func (c *objectCache) newReflector(namespace, name string) *objectCacheItem { ... reflector := cache.NewNamedReflector( fmt.Sprintf(\u0026#34;object-%q/%q\u0026#34;, namespace, name), \u0026amp;cache.ListWatch{ListFunc: listFunc, WatchFunc: watchFunc}, c.newObject(), store, 0, ) ... isImmutableFunc\n最後要討論的就是在 Get function 會把 isImmutableFunc 作為判斷 object 是否為 Immutable 物件的方法。\n對應到的 code 是以下這一段。\nsoure code 1 2 3 4 5 6 7 if utilfeature.DefaultFeatureGate.Enabled(features.ImmutableEphemeralVolumes) \u0026amp;\u0026amp; c.isImmutable(object) { if item.stop() { klog.V(4).Infof(\u0026#34;Stopped watching for changes of %q/%q - object is immutable\u0026#34;, namespace, name) } } return object, nil } 小結 終於到了做結論的地方了，這章節比較繁瑣涉及到多個物件如何共用 objectCache ，這裡就牽扯到以下概念的實作。\n共用 objectCache 需要紀錄多少人（pod 中使用 configmap/secret 的各個欄位）使用 objectCacheItem 透過 map 的 key 去紀錄對應的使用者是誰， value 去紀錄 objectCacheItem 第一次建立的時候需要建立 objectCacheItem 並且啟動 reflector 當沒有人使用的時候需要回收 objectCacheItem 透過 stop channel 關閉 reflector 依賴 reflector 的資源監聽 需注入 list watch function 告知 reflector 如何過濾 需注入 obejct store 告知 reflector 在哪裡儲存 需注入 GVK 確保 reflector 的資料正確性（我不確定xD） 監聽的 resource 是否處於 Immutable 狀態 需要注入 isImmutable function 用來確認監聽的 resource 是否處於Immutable 狀態 以上的實作是為了前一章節提到 cacheBasedManager 解析完 pod spec 得到了 secret/configmap 的欄位後，後需要有人監聽個欄位對應的 kubernetes 資源的變化同時可能會有多個 pod spec 使用到相同的 secret/configmap 資源所 objectCache 以需要額外的處理。\n下一章節會開始進入 kubernetres configmap 與 cacheBasedManager 、objectCache 之間的愛恨情仇，文章中若有出現錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":14,"section":"posts","tags":["kubernetes","source-code"],"title":"Kubernetes kubelet cacheBasedManager 好喜歡 objectcache 的原因","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/kubelet/configmapsecret/kubernetes-kubelet-cacheobject/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n本章節將探討在 kubernetes 中每個 node 是如何得知 kubernetes 資源的變化，這裡的資源需要特別說明一下是指 configmap 與 secret ，我們都知道當 configmap 或是 secret 發生變化的時候只要重啟 pod （沒有設定 hot reload 的情況）就能得到最新的資料，這中間生了什麼事情 node 怎麼得知 configmap / secret 發生變化了？\n我們先把焦點移到 Manager interface 吧！\ninterface 這個 interface 主要定義了幾個方法這些方法，這些方法都是針對 pod 的資源我們就來看看定義了什麼吧。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 type Manager interface { // 通過 pod namespace 和 pod name 就能獲得相應的 kubernetes 物件 GetObject(namespace, name string) (runtime.Object, error) // Register function 主要是我們給這個 function 一個 pod spec 實作的物件需要產生對應的 reflector //以 pod 中用到 configmap 為例，RegisterPod 就需要產生 pod 內用到所得有的 configmap reflector RegisterPod(pod *v1.Pod) // UnregisterPod function 定義了當給定一個 pod spec 實作的物件必須把 pod 物件所用到 reflector 都消滅掉 // 當 pod 被刪除時， pod 中用到 configmap 為例，UnregisterPod 就需要 pod 內用到所得有的 configmap reflector 都移除 UnregisterPod(pod *v1.Pod) } 我們大概了解這個 interface 定義的方向後，就需要去了解哪個物件實作這個 interface 囉！\ncacheBasedManager 實作 manger interface 的是 cacheBasedManager 這個物件，我們看這物件定義了什麼吧！\nsource code\n1 2 3 4 5 6 7 8 type cacheBasedManager struct { objectStore Store\t//主要用來儲存 reflector 觀測到的物件狀態 getReferencedObjects func(*v1.Pod) sets.String\t//主要用來找到 pod 內所有關聯的欄位，例如 configmap 就有可能出現在 envform configMapKeyRef 、 envform configMapRef 或是 volumes configMap lock sync.Mutex\t//可能有大量的 pod 同時塞入我們必須保證狀態的一致性 registeredPods map[objectKey]*v1.Pod\t//主要用來儲存哪個 pod 已經在觀測名單了 } 了解了 cacheBasedManager 的屬性之後就可以來了解怎麼新增出這個物件。\nnew function new function 其實也很簡單，就是使用者傳什麼這裡接收什麼，沒有偷做事。\nsource code\n1 2 3 4 5 6 7 func NewCacheBasedManager(objectStore Store, getReferencedObjects func(*v1.Pod) sets.String) Manager { return \u0026amp;cacheBasedManager{ objectStore: objectStore, getReferencedObjects: getReferencedObjects, registeredPods: make(map[objectKey]*v1.Pod), } } 快速地了解怎麼建立起 cacheBasedManager 這個物件之後，我們接著要來了解實作層面做了什麼吧！\nimpliment GetObject 這一個實作其實也很簡單就是把請求委託給 objectStore 去拿到對應的物件， objectStore 的詳細實作我想保留在下一章節再介紹（會失焦QQ），簡單來說就是一個儲存空間可以從這個儲存空間拿到物件最新的狀態，這個狀態會是 runtime object 需要經過二次轉換成對應的物，例如： configmap secret 等等。\nsource code\n1 2 3 func (c *cacheBasedManager) GetObject(namespace, name string) (runtime.Object, error) { return c.objectStore.Get(namespace, name) } RegisterPod 這個實作的 function 比較複雜一點，需要透過 getReferencedObjects function 拆解 pod spec 取出對應的資料，這個 getReferencedObjects 可以抽換成 get configmap ReferencedObjects 的或是 secret ReferencedObjects 的。\n以 configmap ReferencedObjects 來說就可以會從 pod spec 拆解 envform configMapKeyRef 、 envform configMapRef 或是 volumes configMap 得到 pod spec 中 configmap 對應的名稱，透過這些名稱我們將建立相應的 reflector 。\n最後將 pod 的 namespace 與 pod name 作為 key 儲存在 registeredPods ，表示 kubelet 已經監控這個物件了。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func (c *cacheBasedManager) RegisterPod(pod *v1.Pod) { //當 kubernetes assign 一個 pod 到 node 上的時候 //kubelet 會取得 pod 的相關資訊，getReferencedObjects 會將 pod spec 庖丁解牛 //檢查裡面有沒有我們要的欄位，目前追 code 只看到 configmap 與 secret 有用到，等等看範例會比較容易理解 names := c.getReferencedObjects(pod) //防止競爭加鎖 c.lock.Lock() defer c.lock.Unlock() //針對 pod 內每一個用到的物件，例如 configmap 、 secret 建立一個對應的 reflector 用以觀察物件的變化 for name := range names { c.objectStore.AddReference(pod.Namespace, name) } //以 pod namespace 與 pod name 作為 key 儲存在 registeredPods 的 map 中 value 會儲存 pod 的資訊 //用來之後判斷 pod 更新哪些關聯到 reference 要更新或是刪除。 var prev *v1.Pod key := objectKey{namespace: pod.Namespace, name: pod.Name} prev = c.registeredPods[key] c.registeredPods[key] = pod // 如果發生某一個狀況，例如 pod 更新了 // registeredPods map 中會包含舊的 pod spec 也就是說 prev 的會有資料 //因為 pod 更新的時候有可能某些的資料已經不需要觀測了，例如 configmap 欄位沒用到了 //因此在這裡會判斷是否存在舊 pod 資料若是有的話會透過線透過舊資料的 pod name 與 namespace 作為 objectKey //以這個 objectkey 透過 getReferencedObjects function 找到舊 pod secret、configmap 用到的欄位 //最後透過 objectStore.DeleteReference 更新關聯到的 reference ，為什麼要更新呢？ //因為 pod 更新的時候有可能某些的資料已經不需要觀測了，例如 configmap 、 Secret 欄位沒用到了需要把沒用到 reference 刪掉。 if prev != nil { for name := range c.getReferencedObjects(prev) { c.objectStore.DeleteReference(prev.Namespace, name) } } } 要理解這裡的 code 我認為要搭配 test code 來互相配合會比較好理解，簡單來說就是當有 pod 來註冊，會將部分資訊提取出來並且生成 對應的 Reflector。\n底下為 test code 部分，重點在於 cacheBasedManager 的 getReferencedObjects 參數在做什麼，總結一句話的話就是過濾 pod spec 中的欄位，例如擷取 pod 中的 configmap 欄位或是 secret 欄位，我們直接來看 code 吧。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 //這個 function 等等會用來作爲 cacheBasedManager getReferencedObjects() //當 cacheBasedManager 呼叫 getReferencedObjects() 時會使用這一個 function 計算 pod 內用到 secret 的地方。 func getSecretNames(pod *v1.Pod) sets.String { //建立一個 set result := sets.NewString() //下面會看到實作，這裡就簡單的解釋一下這個 function 會遞迴解析 pod spec 中所有用到 secret 的欄位 //有用到的就全部透過 call back function 傳回來用 set 儲存。 podutil.VisitPodSecretNames(pod, func(name string) bool { result.Insert(name) return true }) //回傳 set return result } //傳入 store 物件用來儲存 kubernetes 物件狀態並且使用 getSecretNames function 作為 getReferencedObjects //建立一個實作 Manger 的 cacheBasedManager物件 func newCacheBasedSecretManager(store Store) Manager { return NewCacheBasedManager(store, getSecretNames) } //模擬 pod 內有的 secret 欄位 type secretsToAttach struct { imagePullSecretNames []string containerEnvSecrets []envSecrets } func TestCacheInvalidation(t *testing.T) { ... // 這裡的 store 就先把它當作 一個儲存物件狀態的地方就好 就好 store := newSecretStore(fakeClient, fakeClock, noObjectTTL, time.Minute) //建立一個實作 Manger 的 cacheBasedManager 物件 manager := newCacheBasedSecretManager(store) // 模擬一個 pod 有 secret 的地方，這裡模擬 pod spec 中友 image pull secret 以及 env secret s1 := secretsToAttach{ imagePullSecretNames: []string{\u0026#34;s1\u0026#34;}, containerEnvSecrets: []envSecrets{ {envVarNames: []string{\u0026#34;s1\u0026#34;}, envFromNames: []string{\u0026#34;s10\u0026#34;}}, {envVarNames: []string{\u0026#34;s2\u0026#34;}}, }, } //分成兩階段來看 podWithSecrets(\u0026#34;ns1\u0026#34;, \u0026#34;name1\u0026#34;, s1) 這裡就是建立一個 pod spec // pod 名稱為 name1 , namespace 為 ns1 最後把 剛剛模擬 secret 的地方放入 pod spec // 第二階段為將 pod spec 註冊到 cacheBasedManager 中，這裡就可以參考上面提到的 RegisterPod 時會針對每個欄位建立對應的 Reference manager.RegisterPod(podWithSecrets(\u0026#34;ns1\u0026#34;, \u0026#34;name1\u0026#34;, s1)) // Fetch both secrets - this should trigger get operations. // 以下就不屬於本文要討論的範疇，還是簡單的過水一下 // 這裡直接觸發這裡直接觸發 store get 表示 store 拿到這個物件的變化 // 分別拿到 namespace ns1 的 s1 變化 s10 變化以及 s2的變化 store.Get(\u0026#34;ns1\u0026#34;, \u0026#34;s1\u0026#34;) store.Get(\u0026#34;ns1\u0026#34;, \u0026#34;s10\u0026#34;) store.Get(\u0026#34;ns1\u0026#34;, \u0026#34;s2\u0026#34;) //最後透過 kubernetes mock 出來的 client 看看是否觀察到三次的變化。 actions := fakeClient.Actions() assert.Equal(t, 3, len(actions), \u0026#34;unexpected actions: %#v\u0026#34;, actions) // 清除 mock client 的變化，也就是重新計署的意思 fakeClient.ClearActions() ... } source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 // VisitPodSecretNames invokes the visitor function with the name of every secret // referenced by the pod spec. If visitor returns false, visiting is short-circuited. // Transitive references (e.g. pod -\u0026gt; pvc -\u0026gt; pv -\u0026gt; secret) are not visited. // Returns true if visiting completed, false if visiting was short-circuited. func VisitPodSecretNames(pod *v1.Pod, visitor Visitor) bool { //透過 for range 遞迴 pod spec 中的 ImagePullSecrets 欄位 for _, reference := range pod.Spec.ImagePullSecrets { //把找到的名稱傳入 visitor function 中，還記得上面有提過的 visitor function 嗎？ //上面 visitor function 做的事情就是將 name 存入 set 回傳 true if !visitor(reference.Name) { return false } } //下面會看到實作方式這裡就簡短的解釋一下，透過 for range 遞迴 pod spec 中的 continaer 的欄位中會出現 secret 的欄位 //若是欄位有數值就傳入 visitor function 將 name 存入 set 。 VisitContainers(\u0026amp;pod.Spec, AllContainers, func(c *v1.Container, containerType ContainerType) bool { return visitContainerSecretNames(c, visitor) }) //恩...我覺得在這裏定了這個沒有什麼特別的意義．．．就是等等用來承載 pod spec 中 VolumeSource 欄位的數值 //放到 for 迴圈裡面應該也行吧？xD var source *v1.VolumeSource // 透過for 迴圈遞迴 pod spec 中的 volumes欄位，這裡可以看到各式各樣的volume 例如 Ceph Cinder Flex // 這些都有可能會用到 secret 我們需要一個一個檢視，若是有找到 secret 就要傳入 visitor function 儲存在 set 中。 for i := range pod.Spec.Volumes { source = \u0026amp;pod.Spec.Volumes[i].VolumeSource //由於 volume 種類眾多我這邊只挑幾個來說明 switch { // 如果VolumeSource的欄位是 Azure file 的話就需要進一步判斷 // 底下的 secret name 欄位，若是有這個欄位就把裡面的數值傳入 visitor function // 透過 visitor function 儲存在 set 中。 case source.AzureFile != nil: if len(source.AzureFile.SecretName) \u0026gt; 0 \u0026amp;\u0026amp; !visitor(source.AzureFile.SecretName) { return false } // 如果 VolumeSource 的欄位是 CephFS 的話就需要進一步判斷 // 底下的 secret name 欄位，若是有這個欄位就把裡面的數值傳入 visitor function // 透過 visitor function 儲存在 set 中。 case source.CephFS != nil: if source.CephFS.SecretRef != nil \u0026amp;\u0026amp; !visitor(source.CephFS.SecretRef.Name) { return false } ... //其他實作方式都差不多，有興趣的小夥伴可以回 source code code base 看看 } } return true } func VisitContainers(podSpec *v1.PodSpec, mask ContainerType, visitor ContainerVisitor) bool { if mask\u0026amp;InitContainers != 0 { for i := range podSpec.InitContainers { if !visitor(\u0026amp;podSpec.InitContainers[i], InitContainers) { return false } } } if mask\u0026amp;Containers != 0 { for i := range podSpec.Containers { if !visitor(\u0026amp;podSpec.Containers[i], Containers) { return false } } } if mask\u0026amp;EphemeralContainers != 0 { for i := range podSpec.EphemeralContainers { if !visitor((*v1.Container)(\u0026amp;podSpec.EphemeralContainers[i].EphemeralContainerCommon), EphemeralContainers) { return false } } } return true } UnregisterPod 顧名思意就是反註冊 pod ，那到底是反註冊什麼呢？在上面我們有提到 RegisterPod 就是遞迴 pod spec 的每個欄位（ configmap / secret ），以及 pod spec 中的 namespace 與 pod name 作為 key 儲存在 registeredPods 的 map。\n反過來說反註冊就是要遞迴 pod spec 的每個欄位（ configmap / secret ），並且透過 pod spec 中的 namespace 與 pod name 作為 key 刪除 registeredPods map 中對應的資料，廢話就不多說了來 code 吧。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func (c *cacheBasedManager) UnregisterPod(pod *v1.Pod) { var prev *v1.Pod //以 pod spc 中的 namespace 與 pod name 封裝為 object key key := objectKey{namespace: pod.Namespace, name: pod.Name} //防止競爭加鎖 c.lock.Lock() defer c.lock.Unlock() //透過 pod spec 中的 namespace 與 pod name 作為 key 取得 registeredPods map 中對應的資料 prev = c.registeredPods[key] //透過 pod spec 中的 namespace 與 pod name 作為 key 刪除 registeredPods map 中對應的資料 delete(c.registeredPods, key) //如果有資料的話，要刪除對應的 `Reflector` ，這裡也是用到 getReferencedObjects 去解析 pod spec 的每個欄位 //上面有提過 getReferencedObjects 如果不熟悉的小夥伴可以往上滑去找找 if prev != nil { for name := range c.getReferencedObjects(prev) { c.objectStore.DeleteReference(prev.Namespace, name) } } } 結論 本篇文章主要我們了解了 cacheBasedManager 透過 Register function 將一個 pod spec 物件拆解並且交由 objectStore 產生對應的 reflector 。以 pod 中用到 configmap 為例，RegisterPod 就需要產生 pod 內用到所得有的 configmap 並且交由 objectStore 產生對應的 reflector 。\ncacheBasedManager 透過 UnregisterPod function 將一個 pod spec 把 pod 物件所用到 reflector 都消滅掉，以 configmap 為例，UnregisterPod 就需要 pod 內用到所得有的 configmap reflector 協同 objectStore 都移除。\ncacheBasedManager 透過 GetObject function 通過 pod namespace 和 pod name 經由 objectStore 的協助我們就能從 objectStore 獲得相應的 kubernetes 物件。\n上述有提到 objectStore 主要用來儲存 reflector 觀測到的物件狀態，下一篇文章會解析 objectStore 的實作，看 cacheBasedManager 把 pod 內的欄位解析完後，怎麼把需要的資料整理成 reflector 。\n文章中若有出現錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":15,"section":"posts","tags":["kubernetes","source-code"],"title":"Kubernetes kubelet configmap \u0026 secret 與 cacheBasedManager 激情四射","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/kubelet/configmapsecret/kubernetes-kubelet-cachebasedmanager/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n本篇文章仍然跟前幾章節 使用 Backoff 指數大漲 以及 使用 Backoff 抖了一下 有著密切的關係，繼續把這條任務解完，以下文章需要前兩章的知識作為鋪墊若有不了解的地方可以透過連結回到前兩章複習相關概念，廢話不多說就直接開始吧！\nBackoffUntil 這個 BackoffUntil function 做的事情很簡單，就是透過相隔 backoff manager 所給定的時間觸發使用者所指定的 function ，若是 stop channel 被關閉了就結束整個 BackoffUntil function 的生命週期，來看在 kubernetes是如何實作的吧。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 // BackoffUntil loops until stop channel is closed, run f every duration given by BackoffManager. // // If sliding is true, the period is computed after f runs. If it is false then // period includes the runtime for f. func BackoffUntil(f func(), backoff BackoffManager, sliding bool, stopCh \u0026lt;-chan struct{}) { var t clock.Timer //先定義一個空的 timer 等等用來接 backoffmanger 算出的 timer for { //無限迴圈開始 select { case \u0026lt;-stopCh: //當收到 stop channel 時關閉整個 BackoffUntil function 的生命週期 return default: //防止 channel卡住 } //如果 sliding 不開啟的話，會先從 BackoffManger 算出 timer //仔細看下面的話會發現 timer 的時間會包含執行使用者所指定的 function 後才等待 timer 的 ticker if !sliding { t = backoff.Backoff() } //這裡的做法就是卻跑每次執行使用者所指定的 Function 後都會跑入 defer確保panic 處理 func() { defer runtime.HandleCrash() f() }() //那一個if !sliding 有點類似 ，差別在於這裡是執行完使用者所指定的 function 後才算出 timer ， timer 等待時間不包含執行使用者的 function if sliding { t = backoff.Backoff() } //在 golang select channel 有個小問題 //當 select A channel , B channel , C channel 時 //A B C channel 都剛剛好都有訊號同時到達那 select channel 會選哪一個呢？ //在 golang 的世界中答案是隨機的， A B C channel 哪一個都有可能被選到xD //當然 kubernetes 的開發者們一定也知道這個問題，在這裡就有了相應的註解 //我這裡就保留原始的註解，整段註解的大意大概是如果 stop channel 與 ticker channel 同時到達 //因為golnag select chennel 機制剛好選中 ticker channel 那會造成使用者指定的 function 多跑一次，這樣是不符合預期的行為。 //因此在for loop 的一開始會判斷 stop channel 是否有訊號 //用來防止 stop channel 與 ticker channel 同時到達並且golang select channel 剛好選中 ticker 的問題 // NOTE: b/c there is no priority selection in golang // it is possible for this to race, meaning we could // trigger t.C and stopCh, and t.C select falls through. // In order to mitigate we re-check stopCh at the beginning // of every loop to prevent extra executions of f(). select { case \u0026lt;-stopCh: return case \u0026lt;-t.C(): } } } example 範例簡單的帶一下怎麼使用這個 function ，我們把重點看在 wait.BackoffUntil 就好其他就先不要管，用法十分簡單直接來看 code。\n下面的 source code 可能會有些小夥伴覺得很熟悉，對這裡就是之前花了滿大一個篇幅在介紹的 kubenretes controler ，我們可以再複習一下相關的用法！\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (r *Reflector) Run(stopCh \u0026lt;-chan struct{}) { klog.V(2).Infof(\u0026#34;Starting reflector %s (%s) from %s\u0026#34;, r.expectedTypeName, r.resyncPeriod, r.name) // wait.BackoffUntil 要求輸入一個 function 後 wait.BackoffUntil 會間隔 backoffManager 所算出的時間 // 週期性的執行輸入的 function ，如果接收到 stopCh 的訊號就退出 // 在這裡輸入的 function 就是 Reflector 的 listAndWatch 了 // 複習一下 listwatch 負責觀測 kubernetes 資源例如 pod , configmap ,secret .e.t.c wait.BackoffUntil(func() { if err := r.ListAndWatch(stopCh); err != nil { r.watchErrorHandler(r, err) } }, r.backoffManager, true, stopCh) klog.V(2).Infof(\u0026#34;Stopping reflector %s (%s) from %s\u0026#34;, r.expectedTypeName, r.resyncPeriod, r.name) } JitterUntil 剛剛看了 BackoffUntil function 主要是透過 backoff manger 計算出 ticker 時間，並且依據是否收到 stop channel 作為是否要結束 BackoffUntil function 的生命週期。\n在 kubernetes 中開發者通常不會直接使用 BackoffUntil function 而是使用 jitter 讓所要執行的 function 在 backoff manger 計算時多考慮 jitter 的抖動，實作上也相當簡單，一來Backoff Manger 的實作抽換在這裡抽換成 NewJitteredBackoffManager 。\nsource code\n1 2 3 4 5 6 func JitterUntil(f func(), period time.Duration, jitterFactor float64, sliding bool, stopCh \u0026lt;-chan struct{}) { //BackoffUntil 使用方法在上一小節有說明過，還不了解的朋友請向上滑動複習 //NewJitteredBackoffManager 在前兩個章節有詳細說明 //所以這裡的 timer 計算實作是由 JitteredBackoffManager 實作的，其餘的用法都是一樣。 BackoffUntil(f, NewJitteredBackoffManager(period, jitterFactor, \u0026amp;clock.RealClock{}), sliding, stopCh) } exmaple 範例簡單的帶一下怎麼使用這個 function ，我們把重點看在 wait.JitterUntil 就好其他就先不要管，用法十分簡單直接來看 code。\n範例是一個 LeaderElector 的 acquire 去定時的取得 kubernetes 上的資源鎖，細節在本篇不去探討有興趣的小夥伴我在早期的文章 kubernetes 分散式資源鎖 有分享過怎麼在 kubernetes 內實作一個分散是資源鎖，有興趣的可以去看看。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func (le *LeaderElector) acquire(ctx context.Context) bool { ctx, cancel := context.WithCancel(ctx) defer cancel() succeeded := false desc := le.config.Lock.Describe() klog.Infof(\u0026#34;attempting to acquire leader lease %v...\u0026#34;, desc) // 這裡直接指定一個 function ，在這裡是一個匿名函數做的事情大概是嘗試取得/更新物件 // 從輸入的幾個參數來看就是每 le.config.RetryPeriod 執行一次 嘗試取得/更新物件的匿名函數 // 間隔時間的抖動因子由 JitterFactor 決定 ，這裡就決定了最大的間隔時間了 // 這個 function Sliding （Sliding = true）表示 backofftime 包含了執行使用者自訂的 function 時間。 // 以及最後 context.done 的部份決定了，這個 wait.JitterUntil function 的生命週期跟著整個 process 而不是某一個 channel 訊號。 wait.JitterUntil(func() { succeeded = le.tryAcquireOrRenew(ctx) le.maybeReportTransition() if !succeeded { klog.V(4).Infof(\u0026#34;failed to acquire lease %v\u0026#34;, desc) return } le.config.Lock.RecordEvent(\u0026#34;became leader\u0026#34;) le.metrics.leaderOn(le.config.Name) klog.Infof(\u0026#34;successfully acquired lease %v\u0026#34;, desc) cancel() }, le.config.RetryPeriod, JitterFactor, true, ctx.Done()) return succeeded } NonSlidingUntil NoSlidingUntil function 是對 JitterUntil function 進行簡單的封裝，我們很簡單的帶過去，主要就是把 Sliding 的功能強制關掉，忘記 Sliding是什麼的我們簡單複習一下。\n當禁用 Sliding （Sliding = false）表示 backofftime 不包含了執行使用者自訂的 function 時間。\nsource code\n1 2 3 func NonSlidingUntil(f func(), period time.Duration, stopCh \u0026lt;-chan struct{}) { JitterUntil(f, period, 0.0, false, stopCh) } exmaple 範例簡單的帶一下怎麼使用這個 function ，我們把重點看在 wait.NonSlidingUntil 就好其他就先不要管，用法十分簡單直接來看 code。\n範例是一個cloud provider 的 RouteController 去定時的執行 reconcileNodeRoutes 也就是同步節點上的路由資訊，細節我們不去深入研究把焦點擺在怎麼使用這個工具就好。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func (rc *RouteController) Run(stopCh \u0026lt;-chan struct{}, syncPeriod time.Duration) { defer utilruntime.HandleCrash() klog.Info(\u0026#34;Starting route controller\u0026#34;) defer klog.Info(\u0026#34;Shutting down route controller\u0026#34;) ... // 這裡透過 goroutine 直接指定一個 function ，在這裡就是 rc.reconcileNodeRoutes() 並且判斷執行的結果 // 若是有錯誤就 log 出相關的錯誤訊息 // 從輸入的幾個參數來看就是每 syncPeriod 執行一次 rc.reconcileNodeRoutes() ，如果 stop channel 收到訊息那 就會結束 wait.Until 的生命週期 //使用 wait.NonSlidingUntil 這個 function 表示表示 backofftime 不包含了執行使用者自訂的 function 時間。 go wait.NonSlidingUntil(func() { if err := rc.reconcileNodeRoutes(); err != nil { klog.Errorf(\u0026#34;Couldn\u0026#39;t reconcile node routes: %v\u0026#34;, err) } }, syncPeriod, stopCh) \u0026lt;-stopCh } Until Unti function 也是對 JitterUntil function 進行簡單的封裝，我們很簡單的帶過去，主要就是把 Sliding 的功能強制開啟，忘記 Sliding 是什麼的我們簡單複習一下。\n當啟用 Sliding （Sliding = true）表示 backofftime 包含了執行使用者自訂的 function 時間。\nsource code\n1 2 3 func Until(f func(), period time.Duration, stopCh \u0026lt;-chan struct{}) { JitterUntil(f, period, 0.0, true, stopCh) } example 範例簡單的帶一下怎麼使用這個 function ，我們把重點看在 wait.Until 就好其他就先不要管，用法十分簡單直接來看 code。\n範例是一個 CRDFinalizer 去啟動指定的 work 讓他工作，\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func (c *CRDFinalizer) Run(workers int, stopCh \u0026lt;-chan struct{}) { defer utilruntime.HandleCrash() defer c.queue.ShutDown() klog.Infof(\u0026#34;Starting CRDFinalizer\u0026#34;) defer klog.Infof(\u0026#34;Shutting down CRDFinalizer\u0026#34;) if !cache.WaitForCacheSync(stopCh, c.crdSynced) { return } // 這裡透過 goroutine 直接指定一個 function ，在這裡就是 c.runWorker // 從輸入的幾個參數來看就是每秒執行一次 c.runWorker ，如果 stop channel 收到訊息那 就會結束 wait.Until 的生命週期 for i := 0; i \u0026lt; workers; i++ { go wait.Until(c.runWorker, time.Second, stopCh) } \u0026lt;-stopCh } Forever Forever function 是對 Until 進行簡單的封裝，簡單來看就是執行一個永遠不會中止且定時觸發的 function。\nsource code\n1 2 3 4 var NeverStop \u0026lt;-chan struct{} = make(chan struct{}) func Forever(f func(), period time.Duration) { Until(f, period, NeverStop) } example source code\n範例簡單的帶一下怎麼使用這個 function ，我們把重點看在 wait.Forever 就好其他就先不要管，用法十分簡單直接來看 code。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func (l *log) Check(_ *http.Request) error { l.startOnce.Do(func() { l.lastVerified.Store(time.Now()) // 這裡透過 goroutine 直接指定一個 function // function 的內容看說來就是把 log flush以及 做時間的儲存。 // 並且這個 function 每一分鐘執行一次。 go wait.Forever(func() { klog.Flush() l.lastVerified.Store(time.Now()) }, time.Minute) }) lastVerified := l.lastVerified.Load().(time.Time) if time.Since(lastVerified) \u0026lt; (2 * time.Minute) { return nil } return fmt.Errorf(\u0026#34;logging blocked\u0026#34;) } JitterUntilWithContext 這就表示定時會呼叫使用者指定的 function ，若是外面有人將 context cancel 掉，那就會結束 wait.JitterUntilWithContext 的生命週期。基本上複用了 JitterUntil 讓使用者可以自行決定 Sliding 與 jitterFactor 。\n1 2 3 func JitterUntilWithContext(ctx context.Context, f func(context.Context), period time.Duration, jitterFactor float64, sliding bool) { JitterUntil(func() { f(ctx) }, period, jitterFactor, sliding, ctx.Done()) } example 範例簡單的帶一下怎麼使用這個 function ，我們把重點看在 wait.JitterUntilWithContext 就好其他就先不要管，用法十分簡單直接來看 code。\n在這格 function 透過 context.WithCancel(context.Background()) 啟動了一個 context，接著拿到 endpoint slice 透過 for 迴圈遞迴每個 endpoint ，透過 goroutine 呼叫 wait.JitterUntilWithContext 並且帶入檢查 endpoint status 的 function ，當啟用 Sliding （Sliding = true）表示 backofftime 包含了執行使用者自訂的 function 時間。\n這就表示定時會呼叫指定的 function ，這個例子就是檢查 endpoint status 。若是外面有人將 context cancel 掉，那就會結束 wait.JitterUntilWithContext 的生命週期。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func startDBSizeMonitorPerEndpoint(client *clientv3.Client, interval time.Duration) (func(), error) { ... ctx, cancel := context.WithCancel(context.Background()) for _, ep := range client.Endpoints() { if _, found := dbMetricsMonitors[ep]; found { continue } dbMetricsMonitors[ep] = struct{}{} endpoint := ep klog.V(4).Infof(\u0026#34;Start monitoring storage db size metric for endpoint %s with polling interval %v\u0026#34;, endpoint, interval) go wait.JitterUntilWithContext(ctx, func(context.Context) { epStatus, err := client.Maintenance.Status(ctx, endpoint) if err != nil { klog.V(4).Infof(\u0026#34;Failed to get storage db size for ep %s: %v\u0026#34;, endpoint, err) metrics.UpdateEtcdDbSize(endpoint, -1) } else { metrics.UpdateEtcdDbSize(endpoint, epStatus.DbSize) } }, interval, dbMetricsMonitorJitter, true) } return func() { cancel() }, nil } UntilWithContext 這就表示定時會呼叫使用者指定的 function ，若是外面有人將 context cancel 掉，那就會結束 wait.UntilWithContext 的生命週期。基本上複用了 JitterUntilWithContext 差別在每此重新呼叫 function 的間隔時間有沒有抖動而已。\n1 2 3 func UntilWithContext(ctx context.Context, f func(context.Context), period time.Duration) { JitterUntilWithContext(ctx, f, period, 0.0, true) } exmaple 很可惜 kubernetes 內沒有使用這個 function 我們可以透過 test code 來觀摩怎麼使用這個 function 。\n題外話我覺得這個測試寫得真不錯，沒想過可以這樣測試 ！！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 func TestUntilWithContext(t *testing.T) { //建立 with cancel 的 context ctx, cancel := context.WithCancel(context.TODO()) //直接種 context 的生命週期 cancel() //這時候在透過 UntilWithContext 呼叫我們自定義的 function //記得....因為 context 生命週期已經死掉了照理來說不會觸發我們自訂的 function 。 UntilWithContext(ctx, func(context.Context) { t.Fatal(\u0026#34;should not have been invoked\u0026#34;) }, 0) //建立 with cancel 的 context ctx, cancel = context.WithCancel(context.TODO()) //建立取消的 channel called := make(chan struct{}) //啟動一個 go routine ，主要透過異步的經由 UntilWithContext 定時呼叫自定義的 function //因為 main thread 會卡在 \u0026lt;-called，當執行到 goroutine 時會送訊號到 called channel，讓外部收到 go func() { UntilWithContext(ctx, func(context.Context) { called \u0026lt;- struct{}{} }, 0) //發送完訊號後就關閉 channel close(called) }() //收到 goroutine才會往下繼續執行 \u0026lt;-called //關閉 context 的生命週期讓 UntilWithContext 不再執行我們自定義的 function cancel() //收到關閉 channel 的訊號 \u0026lt;-called } 小結 Kubernetes 內好用工具非常非常多讓我們不需要造輪子，但是開發者要記住的一個鐵則是盡信書不如無書，前人為我們鋪好的路我們需要了解是怎麼鋪路的，有沒有更好的工法。承襲著前人的智慧將問題以更快速且更好的方式解決。\n上述分享的內容中間可能會有錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":16,"section":"posts","tags":["kubernetes","utils"],"title":" Kubernetes util tool 常用的 backoff 組合技","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/utils/kubernetes-utils-tool-wait-backoff/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\nSafeWaitGroup 這一段在先前一版的文章中理解錯誤，在明天將補上新版，請大家見諒。\nsource code 1 2 3 4 5 6 7 8 9 // SafeWaitGroup must not be copied after first use. type SafeWaitGroup struct { wg sync.WaitGroup mu sync.RWMutex // wait indicate whether Wait is called, if true, // then any Add with positive delta will return error. wait bool } ... wiat 在 kubernetes 中比起 package waitgroup 從 source code 中更長看到使用者使用 package wait ，在這一小節中會看一些範例了解 kubernetes 中哪裡有用到這一個 package。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // package group 很簡單就是嵌入了一個 go startand librart 的 sync.WaitGroup type Group struct { wg sync.WaitGroup } // StartWithChannel 可以透過 stopCh 關閉在這個 goroutine group 中的啟動function。 func (g *Group) StartWithChannel(stopCh \u0026lt;-chan struct{}, f func(stopCh \u0026lt;-chan struct{})) { g.Start(func() { f(stopCh) }) } // StartWithContext 可以透過 Context 關閉在這個 goroutine group 中的啟動 function。 func (g *Group) StartWithContext(ctx context.Context, f func(context.Context)) { g.Start(func() { f(ctx) }) } // 這裡就是對 sync.WaitGroup 進行簡單的封裝，上述兩種 StartWithContext 與 StartWithChannel // 都會呼叫 start function 增加 WaitGroup 的 wg 與 執行 goroutine func (g *Group) Start(f func()) { g.wg.Add(1) go func() { defer g.wg.Done() f() }() } // 透過 Wait function 等待所有透過 goroutine group 啟動的 function 執行完畢也就是 執行 defer g.wg.Done() func (g *Group) Wait() { g.wg.Wait() } example 分成兩個範例來看，第一個是單純跑 Group 的 Start function 。\n這個 function 裡面在做什麼我們先不深入研究，把焦點放在 Group 的 Start function 與 Wait function 就好。\n可以看到一開始透過匿名函數使用 for 迴圈經由 wg.Start 啟動了 n 個 goroutine ，接著卡在 stop channel 。\n後續若是收到 stop channel 的話可以簡單地理解成工作要收工了，因此後面再用一個 for 迴圈把所有 listeners 的 add channel 關閉，最後透過 wg.Wait() 等待所有 channel 把關閉工作完成才退出function 。\nwg.Start 與 wg.wait 整體來說使用上不複雜可以配合多種情境，例如範例所示範的開啟多個 worker 等待 worker 完成。\nTips 至於為什麼不用 StartWithChannel 來處理範例中的 listener.run listener.pop ，好問題我也不知道xDDD\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func (p *sharedProcessor) run(stopCh \u0026lt;-chan struct{}) { func() { p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { p.wg.Start(listener.run) p.wg.Start(listener.pop) } p.listenersStarted = true }() \u0026lt;-stopCh p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { close(listener.addCh) // Tell .pop() to stop. .pop() will tell .run() to stop } p.wg.Wait() // Wait for all .pop() and .run() to stop } 接著來看另外一個範例 wg.StartWithChannel ，範例這個 function 裡面在做什麼我們先不深入研究，把焦點放在 Group 的 StartWithChannel function 與 Wait function 就好。\n可以看到一開始透過 Run function 離開時第一個觸發的是 close processor Stop Channel。\n接著觸發的 defer 是 wg.Wait() 這裡就會等待所有的透過 wait.Group 啟動的 goroutine 完成才結束工作。\n可以簡單地理解成廣播說下課，要等所有同學把課本收好才可以離開，而老師就站在前面看著大家收拾。就可以把 wg 想像成老師負責監督大家只要有人沒收好東西就不准最下一步，processorStopCh 可以想成學校的廣播，當廣播發號司令同學就要做某某事情。\nsource code\n1 2 3 4 5 6 7 8 9 10 func (s *sharedIndexInformer) Run(stopCh \u0026lt;-chan struct{}) { defer utilruntime.HandleCrash() ... var wg wait.Group defer wg.Wait() // Wait for Processor to stop defer close(processorStopCh) // Tell Processor to stop wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) wg.StartWithChannel(processorStopCh, s.processor.run) ... } 小結 簡單來說 package wait 只是對 go startand librart 進行簡單的封裝，帶來的效益就是讓大多數的開發者可以沿用這這個封裝好的 package ，因為要達到某一個功能實作的方式有很多種，有了適當的封裝能讓開發風格更統一。文章中若有出現錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":17,"section":"posts","tags":["kubernetes","utils"],"title":"Kubernetes util tool 使用 wait.waitgroup 抓住 goroutine ","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/utils/kubernetes-utils-tool-wiat/"},{"content":" 還記得前一章討論 Controller 的時候，我們保留了一個 Reflector ， Reflector 將會在本篇中會揭開它神秘的面紗，就讓我來解剖這個元件吧！\nReflector 上一章節可以看到 Reflector 的初始化是在 Controller 的 Run Function ，我們來複習一次 Run Function 。\nController——Run Run function 是 Controller 最主要的 function\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 // Run begins processing items, and will continue until a value is sent down stopCh or it is closed. // It\u0026#39;s an error to call Run more than once. // Run blocks; call via go. func (c *controller) Run(stopCh \u0026lt;-chan struct{}) { //錯誤處理，有機會再來談，先不理他 defer utilruntime.HandleCrash() //當收到stop 訊號關閉 delta fifo queue go func() { \u0026lt;-stopCh //關閉 delta fifo queue 的細節我們前幾章節在討論 delta fifo queue 已經談過，不了解的小夥伴可以回去複習 c.config.Queue.Close() }() //建立一個 Reflector ，今天的重點會著重於 Reflector內部的實作 r := NewReflector( c.config.ListerWatcher, //傳入資源監控器 c.config.ObjectType, //傳入欲監視的物件型態 c.config.Queue, //傳入delta fifo queue c.config.FullResyncPeriod, //設定多久要sync一次 ) //Reflector 設定 resync 時間 r.ShouldResync = c.config.ShouldResync //Reflector 設定list watch 的 chunk size. r.WatchListPageSize = c.config.WatchListPageSize //Reflector 設定時間 r.clock = c.clock //Reflector 套用錯誤處理 if c.config.WatchErrorHandler != nil { r.watchErrorHandler = c.config.WatchErrorHandler } // 不知道為什麼 controller 綁定 reflector 的時候要加鎖 c.reflectorMutex.Lock() // controller 綁定 Reflector c.reflector = r c.reflectorMutex.Unlock() //kubernetes wait.Group 預計未來還會拉出來再講一篇 //簡單來就是被這個 wg 管理的 thread 全部都 done 了之後才會退出 wait var wg wait.Group //這個 function 會啟動一個 thread 並且在裡面呼叫 剛剛建立的 reflector.run 並且傳入 stop channel //stop channel用來終止 thread wg.StartWithChannel(stopCh, r.Run) //規律性的呼叫 processLoop()，若是收到 stop channel 的訊號就退出 //processLoop()在上一篇有討論過，不了解的朋友可以回到上一章節看 wait.Until(c.processLoop, time.Second, stopCh) //等待所有 wait.Group 的 thread done 才能離開，不然會一直卡在這裡～ wg.Wait() } Anyway ,總之 Controller 在執行 Run Function 的時候初始化了 Reflector ，這時候我們要去看看 NewReflector function 會不會偷藏了其他初始化， ＧＯ ！\nNew function 剛剛我們看到 Controller 透過 Run function ，並且執行 NewReflector() 去建立一個 Reflector ，可以從下面的 source code 看到實際上會透過 NewNamedReflector 去建立 Reflector，我們就把重點把在 NewNamedReflector 吧！\n1 2 3 4 5 6 7 8 func NewReflector(lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { return NewNamedReflector( naming.GetNameFromCallsite(internalPackages...), //Reflector log 要印出的名稱 lw, //資源監控器 expectedType, //預期資源監控器要拿到物件 store, //delta fifo queue resyncPeriod) //多久要 sync 一次 } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 // NewNamedReflector same as NewReflector, but with a specified name for logging //傳入 log 要印出的名稱 //傳入監控物件變化的觀察者 //傳入要觀察的物件資料型態 //傳入本地儲存器(local storage) //傳入多久要同步一次 func NewNamedReflector(name string, lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { realClock := \u0026amp;clock.RealClock{} r := \u0026amp;Reflector{ name: name, //log 要印出的名稱 listerWatcher: lw, //監控物件變化的觀察者 store: store, //要觀察的物件資料型態 //BackoffManager 主要設計減少 upstream 不健康期間的負載。 //這裡實作有點複雜我們只要先知道，透過 BackoffManager 管理 listwatch 多久要觸發一次 backoffManager: wait.NewExponentialBackoffManager(800*time.Millisecond, 30*time.Second, 2*time.Minute, 2.0, 1.0, realClock), //當 listerwatcher list發生問題時 //透過 initConnBackoffManager 管理多久後再重新 list 一次 initConnBackoffManager: wait.NewExponentialBackoffManager(800*time.Millisecond, 30*time.Second, 2*time.Minute, 2.0, 1.0, realClock), resyncPeriod: resyncPeriod, //多久要 sync 一次 clock: realClock, //給測試用的時鐘 //當lister watcher watcher 階段發生錯誤的時候錯誤處理 watchErrorHandler: WatchErrorHandler(DefaultWatchErrorHandler), } //設定 refleactor 預期要觀察的物件 r.setExpectedType(expectedType) return r } //本篇不討論反射機制這裡我們只了解透過反射我們設定了 Refector 預期要看的資源 //設定 Reflector 想要看的資源（e.g. pod configmap） func (r *Reflector) setExpectedType(expectedType interface{}) { //透過反射先設定 （e.g. pod configmap） r.expectedType = reflect.TypeOf(expectedType) //如果有問題就設定為預設值 if r.expectedType == nil { r.expectedTypeName = defaultExpectedTypeName return } //不太了解為什麼要這樣設計，特地要把 expectedType 轉 string 存在 r.expectedTypeName //直接拿r.expectedType.string()不好用嗎？ r.expectedTypeName = r.expectedType.String() //檢查expected type 的GVK (group version kind 未來我還會再開一篇討論 GVK ) if obj, ok := expectedType.(*unstructured.Unstructured); ok { // Use gvk to check that watch event objects are of the desired type. gvk := obj.GroupVersionKind() if gvk.Empty() { klog.V(4).Infof(\u0026#34;Reflector from %s configured with expectedType of *unstructured.Unstructured with empty GroupVersionKind.\u0026#34;, r.name) return } //設定reflator 想要看到GVK r.expectedGVK = \u0026amp;gvk //不太了解為什麼要這樣設計，特地要把 GVK 轉 string 存在 r.expectedTypeName，直接拿r.expectedGVK.string()不好用嗎？ r.expectedTypeName = gvk.String() } } 看完了如何建立一個 Reflector 後我們來看看 Reflector 的資料結構長怎麼樣\nStruct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 //Reflector 預期解析的物件沒有設定的話就使用這個名字 const defaultExpectedTypeName = \u0026#34;\u0026lt;unspecified\u0026gt;\u0026#34; // Reflector watches a specified resource and causes all changes to be reflected in the given store. type Reflector struct { name string //定義 Reflector 的名稱 expectedTypeName string //一般來說會設定成GVK.String（）的名稱 expectedType reflect.Type //Reflector 要解析的物件 //當 expectedType 跟 watcher 監控的物件一樣時才會放到 delta fifo queue expectedGVK *schema.GroupVersionKind //Reflector 要解析的物件的GVK //當 expectedType 跟 watcher 監控的物件一樣時才會放到 delta fifo queue store Store // 存放物件變化的 delta FIFO Queue listerWatcher ListerWatcher //用以監控與列出指定的資源 backoffManager wait.BackoffManager //透過BackoffManager 管理 listwatch 多久要觸發一次 initConnBackoffManager wait.BackoffManager //當 listerwatcher list發生問題時 //透過 initConnBackoffManager 管理多久後再重新 list 一次 resyncPeriod time.Duration //設定多久 delta fifo queue 要 reqync 一次的時間 ShouldResync func() bool //定期會呼叫ShouldResync確認 delta fifo queue 是否同步了 clock clock.Clock //給測試用的 paginatedResult bool //如果 listerwatcher list 的結果是有做分頁的話，該數值標記為true。 lastSyncResourceVersion string //lister watcher觀測到到物件版本會記錄在這 isLastSyncResourceVersionUnavailable bool //當有\u0026#34;expired\u0026#34; 或是 \u0026#34;too large resource version\u0026#34;出現的時候 //會在標記在這裡 lastSyncResourceVersionMutex sync.RWMutex //對版本的讀寫鎖 WatchListPageSize int64 //在 lister watcher list 資源時用來做分頁大小切割的參數 watchErrorHandler WatchErrorHandler //當 lister watcher 斷開連結時會透過這個 function 處理 } 看完了資料結構我們就可以進入到一下個階段 Reflector 到底做了什麼，就讓我們繼續把 Reflector 攤開來看！\nimpliment Run 還記得誰呼叫了 Reflector 的 Run Function 嗎？\n是在 Controller 的 Run Function 喔，複習下 controller 的 run function 做了什麼事情吧（controller 的實作上一張有介紹過，不了解的地方可以回頭複習看看）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func (c *controller) Run(stopCh \u0026lt;-chan struct{}) { ... r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) ... var wg wait.Group wg.StartWithChannel(stopCh, r.Run) wait.Until(c.processLoop, time.Second, stopCh) wg.Wait() } Reflector 被建立起後從這個 Run Function 開始他的一生，我們來看看 Reflector Run Fucntion 到底做了什麼吧！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Run repeatedly uses the reflector\u0026#39;s ListAndWatch to fetch all the // objects and subsequent deltas. // Run will exit when stopCh is closed. func (r *Reflector) Run(stopCh \u0026lt;-chan struct{}) { //這裡先不管 wait.BackoffUntil 底層是如何實作的，未來再開戰場來講這個 //我們只要知道 wait.BackoffUntil 會週期性(每隔backoffManager)的呼叫 傳入的 function //如果接收到 stopCh 的訊號就退出 wait.BackoffUntil( func() { if err := r.ListAndWatch(stopCh); err != nil { r.watchErrorHandler(r, err) } }, r.backoffManager, true, stopCh) } 在 Reflector Run Function 會定期的執行 ListAndWatch ，當收到 Stop Channel 發過來的訊號的時候才會結束我們接著看 ListAndWatch 做了什麼。\nListAndWatch list and watch 會先列出資源所有的 item 然後以最大的 resource version 開始進行 watch 的動作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 // ListAndWatch first lists all items and get the resource version at the moment of call, // and then use the resource version to watch. // It returns error if ListAndWatch didn\u0026#39;t even try to initialize watch. func (r *Reflector) ListAndWatch(stopCh \u0026lt;-chan struct{}) error { var resourceVersion string //簡單來說就是一開始把 lsit 的過濾條件 resource version 設定成 0 options := metav1.ListOptions{ResourceVersion: r.relistResourceVersion()} if err := func() error { // 承裝 lister watcher 列出的物件 var list runtime.Object // 確定 list 結果是否有分頁 var paginatedResult bool var err error //監聽 list 事件是否完成 listCh := make(chan struct{}, 1) //監聽 list 事件是否有 error panicCh := make(chan interface{}, 1) //啟動 thread 執行 list 動作 go func() { //捕捉錯誤 ，發給監聽錯誤的 channel defer func() { if r := recover(); r != nil { panicCh \u0026lt;- r } }() //主要是建立一個 ListWatcher 的分頁處理器 pager := pager.New(pager.SimplePageFunc(func(opts metav1.ListOptions) (runtime.Object, error) { return r.listerWatcher.List(opts) })) //設定pager相關的參數 switch { //設定chunk size case r.WatchListPageSize != 0: pager.PageSize = r.WatchListPageSize //若是 list 的結果有分頁的話 case r.paginatedResult: // 當有同時設定ResourceVersion且ResourceVersion!=0的時候 case options.ResourceVersion != \u0026#34;\u0026#34; \u0026amp;\u0026amp; options.ResourceVersion != \u0026#34;0\u0026#34;: //不啟用分頁 pager.PageSize = 0 } //透過 pager.List 檢索 (list) 出指定的資源，並透過 options 過濾\u0026lt;過程很複雜...有機會再來看\u0026gt; //我們會得到 list 結果型態是 runtime.Object //並且拿到回傳的資料是否有做分頁以及相關錯誤 list, paginatedResult, err = pager.List(context.Background(), options) //處理一些已知的錯誤 例如 StatusReasonExpired , StatusReasonGone 等等，緣由可以看一下原 source code 的註解（歷史因素） if isExpiredError(err) || isTooLargeResourceVersionError(err) { //標記有出現過 StatusReasonExpired , StatusReasonGone 等等的錯誤 r.setIsLastSyncResourceVersionUnavailable(true) //簡單來說就是退回第零版再重新list一次 list, paginatedResult, err = pager.List(context.Background(), metav1.ListOptions{ResourceVersion: r.relistResourceVersion()}) } //表示資源檢索(list)完成，透過 channel 發送訊號 close(listCh) }() //阻塞操作，等 list 完成或是觸發 panic error ，或者接收到 stop channel 的訊號終止 select { case \u0026lt;-stopCh: return nil case r := \u0026lt;-panicCh: panic(r) case \u0026lt;-listCh: } if err != nil { return fmt.Errorf(\u0026#34;failed to list %v: %v\u0026#34;, r.expectedTypeName, err) } //如果 resource 為 0 並且 list 結果的 paginatedResult 也表示資料有分頁 //就要標記 Reflector 的結果是有分頁的 if options.ResourceVersion == \u0026#34;0\u0026#34; \u0026amp;\u0026amp; paginatedResult { r.paginatedResult = true } //標記 list 成功 r.setIsLastSyncResourceVersionUnavailable(false) // list was successful //把 list 出的結果轉換成實作 List 的物件（這邊很繞牽扯到apimachinery），先了解意思就好 listMetaInterface, err := meta.ListAccessor(list) if err != nil { return fmt.Errorf(\u0026#34;unable to understand list result %#v: %v\u0026#34;, list, err) } //取得 list 資料內 metadata 的 resourceVersion ，得知當前版本 resourceVersion = listMetaInterface.GetResourceVersion() //把檢索出來的物件取出 items 的欄位會得到[]runtime.Object，例如裡面就是存 [podA{},podB{},e.t.c] items, err := meta.ExtractList(list) if err != nil { return fmt.Errorf(\u0026#34;unable to understand list result %#v (%v)\u0026#34;, list, err) } //同步到DeltaFIFO內，下面會看到如何同步的不急 if err := r.syncWith(items, resourceVersion); err != nil { return fmt.Errorf(\u0026#34;unable to sync list result: %v\u0026#34;, err) } //設定從etcd同步過來的最新的版本 r.setLastSyncResourceVersion(resourceVersion) return nil }(); err != nil { return err } //////////////////以上為 lister watcher lister 的過程，也就是說 reflector 會先完成 list 的工作！ //處理 delta fifo queue 同步錯誤用的channel，非阻塞 resyncerrc := make(chan error, 1) //watcher 處理結束用的channel cancelCh := make(chan struct{}) defer close(cancelCh) go func() { //建立同步用的channel，時間到會從 channel 發出訊號 resyncCh, cleanup := r.resyncChan() defer func() { cleanup() //關閉同步用的channel }() for { //等待同步訊號，stop channel 或是 cancel channel 都是用來監聽關閉的訊號 // resyncCh 則是會被定時觸發 select { case \u0026lt;-resyncCh: case \u0026lt;-stopCh: return case \u0026lt;-cancelCh: return } // ShouldResync 是一個 function //用來用來確定 Delta FIFO Queue 是否已經同步 if r.ShouldResync == nil || r.ShouldResync() { //執行 Delta FIFO Queue 的 resync //不了解的小夥伴可以到之前的章節複習 if err := r.store.Resync(); err != nil { //若是 Delta FIFO Queue 的 resync 執行 就丟到外面 channel 這裡不處理 resyncerrc \u0026lt;- err return } } //關閉同步用的channel cleanup() //綁定新的同步用的channel resyncCh, cleanup = r.resyncChan() } }() ////////////////以上這一小段是定時確認 Delta FIFO QUEUE 同步過程是否有問題 for { // give the stopCh a chance to stop the loop, even in case of continue statements further down on errors //stop channel 收到訊號表示外部要關閉 reflactor 直接退出 select { case \u0026lt;-stopCh: return nil // 讓 for 迴圈不會卡住 default: } //watch timeout 時間 minWatchTimeout 為 300 秒 timeoutSeconds := int64(minWatchTimeout.Seconds() * (rand.Float64() + 1.0)) //設定 watch 過濾條件 options = metav1.ListOptions{ ResourceVersion: resourceVersion, //watch 某一個版本以上的 resource TimeoutSeconds: \u0026amp;timeoutSeconds, //設定watch timeout 時間 AllowWatchBookmarks: true, //用以降低 api server 附載用的...有空再展開來看為什麼可以降低附載 } // start the clock before sending the request, since some proxies won\u0026#39;t flush headers until after the first watch event is sent start := r.clock.Now() //透過 watcher 監控指定的資源，並且透過指定過濾條件進行過濾 w, err := r.listerWatcher.Watch(options) if err != nil { //簡單來說當遇到ConnectionRefused時候會透過initConnBackoffManager，來停等一下 //停等之後再重新 watch 試試看 if utilnet.IsConnectionRefused(err) { \u0026lt;-r.initConnBackoffManager.Backoff().C() continue } return err } //處理 watcher 監控到的資源，下面會看到實作的方法 if err := r.watchHandler(start, w, \u0026amp;resourceVersion, resyncerrc, stopCh); err != nil { //判斷一下錯誤...但沒有特別處理， if err != errorStopRequested { switch { case isExpiredError(err):\tklog.V(4).Infof(\u0026#34;%s: watch of %v closed with: %v\u0026#34;, r.name, r.expectedTypeName, err) default: klog.Warningf(\u0026#34;%s: watch of %v ended with: %v\u0026#34;, r.name, r.expectedTypeName, err) } } return nil } } } // 回傳一個定時器的 channel ，以及關閉定時器的方法 func (r *Reflector) resyncChan() (\u0026lt;-chan time.Time, func() bool) { if r.resyncPeriod == 0 { return neverExitWatch, func() bool { return false } } t := r.clock.NewTimer(r.resyncPeriod) return t.C(), t.Stop } // 透過給定的物件與版本號替換掉 delta fifo queue的資料 func (r *Reflector) syncWith(items []runtime.Object, resourceVersion string) error { //複製一份 list 列出來的所有物件 到 found found := make([]interface{}, 0, len(items)) for _, item := range items { found = append(found, item) } // delta fifo queue 進行替換 return r.store.Replace(found, resourceVersion) } list 的過程可以參考下面的流程圖\nwatch 的過程可以參考下面的流程圖\nReflector 的 Run function 使用到 apimachinery package 的相關方法，主要是對 runtime object 的物件進行處理 但 apimachinery package 是一個相當複雜的東西\u0026hellip;使得上述再分析 Run Function 的細節時不夠清楚與透徹，以及像是 paginatedResult 與 chunkSize 等不是說明的很清楚未來有機會的應該會再開一篇來討論一下相關的機制。\n掩面\u0026hellip;..還需要多努力\nwatchHandler 我們在 Run function 有看到再處理 watch 事件的時候呼叫了 watchHandler ，我們需要看一下 watchHandler 做了什麼事情！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 // watchHandler watches w and keeps *resourceVersion up to date. func (r *Reflector) watchHandler(start time.Time, w watch.Interface, resourceVersion *string, errc chan error, stopCh \u0026lt;-chan struct{}) error { //事件計數器 eventCount := 0 // Stopping the watcher should be idempotent and if we return from this function there\u0026#39;s no way // we\u0026#39;re coming back in with the same watch interface. //離開時結束對資源的監控 defer w.Stop() //標記，當遇到某些情況時，可以跳到這個標記再重新進入 for 迴圈 loop: for { select { //收到關閉訊號，就跳出並帶有結束的錯誤訊息 case \u0026lt;-stopCh: return errorStopRequested //收到其他thread傳來的錯誤訊息，跳出並回報錯誤 case err := \u0026lt;-errc: return err //收到 watcher 傳來的事件變動通知 case event, ok := \u0026lt;-w.ResultChan(): //channel 有分關閉跟非關閉，若是 watcher 關閉的話需要再重新跑一次繼續監聽 if !ok { break loop } //解析 watcher 事件（ watcher 產生 error） if event.Type == watch.Error { return apierrors.FromObject(event.Object) } //解析 watcher 事件 ，透過反射確定 watcher 監控的是我們指定的資源 if r.expectedType != nil { if e, a := r.expectedType, reflect.TypeOf(event.Object); e != a { utilruntime.HandleError(fmt.Errorf(\u0026#34;%s: expected type %v, but watch event object had type %v\u0026#34;, r.name, e, a)) continue } } //解析 watcher 事件 ，確定 watcher 觀察的事件是我們指定的 GVK (Group Version Kind) if r.expectedGVK != nil { if e, a := *r.expectedGVK, event.Object.GetObjectKind().GroupVersionKind(); e != a { utilruntime.HandleError(fmt.Errorf(\u0026#34;%s: expected gvk %v, but watch event object had gvk %v\u0026#34;, r.name, e, a)) continue } } //跟上面 ListAndWatch 提到的 meta.Accessor 是一樣的功能 //總之就是拿到事件的 metaData meta, err := meta.Accessor(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;%s: unable to understand watch event %#v\u0026#34;, r.name, event)) continue } //從 meta data 中取得對應的資源版本，之後 watch 就是要 watch 比這個版本還要新的資源 newResourceVersion := meta.GetResourceVersion() //判斷事件的型態 switch event.Type { //當型態為 Add 的時候觸發 delta FIFO Queue 的 add 行為 case watch.Added: err := r.store.Add(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;%s: unable to add watch event object (%#v) to store: %v\u0026#34;, r.name, event.Object, err)) } //當型態為 Modified 的時候觸發 delta FIFO Queue 的 Update 行為 case watch.Modified: err := r.store.Update(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;%s: unable to update watch event object (%#v) to store: %v\u0026#34;, r.name, event.Object, err)) } //當型態為 Deleted 的時候觸發 delta FIFO Queue 的 Deleted 行為 case watch.Deleted: err := r.store.Delete(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;%s: unable to delete watch event object (%#v) from store: %v\u0026#34;, r.name, event.Object, err)) } //不清楚為什麼bookmark 可以降低使用附載...待深入瞭解後補上(ＴＯＤＯ) case watch.Bookmark: // A `Bookmark` means watch has synced here, just update the resourceVersion //當回傳的事件不是以上幾種的話，當然就要報錯 default: utilruntime.HandleError(fmt.Errorf(\u0026#34;%s: unable to understand watch event %#v\u0026#34;, r.name, event)) } //紀錄當前事件版本號 *resourceVersion = newResourceVersion r.setLastSyncResourceVersion(newResourceVersion) if rvu, ok := r.store.(ResourceVersionUpdater); ok { rvu.UpdateResourceVersion(newResourceVersion) } //計數事件發生次數 eventCount++ } } //當 watch 回應的時間非常短且沒任何事件，表示這是一個異常現象 //這裡 kubernetes 設計了一個超時機制～ watchDuration := r.clock.Since(start) if watchDuration \u0026lt; 1*time.Second \u0026amp;\u0026amp; eventCount == 0 { return fmt.Errorf(\u0026#34;very short watch: %s: Unexpected watch close - watch lasted less than a second and no items received\u0026#34;, r.name) } klog.V(4).Infof(\u0026#34;%s: Watch close - %v total %v items received\u0026#34;, r.name, r.expectedTypeName, eventCount) return nil } func (r *Reflector) setLastSyncResourceVersion(v string) { //同步鎖防止競爭 r.lastSyncResourceVersionMutex.Lock() defer r.lastSyncResourceVersionMutex.Unlock() //紀錄reflector目前最新資源版本號 r.lastSyncResourceVersion = v } 整理思路 大多數的朋友讀到這裡可能還是會一頭問號到底 reflector 跟 controller 之間是什麼關係，之間又有什麼愛恨情仇？\n我想還是透過一些圖示說明加深印象可能會比較好一點\n小結 對 Kubernetes Reflector 做一下總結， 在 Controller 把 Reflector 建立出來後利用 Reflector.Run 執行 list 、 watch 以及向 delta fifo enqueue 的動作。\n其中 list 是透過 apiserver 的 client 列出所有的物件例如 pod,configmagp 等等 (版本為0以後的物件全部列出來) ， list 出來的物件透過 delta fifof 的 replace function 同步到 DeltaFIFO 中，最後紀錄 list 出來的最新版本號，\n這個版本號會在 watch 的步驟中用用到。\n接著開啟一個 thread 定期的執行 Delta FIFO 的 Resync ，這裡要注意的是如果沒有設定 ShouldResync 就不會執行定時做 Resync，對同步還不熟悉的小夥伴可以看我之前分享的 Kubernetes DeltaFIFO 承上啟下。\n最後透過 kubernetes apiserver 的 client watch kubernetes 物件資源的變化，監控的某一版本後的物件資源變化，一旦監控的 kubernetes 物件資源發生變化 例如 add 、 update 、 delete 的變化， watcher 就會根據觀測到 kubernetes 物件資源變化的類型( add 、 update 、 delete ) 呼叫 DeltaFIFO 的對應的 function ，例如新增加一個物件就會觸發 delta fifo 的 add function ，接著產生一個相應的 Delta 並且丟入到 delta fifo queue 中，同時更新當前資源的版本，watch 更新的資源版本。\n我認為這邊有點複雜牽扯到 apimachinery 反序列化的過程，中間可能會有錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":18,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes Reflector 我在盯著你 （ III  ）","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/redlector/reflector-3/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n幾本上這一塊相當的複雜與龐大，有些部分我認為不需要深入理解其運作的機制，我們就來看看一個 Reflector 是透過哪裡零件組裝起來的吧！\n上一篇 Kubernetes Reflector 我在盯著你 （ I ）一文章有提到 client go 的範例建立 informer 的過程，在 NewIndexerInformer 的過程中最後回傳的是 一個 indexer 以及一個 實作 Controller interface 的物件，本篇會展開討論 Controller 到底是什麼\n先來回顧一下 client go 建立 controller 的範例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 indexer, informer := cache.NewIndexerInformer(podListWatcher, \u0026amp;v1.Pod{}, 0, cache.ResourceEventHandlerFuncs{ ... }, cache.Indexers{}) func NewIndexerInformer( lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration, h ResourceEventHandler, indexers Indexers, ) (Indexer, Controller) { // This will hold the client state, as we know it. clientState := NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers) return clientState, newInformer(lw, objType, resyncPeriod, h, clientState) } 從上述的source code中可以看到透過 NewIndexerInformer function 最後會回傳 indexer 以及 controller ， indexer 在先前的章節已經討論過，還不了解的小夥伴可以到本篇文章Kubernetes Indexers local cache 之美 （I）複習相關知識，以下會針對 controller 的實作進行琢磨。\nController 我們先從 Controller 定義了什麼行為開始探討\ninterface 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // Controller is a low-level controller that is parameterized by a // Config and used in sharedIndexInformer. type Controller interface { // Run does two things. One is to construct and run a Reflector // to pump objects/notifications from the Config\u0026#39;s ListerWatcher // to the Config\u0026#39;s Queue and possibly invoke the occasional Resync // on that Queue. The other is to repeatedly Pop from the Queue // and process with the Config\u0026#39;s ProcessFunc. Both of these // continue until `stopCh` is closed. Run(stopCh \u0026lt;-chan struct{}) //執行一個函數，並且透過stopchannel來決定是否跳出 // HasSynced delegates to the Config\u0026#39;s Queue HasSynced() bool //檢查觀測到 object 是不是同步到 indexer 了 // LastSyncResourceVersion delegates to the Reflector when there // is one, otherwise returns the empty string LastSyncResourceVersion() string //觀測到最新的 object version } 從上面的行為應該是看不出來 controller 要做什麼吧！沒關係我們繼續順藤摸瓜，看看這葫蘆裡賣的是什麼藥。\nnew function 我們透過 new function 來了解實作 controller interface 的物件需要什麼參數！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 //傳入的參數我們在上一章節有看過，這邊再來複習一次 //傳入監控物件變化的觀察者 //傳入要觀察的物件資料型態 //傳入多久要同步一次 //傳入事件處理器 //傳入本地儲存器(local storage) func newInformer( lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration, h ResourceEventHandler, clientState Store, ) Controller { //建立delta fifo queue ，之前章節有探討過delta fifo queue //不了解的讀者可以回去複習一次 fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions{ KnownObjects: clientState, EmitDeltaTypeReplaced: true, }) //建立controller config的設定檔 cfg := \u0026amp;Config{ Queue: fifo, //使用delta fifo queue ListerWatcher: lw, //使用觀測哪個物件的 listwatch(e.g. pod configmap e.t.c) ObjectType: objType, //觀測物件的資料型態(e.g. pod configmap e.t.c) FullResyncPeriod: resyncPeriod, //多久要重同步一次 RetryOnError: false, //錯誤是否要重試 Process: func(obj interface{}) error { //事件處理器 // from oldest to newest ... }, } return New(cfg) //建立一個實作 controller interface 的物件 } // New makes a new Controller from the given Config. func New(c *Config) Controller { ctlr := \u0026amp;controller{ //建立一個實作 controller 的物件 config: *c, clock: \u0026amp;clock.RealClock{}, } return ctlr } 從上面看起來就是放入了一些設定（deltafifo , listerwatch, objecttype）接著把實作 controller interface 的物件產出，那實作 controller interface 的物件資料結構長怎麼樣呢？我們接著來看！\nstruct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 // `*controller` implements Controller type controller struct { config Config //contrller 相關設定 reflector *Reflector //下一章節會展開來看，本章節暫時用不到 reflectorMutex sync.RWMutex //Reflector 讀寫鎖，本章節暫時用不到 clock clock.Clock //同步用 } // Config contains all the settings for one of these low-level controllers. type Config struct { Queue //這裡要設定 controller 用的 DeltaFIFO Queue //前幾個章節有詳細的說明,不了解的朋友可以回去複習。 ListerWatcher //前一個章節有帶到，實作監視以及列出特定的資源的物件 Process ProcessFunc //當物件從 DeltaFIFO Queue 彈出，處理事件的function // ObjectType is an example object of the type this controller is // expected to handle. Only the type needs to be right, except // that when that is `unstructured.Unstructured` the object\u0026#39;s // `\u0026#34;apiVersion\u0026#34;` and `\u0026#34;kind\u0026#34;` must also be right. ObjectType runtime.Object //這個我認為很難理解，要告訴controller即將到來的物件是什麼例如 pod , deployment e.t.c. FullResyncPeriod time.Duration //多久要 resync 一次 ShouldResync ShouldResyncFunc //reflector會定期透過ShouldResync function來確定是否重新同步queue， RetryOnError bool //如果為true，則Process（）返回錯誤時，需要requeue object。 //看註解這是有爭議的，有些開發者認為要拉到更該高的層次決掉 error的處理方式 WatchErrorHandler WatchErrorHandler //每當ListAndWatch斷開連接並出現錯誤時會呼叫這個 function 處理。 WatchListPageSize int64 //初始化時設定list watch 的 chunk size. } //reflector會定期透過ShouldResync function來確定是否重新同步queue， type ShouldResyncFunc func() bool //每當ListAndWatch斷開連接並出現錯誤時調用。 type WatchErrorHandler func(r *Reflector, err error) Config 資料結構內有些資料結構是提供給 reflector 做使用，關於 reflector 的細節我想在未來的章節在展開來討論，本篇會專注於 controller 會用到的參數。\n從上面的資料結構大致上可以看出來 一個 controller 需要這幾個東西\nDeltaFIFO\n負責將觀測到的資料放入 FIFO queue，並且標記變化量(Add , Delete , Update e.t.c)\n會把資料存進 localcache (indexer)\nProcessFunc\n處理 DeltaFIFO 的事件變化，例如當 Delta Pop 出一個 Add 事件，會由 ProcessFunc 處理， Pop 出 Update 會由 ProcessFunc 處理依此類推。\nruntime.Object\nController 知道等等pop出來的是什麼物件要如何反序列化\nListerWatcher\n列出與監控某一個物件\n看到這裡我們還要了解 controller 底層實作了什麼，怎麼把上面提到的這四個元素組合再一起使用。\nimpliment Controller 實作以下幾個 function ，我們一個一個來看！\nRun Run function 是最主要的 function\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 // Run begins processing items, and will continue until a value is sent down stopCh or it is closed. // It\u0026#39;s an error to call Run more than once. // Run blocks; call via go. func (c *controller) Run(stopCh \u0026lt;-chan struct{}) { //錯誤處理，有機會再來談，先不理他 defer utilruntime.HandleCrash() //當收到stop 訊號關閉 delta fifof queue go func() { \u0026lt;-stopCh c.config.Queue.Close() }() //建立一個 Reflector ，下一章節會展開討論 Reflector 先不理他 r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) //Reflector 設定 resync 時間 r.ShouldResync = c.config.ShouldResync //Reflector 設定list watch 的 chunk size. r.WatchListPageSize = c.config.WatchListPageSize //Reflector 設定時間 r.clock = c.clock //Reflector 套用錯誤處理 if c.config.WatchErrorHandler != nil { r.watchErrorHandler = c.config.WatchErrorHandler } // todo 我不知道為什麼 controller 綁定 reflector 的時候要加鎖 c.reflectorMutex.Lock() // controller 綁定 Reflector c.reflector = r c.reflectorMutex.Unlock() //wait.Group 預計未來還會拉出來再講一篇 //簡單來就是被這個 wg 管理的 thread 全部都 done 了之後才會退出 wait var wg wait.Group //這個function 會啟動一個 thread 並且在裡面呼叫 剛剛建立的 reflector.run 並且傳入 stop channel //stop channel用來終止 thread wg.StartWithChannel(stopCh, r.Run) //規律性的呼叫processLoop()，若是收到 stop channel 的訊號就退出 wait.Until(c.processLoop, time.Second, stopCh) //等待所有 wait.Group 的 thread done 才能離開，不然會一直卡在這裡～ wg.Wait() } //會被wait.Until 規律性的呼叫 func (c *controller) processLoop() { for { //從 deltafifo pop 出物件， //pop 出的事件會交給 config.Process function 處理 obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil { //如果 deltafifo queue關閉就退出 if err == ErrFIFOClosed { return } //如果處理發生錯誤就重新加回 delta fifo queue 中 if c.config.RetryOnError { // This is the safe way to re-enqueue. c.config.Queue.AddIfNotPresent(obj) } } } } HasSynced 1 2 3 4 5 6 7 // Returns true once this controller has completed an initial resource listing //簡單來說就是看Delta FIFO 是不是把資料同步完了 func (c *controller) HasSynced() bool { //委任給Delta FIFO QUEUE的HasSynced() //不了解的部分可以到前面的章節看一下 Delta FIFO Queue 是怎麼做的 return c.config.Queue.HasSynced() } LastSyncResourceVersion 透過 reflector 得到資源最新的版本\n1 2 3 4 5 6 7 8 9 func (c *controller) LastSyncResourceVersion() string { c.reflectorMutex.RLock() defer c.reflectorMutex.RUnlock() // 透過 reflector 得到資源最新的版本 下一章節會看到！ if c.reflector == nil { return \u0026#34;\u0026#34; } return c.reflector.LastSyncResourceVersion() } 小結 有了前面幾個章節講述的背景知識如 Delta FIFO Queue 、 Indexer 等等作為鋪墊，讓我們在看 Controller 的時候變得相當的容易， Controller 目前還有 reflector 這個重要的元件還不清楚他的底層是如何實作的。\n下一個篇章將會針對 Controller 的 reflector 元件進行解析，文中若有解釋錯誤的地方歡迎各位大大們提出討論，感謝！\n","description":"","id":19,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes Reflector 我在盯著你 （ II  ）","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/redlector/reflector-2/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n幾本上這一塊相當的複雜與龐大，有些部分我認為不需要深入理解其運作的機制，我們就來看看一個 Reflector 是透過哪裡零件組裝起來的吧！\nInformer 實際上我不知道這裡到底要叫 Reflector 又或是 Informer ，anyway 我想要了解這麼龐大的東西要先從範例入手，我從 client go 的範例可以看到 Reflector/Informer 的一些蛛絲馬跡。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 // 先不管他底層是如何實作的，這裡就是會監聽kubernetes api server // 監控 pod 的變化，會帶一些條件例如： 要監聽的 namespace ，label等等 podListWatcher := cache.NewListWatchFromClient(clientset.CoreV1().RESTClient(), \u0026#34;pods\u0026#34;, v1.NamespaceDefault, fields.Everything()) // 傳入監控pod的物件、pod的資料結構，resync的時間 // 處理事件的控制function以及儲存資料的地方(local cache) indexer, informer := cache.NewIndexerInformer(podListWatcher, \u0026amp;v1.Pod{}, 0, cache.ResourceEventHandlerFuncs{ ... , }, cache.Indexers{}) //建立 infomer 物件 func NewIndexerInformer( lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration, h ResourceEventHandler, indexers Indexers, ) (Indexer, Controller) { // 建立一個 indexer 作為local cache用 clientState := NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers) //回傳 indexer 以及 實作 controller interface 的物件（之後會講到先不用管他） return clientState, newInformer(lw, objType, resyncPeriod, h, clientState) } //基本上就是封裝成實作 controller interface 的物件（之後會講到先不用管他） func newInformer( lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration, h ResourceEventHandler, clientState Store, ) Controller { ... return New(cfg) } 我認為 client go 這個範例很好，很清楚明白地看出 informer 需要什麼物件分別是\nListerWatcher runtime.Object ResourceEventHandler Store 像是 Store interface 我們已經看過了，就不再多提，我們先從 ListerWatcher 來看看這傢伙是什麼玩意。\nListerWatcher 從剛剛的範例來看在建立 Informer 的時候傳入了一個 podListWatcher 這是由\ncache.NewListWatchFromClient(clientset.CoreV1().RESTClient(), \u0026quot;pods\u0026quot;, v1.NamespaceDefault, fields.Everything())這個 function 建立起來的，這個 function 回傳了實作 ListWatch interface 的物件，我們先從這個 interface 來看吧！\ninterface 這個 interface 很簡單就是列出(list)要觀測的 object 以及追蹤(watch)要觀測的 obejct 。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // ListerWatcher 組合了 Lister 跟 Watcher 接下去看 這兩個interface負責什麼 type ListerWatcher interface { Lister Watcher } // Lister is any object that knows how to perform an initial list. type Lister interface { // 根據 ListOptions 來決定要列出哪些物件 List(options metav1.ListOptions) (runtime.Object, error) } // Watcher is any object that knows how to start a watch on a resource. type Watcher interface { // 根據 ListOptions 來決定要跟蹤哪些物件 Watch(options metav1.ListOptions) (watch.Interface, error) } 看完了 ListerWatcher 相關的 Interface 後我們來看一下實作 ListerWatcher 的資料結構吧！\nStruct 先打預防針不是每一個 ListerWatcher 都是透過以下方式實作的，本章節只討論 client go 範例的調用鏈，我猜其他實作的方式也差不多吧？（有時間再來研究其他的 listwatch 怎麼做）\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // 定義實作 Lister 的 struct type ListFunc func(options metav1.ListOptions) (runtime.Object, error) // 定義實作 Watcher 的 struct type WatchFunc func(options metav1.ListOptions) (watch.Interface, error) // ListWatch knows how to list and watch a set of apiserver resources. It satisfies the ListerWatcher interface. // It is a convenience function for users of NewReflector, etc. // ListFunc and WatchFunc must not be nil type ListWatch struct { ListFunc ListFunc WatchFunc WatchFunc // DisableChunking requests no chunking for this list watcher. DisableChunking bool } New Function 我們來看一下 cache.NewListWatchFromClient(clientset.CoreV1().RESTClient(), \u0026quot;pods\u0026quot;, v1.NamespaceDefault, fields.Everything())這個 function 做了什麼\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 // NewListWatchFromClient通過指定的 client ， resource ，namespace // 和 fieldsSelector 建立一個ListWatch。 func NewListWatchFromClient(c Getter, resource string, namespace string, fieldSelector fields.Selector) *ListWatch { //封裝 fieldSelector 為 func(options *metav1.ListOptions) optionsModifier := func(options *metav1.ListOptions) { options.FieldSelector = fieldSelector.String() } //建立一個ListWatch return NewFilteredListWatchFromClient(c, resource, namespace, optionsModifier) } // NewFilteredListWatchFromClient 跟上面那個很類似不過他是接收 optionsModifier 而不是 fieldsSelector func NewFilteredListWatchFromClient(c Getter, resource string, namespace string, optionsModifier func(options *metav1.ListOptions)) *ListWatch { //建立一個實作 lister 的物件 listFunc := func(options metav1.ListOptions) (runtime.Object, error) { //這個手法滿厲害的，就是在其他地方使用listFunc(ListOptions)時候 //把傳入的 ListOptions 透過 optionsModifier function 處理 optionsModifier(\u0026amp;options) //這裡就是 kubernetes client 請求 kubernetes api server 的方法 return c.Get(). Namespace(namespace). Resource(resource). VersionedParams(\u0026amp;options, metav1.ParameterCodec). Do(context.TODO()). Get() } //建立一個實作 Watcher 的物件 watchFunc := func(options metav1.ListOptions) (watch.Interface, error) { options.Watch = true //這個手法滿厲害的，就是在其他地方使用watchFunc(ListOptions)時候 //把傳入的 ListOptions 透過 optionsModifier function 處理 optionsModifier(\u0026amp;options) //這裡就是 kubernetes client 請求 kubernetes api server 的方法 return c.Get(). Namespace(namespace). Resource(resource). VersionedParams(\u0026amp;options, metav1.ParameterCodec). Watch(context.TODO()) } //建立實作 ListWatcher 的物件 return \u0026amp;ListWatch{ListFunc: listFunc, WatchFunc: watchFunc} } impliment List source code\n1 2 3 4 5 6 // 委託給剛剛的 object 列出物件的情況 func (lw *ListWatch) List(options metav1.ListOptions) (runtime.Object, error) { // ListWatch is used in Reflector, which already supports pagination. // Don\u0026#39;t paginate here to avoid duplication. return lw.ListFunc(options) } Watch source code\n1 2 3 4 5 // 委託給剛剛的 object 監控物件的情況 func (lw *ListWatch) Watch(options metav1.ListOptions) (watch.Interface, error) { return lw.WatchFunc(options) } 到這裡就是一個最基本的 listwatch ，接下來要看看要怎麼組合這個功能到 Reflector/Informer 內。\n小結 kubernetes Reflector 中的 listwatch 的部分就先看到這裡，下一章節要接著看本篇最一開始提到的 controller interface 部分，kubernetes 底層設計的非常精美，透過閱讀程式碼的方式提升自己對 kubernetes 的了解，若文章有錯的部分希望大大們指出，謝謝！\n","description":"","id":20,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes Reflector 我在盯著你 （ I ）","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/redlector/reflector-1/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n本篇文章是基於上一篇 Kubernetes util tool 使用 Backoff 抖了一下的拓展 ，主要會看到 kubernetes 除了透過 Jittered Backoff Manager Impl 實作了 BackoffManager 之外，還額外實作了另外一種 Exponential Backoff Manager Impl命名方式也十分 java xD。\n我們也先複習一下上一張有提到的 BackoffManager，先以一個情境來描述為甚麼要backoff ！\n當 client 發送請求給 server 如果有可重試類型的失敗那我們就會重新發起請求，but!\n這裡有個問題如果說我們寫的 client 一次打出去的請求有上百個甚至上千個呢？ client 的重試可能會把 server 打掛，所以需要設計一個 backoff 讓重試的請求每次退後一點，每次退後一點（這裡的退後可以想像成重試時間垃長一點）。\n1 2 3 4 // The BackoffManager is supposed to be called in a single-threaded environment. type BackoffManager interface { Backoff() clock.Timer } 是不是非常的簡單的，實作 BackoffManager 的物件只要實作 Backoff() clock.Timer 就好了～\n我們來看一下今天要講的主題 exponentialBackoffManagerImpl 的資料結構吧！\nstruct 1 2 3 4 5 6 7 8 type exponentialBackoffManagerImpl struct { backoff *Backoff //用來計算 back off 延遲時間，等等會看到Backoff物件是如何定義的。 backoffTimer clock.Timer //用 backoff 計算出的延遲時間來建立 timer lastBackoffStart time.Time //上一次觸發back off 是什麼時候 initialBackoff time.Duration //初始化 back off 基礎延遲時間 backoffResetDuration time.Duration //設定 back off 多久沒被觸發要重置的時間 clock clock.Clock //傳入現在時間 } 其中比較疑慮的參數應該是 backoff 竟然是一個物件，那\u0026hellip;到底是長圓的還是扁的？就讓我們來瞧瞧吧！\nBackoff 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type Backoff struct { Duration time.Duration //初始要延遲的時間 Factor float64 //延遲時間倍數因子，等等會看到實作 Jitter float64 //延遲抖動範圍 Steps int //主要用於 backoff 還有幾次，若是次數小於 0 次 延遲時間就不會再改變（除非有用 jitter） Cap time.Duration //主要用於設定 backoff 延遲時間成長的最大限制，如果到了最大限制 step 也將歸 0 //換句話說就是延遲時間就不會再改變（除非有用 jitter） } 看完了資料結構就要接著看 Backoff 的實作囉，這部分有點小複雜!\nstep 這裡我看了滿久的，基本上要有幾個情境才能看得出來他的應用，我們先來看程式碼等等再來看使用情境。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 func (b *Backoff) Step() time.Duration { //如果 step 用完或是超出 duration 超出 cap (底下會看到為什麼)都會進到這個判斷式 //另外有設定 jitter 的話就會丟給 jitter 運算延遲時間 //不然都是回傳 Backoff 上一次計算好的延遲時間 if b.Steps \u0026lt; 1 { if b.Jitter \u0026gt; 0 { return Jitter(b.Duration, b.Jitter) } return b.Duration } //每次使用過都要減少 step b.Steps-- //採用 Backoff 上一次計算好的 Duration 數值 duration := b.Duration //如果有設定延遲倍數因子的話，延遲時間就要乘上倍數因子 //另外如果有設定 cap 數值的話計算完的延遲時間要額外判斷是否超過 cap //超過 cap 就以 cap 為 下一次的延遲時間，並且將 step 歸 0 if b.Factor != 0 { b.Duration = time.Duration(float64(b.Duration) * b.Factor) if b.Cap \u0026gt; 0 \u0026amp;\u0026amp; b.Duration \u0026gt; b.Cap { b.Duration = b.Cap b.Steps = 0 } } //如果有設定 jitter 的話，需要 透過 jitter 計算本次延遲時間 if b.Jitter \u0026gt; 0 { duration = Jitter(duration, b.Jitter) } return duration } Step function 大概會包含以下四種情境，需要搭配著 code 閱讀會比較好理解一點。\n以下情境全部 b.Duration 保持 0.5 s\n僅設定 step step 設定 4 次\n程式流程會怎麼樣呢？ 第一次近來來 step \u0026ndash; (step=3)，duration := b.Duration \u0026lt;0.5\u0026gt;，回傳 duration 0.5 s。\n第二次近來來 step \u0026ndash; (step=2)，duration := b.Duration \u0026lt;0.5\u0026gt;，回傳 duration 0.5 s。\n第三次近來來 step \u0026ndash; (step=1)，duration := b.Duration \u0026lt;0.5\u0026gt;，回傳 duration 0.5 s。\n第四次近來來 step \u0026ndash; (step=0)，duration := b.Duration \u0026lt;0.5\u0026gt;，回傳 duration 0.5 s。\n僅設定 step 與 factor step 設定 4 次 factor 設定 2\n程式流程會怎麼樣呢？ 第一次近來來\nstep \u0026ndash; (step=3)\nduration := b.Duration \u0026lt;0.5\u0026gt;\nb.Duration = b.Duration\u0026lt;0.5\u0026gt; * factor \u0026lt;2\u0026gt;\n回傳 duration 0.5 s。\n第二次近來來\nstep \u0026ndash; (step=2)\nduration := b.Duration \u0026lt;1\u0026gt;\nb.Duration = b.Duration\u0026lt;1\u0026gt; * factor \u0026lt;2\u0026gt;\n回傳 duration 1 s。\n第三次近來來\nstep \u0026ndash; (step=1)\nduration := b.Duration \u0026lt;2\u0026gt;\nb.Duration = b.Duration\u0026lt;2\u0026gt; * factor \u0026lt;2\u0026gt;\n回傳 duration 2 s。\n第四次近來來\nstep \u0026ndash; (step=0)\nduration := b.Duration \u0026lt;2\u0026gt;\nb.Duration = b.Duration\u0026lt;4\u0026gt; * factor \u0026lt;2\u0026gt;\n回傳 duration 4 s。\n設定 step 、 factor 與 Cap step 設定 4 次 factor 設定 2 Cap 設定 2 s\n程式流程會怎麼樣呢？ 第一次近來\nstep \u0026ndash; (step=3)\nduration := b.Duration \u0026lt;0.5\u0026gt;\nb.Duration = b.Duration\u0026lt;0.5\u0026gt; * factor \u0026lt;2\u0026gt;\n回傳 duration 0.5 s。\n第二次近來\nstep \u0026ndash; (step=2)\nduration := b.Duration \u0026lt;1\u0026gt;\nb.Duration = b.Duration\u0026lt;1\u0026gt; * factor \u0026lt;2\u0026gt;\n回傳 duration 1 s。\n第三次近來\nstep \u0026ndash; (step=1)\nduration := b.Duration \u0026lt;2\u0026gt;\nb.Duration = b.Duration\u0026lt;2\u0026gt; * factor \u0026lt;2\u0026gt;\nb.Duration = b.Cap \u0026lt;2\u0026gt;\nb.Step = 0\n回傳 duration 2 s。\n第四次近來\nduration := b.Duration \u0026lt;2\u0026gt;\n回傳 duration 2 s。\n設定 step 、 factor 、 Cap 與 jitter step 設定 4 次 factor 設定 2 Cap 設定 2 s jitter 設定 1 s 程式流程會怎麼樣呢？\n第一次近來\nstep \u0026ndash; (step=3)\nduration := b.Duration \u0026lt;0.5\u0026gt;\nb.Duration = b.Duration\u0026lt;0.5\u0026gt; * factor \u0026lt;2\u0026gt;\n假設 jitter 回傳的結果為 0.6\n回傳 duration 0.6 s。\n第二次近來\nstep \u0026ndash; (step=2)\nduration := b.Duration \u0026lt;1\u0026gt;\nb.Duration = b.Duration\u0026lt;1\u0026gt; * factor \u0026lt;2\u0026gt;\n假設 jitter 回傳的結果為 1.3\n回傳 duration 1.3 s。\n第三次近來\nstep \u0026ndash; (step=1)\nduration := b.Duration \u0026lt;2\u0026gt;\nb.Duration = b.Duration\u0026lt;2\u0026gt; * factor \u0026lt;2\u0026gt;\nb.Duration = b.Cap \u0026lt;2\u0026gt;\nb.Step = 0\n假設 jitter 回傳的結果為 2.6\n回傳 duration 2.6 s。\n第四次近來\n假設 jitter 回傳的結果為 4.1\n回傳 duration 4.1 s。\n看完比較神秘的 backoff 物件後 要回來理解要如何把 Exponential Backoff Manager 建立出來，觀察他的 new function 有沒有偷藏什麼！\nnew function 在這之前先再次複習一下 exponentialBackoffManagerImpl 的資料結構\n1 2 3 4 5 6 7 8 type exponentialBackoffManagerImpl struct { backoff *Backoff //用 back off 物件計算延遲時間， back off 透過 基礎延遲時間 ，延遲倍數因子 ， step ， jitter 以及 cap 等來計算出下一次要延遲多久。 backoffTimer clock.Timer //用 backoff 計算出的延遲時間來建立對應的 timer lastBackoffStart time.Time //上一次觸發back off 是什麼時候 initialBackoff time.Duration //初始化 back off 基礎延遲時間 backoffResetDuration time.Duration //設定 back off 多久沒被觸發要重置的時間 clock clock.Clock //傳入現在時間 } 複習完了 exponentialBackoffManagerImpl 的資料結構後，我們馬上來看看怎麼新增這個物件吧！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func NewExponentialBackoffManager(initBackoff, maxBackoff, resetDuration time.Duration, backoffFactor, jitter float64, c clock.Clock) BackoffManager { return \u0026amp;exponentialBackoffManagerImpl{ backoff: \u0026amp;Backoff{ //建構 backoff 物件 Duration: initBackoff, //backoff 初始延遲時間 Factor: backoffFactor, //backoff 延遲倍數因子 Jitter: jitter, //backoff 抖動數值 Steps: math.MaxInt32, //backoff 剩下幾次預設...很多次xDDD Cap: maxBackoff, //backoff 延遲最大閥值 }, backoffTimer: nil, //這時候還不會賦值，等到有人呼叫 backoff 的時候會計算出來 initialBackoff: initBackoff, //backoff 初始延遲時間 lastBackoffStart: c.Now(), //用來記住上次什麼時候呼叫 backoff 的 backoffResetDuration: resetDuration, //設定多久 backoff 物件需要重置 clock: c, //用來對準時間 } } Backoff 瞭解了 exponentialBackoffManagerImpl 怎麼新增物件後，趕緊來看實作的部分廢話就不多說直接上 code !\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func (b *exponentialBackoffManagerImpl) Backoff() clock.Timer { //如果還沒設定 backoffTimer 那麼就從 getNextBackoff() 算一個延遲時間作為 backoff timer //如果有了 backoff timer 就需要刷新 timmer if b.backoffTimer == nil { b.backoffTimer = b.clock.NewTimer(b.getNextBackoff()) } else { b.backoffTimer.Reset(b.getNextBackoff()) } return b.backoffTimer } func (b *exponentialBackoffManagerImpl) getNextBackoff() time.Duration { //如果現在時間 剪去 lastBackoffStart [上次 backoff 開始時間] 大於 backoffResetDuration [預設重置時間] 的話 // 設定 backoff 物件的 step 為最大數值 // 設定 backoff 物件的基礎延遲時間 if b.clock.Now().Sub(b.lastBackoffStart) \u0026gt; b.backoffResetDuration { b.backoff.Steps = math.MaxInt32 b.backoff.Duration = b.initialBackoff } //設定 lastBackoffStart [上次 backoff 開始時間] 為現在 b.lastBackoffStart = b.clock.Now() //透過 backoff 計算需要延遲多久 return b.backoff.Step() } exponentialBackoffManagerImpl Backoff function 的重點基本上在於 backoff 物件如何計算延遲時間，以及當執行時間超過 backoffResetDuration 就要將 backoff 物件設定成預設的閥值。\n小結 簡單的來看這個 kubernetes utils tool ，屬於 wait package 的一部分主要實作 backoff 邏輯，防止 client 端因為大量的請求打壞 server ，透過上一篇提到的 jitter 結合本篇的 exponential Backoff 可以讓 user 只要輕鬆進行幾個簡單的設定例如 initBackoff 基礎延遲時間 ，backoffFactor 延遲倍數因子 ， jitter 抖動範圍, maxBackoff 最大延遲時間 以及當多久沒有呼叫 backoff 需要重置的 resetDuration 就能獲得有 backoff 功能 clock ，整個過程相當的簡單～\n","description":"","id":21,"section":"posts","tags":["kubernetes","utils"],"title":"Kubernetes util tool 使用 Backoff 指數大漲","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/utils/kubernetes-utils-tool-wait-exponential/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n有朋友私訊我為什麼關於 kubernetes controller 的Reflector 我在盯著你 （ III ）文章還沒出\n因為筆者最近事情有點多加上是系列文的最後幾篇，Reflector 有些部分還不是非常了解，既然要 dig controller/operator 的實作就不想輕易的草率帶過。再麻煩各位再等等了，感謝！\nJittered Backoff Manager Impl 今天要來探討的是 kubernetes Jittered Backoff Manager Impl 這個名稱非常 java ，就名稱來看就是抖動的 Backoff Manage 實作，既然是 Backoff Manage 的實作的話，我們就要先來看一下 Backoff Manage 是個什麼樣的狠角色。\nBackoffManager 先從名稱來推敲應該是退後管理器，那什麼是退後管理器。\n我用一個簡單的範例說明，通常 client 發送請求給 server 如果有可重試類型的失敗那我們就會重新發起請求，but!\n這裡有個問題 client 有上百上千個呢？ client 的重試可能會把 server 打掛，所以需要設計一個 backoff 讓重試的請求每次退後一點，每次退後一點（這裡的退後可以想像成重試時間垃長一點）。\n1 2 3 4 5 // The BackoffManager is supposed to be called in a single-threaded environment. type BackoffManager interface { Backoff() clock.Timer //回傳一個 clock timer } 是不是非常的簡單的，實作 BackoffManager 的物件只要實作 Backoff() clock.Timer 就好了～\n我們來看一下今天要講的主題 jitteredBackoffManagerImpl 的資料結構吧！\nstruct 1 2 3 4 5 6 type jitteredBackoffManagerImpl struct { clock clock.Clock //給定 clock ，這一個實作應該用不到xD duration time.Duration // jitter float64 backoffTimer clock.Timer } 看完了 jitteredBackoffManagerImpl 的資料結構後我們要來看一下怎麼初始化這個物件，看看有沒有什麼東西偷偷藏在裡面。\nnew function 1 2 3 4 5 6 7 8 func NewJitteredBackoffManager(duration time.Duration, jitter float64, c clock.Clock) BackoffManager { return \u0026amp;jitteredBackoffManagerImpl{ clock: c, //給定 clock ，這一個實作應該用不到xD duration: duration, //最少要延遲多久 jitter: jitter, //給定抖動範圍 backoffTimer: nil, //一開始不需要初始化 backoffTimer 會由 使用者呼叫 backoff 時經由計算後再賦值 } } 看完了如何初始化就是進入 Jittered Backoff Manager Impl 如何實作 BackoffManager !\nBackoff 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func (j *jitteredBackoffManagerImpl) Backoff() clock.Timer { backoff := j.getNextBackoff() //進來的時候就先計算出 backoff 時間，等等會看到怎麼做的。 if j.backoffTimer == nil { //如果當前沒有 backoff timer 就要賦值 //賦與的數就是剛剛get next back off 曲的的時間 j.backoffTimer = j.clock.NewTimer(backoff) } else { j.backoffTimer.Reset(backoff) //如果存在的話需要刷新 backoff 時間 } return j.backoffTimer } func (j *jitteredBackoffManagerImpl) getNextBackoff() time.Duration { jitteredPeriod := j.duration if j.jitter \u0026gt; 0.0 { jitteredPeriod = Jitter(j.duration, j.jitter) //等等會看到jitter 在做什麼，這邊只要了解輸入基礎延遲時間跟抖動範圍 //我們就能到最後要延遲的時間 } return jitteredPeriod } Jitter 負責抖動延遲時間的運算\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Jitter returns a time.Duration between duration and duration + maxFactor * duration. // // This allows clients to avoid converging on periodic behavior. If maxFactor // is 0.0, a suggested default value will be chosen. func Jitter(duration time.Duration, maxFactor float64) time.Duration { //抖動因子參數（抖動範圍）可以自行調整，但不能小於等於0 if maxFactor \u0026lt;= 0.0 { maxFactor = 1.0 } //計算方式也很間單 基礎的延遲時間 + 隨機時間*抖動因子*基礎的延遲時間 wait := duration + time.Duration(rand.Float64()*maxFactor*float64(duration)) return wait } how to use 由於 kubernetes 沒有直接使用 jitteredBackoffManagerImpl ，只能看一下測試是怎麼寫的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func TestJitterBackoffManagerWithRealClock(t *testing.T) { //定義抖動的參數，基礎延遲時間為1*time.Millisecond 抖動範圍為0 (這裡等等計算的時候會設定成1) backoffMgr := NewJitteredBackoffManager(1*time.Millisecond, 0, \u0026amp;clock.RealClock{}) //測試跑五次，如果抖動的延遲數值小於基礎延遲時間就是測試失敗 for i := 0; i \u0026lt; 5; i++ { start := time.Now() //還記得Backoff()回傳一個 clock.Timer 嗎？ //他就像是一個鬧鐘，當設定的時間到鬧鐘就會響，我們就可以從 channel 被喚醒 \u0026lt;-backoffMgr.Backoff().C() passed := time.Now().Sub(start) if passed \u0026lt; 1*time.Millisecond { t.Errorf(\u0026#34;backoff should be at least 1ms, but got %s\u0026#34;, passed.String()) } } } 小結 kubernetes utils 裡面有許多有趣的工具，像是今天分享就屬於 wiat utils tool 其中一小部分， wait tool 中還有許多有趣的小工具，下一篇會繼續探討 wait 中其他的 Backoff 實作，若是文中有錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":22,"section":"posts","tags":["kubernetes","utils"],"title":"Kubernetes util tool 使用 Backoff 抖了一下","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/utils/kubernetes-utils-tool-wait-jitterbackoff/"},{"content":" LRU 今天先從 kubernetes util tool 工具裡面一把抓，抓出 cache 這個工具要使用這個工局前我們要先了解 LRU 是什麼，實際上很間單，我這邊簡單帶過 LRU 是什麼。\nLRU 全名 Least Recently Used（最近最少使用策略），顧名思義是根據使用的記錄來淘汰資料，A資料被存取過那A資料在未來被存取的機率相對來高。\nLRU 整體結構是依靠 Doubly linked list 作為 CRUD 的後盾，我們可以看一下圖來了解整體結構!\n插入A， 將插入的物件Ａ移動到 link list 的 header 插入B， 將插入的物件Ｂ移動到 link list 的 header 插入C， 將插入的物件Ｃ移動到 link list 的 header 讀取A， 將剛剛讀取的物件Ａ移動到 link list 的 header 插入D， 將插入的物件D移動到 link list 的 header 插入G， 將插入的物件G移動到 link list 的 header 插入F， 將插入的物件F移動到 link list 的 header ，超出 link list 的長度，將tail 的物件踢出！ 基本上就這麼簡單!\ntips:\n為什麼要用 Doubly linked list 而不是 Singly linked list?\n因為Doubly在已知位置插入和刪除的複雜度為O（1）而Singly在已知位置插入和刪除的複雜度為O（n）\nsimplelru Kubernetes LRUExpireCache 簡單來說就是基於 github.com/hashicorp/golang-lru ，封裝成有 Expire 功能的 LRU。\nhashicorp 的 LRU 基於 hashicorp/golang-lru/simplelru 封裝成有 thread safe 功能的 LRU。\n所以我們要了解的地方總共有三層，第一層 kubernetes LRUExpireCache 第二層 hashicorp 的 LRU ，第三層 hashicorp/golang-lru/simplelru 。\n我們先從 hashicorp/golang-lru/simplelru 作為入口開始了解整個架構。\ninterface 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 // LRUCache is the interface for simple LRU cache. type LRUCache interface { Add(key, value interface{}) bool //向 cache add 一個 key value paire ，如果發生逐出，需要回傳 true 同時更新 lru 狀態。 Get(key interface{}) (value interface{}, ok bool) //從 cache 中透過 key 去找對應的 value 同時更新 lru 狀態。 Contains(key interface{}) (ok bool) //確認 cache 是否存在某一個 key Peek(key interface{}) (value interface{}, ok bool) //回傳 key 對應的 value ，而不更新 lru 狀態。 Remove(key interface{}) bool //透過 key 刪除 cache 中指定的 物件 RemoveOldest() (interface{}, interface{}, bool) //刪除 cache 中最舊的物件 GetOldest() (interface{}, interface{}, bool) //取得 cache 中最舊的問間 Keys() []interface{} //從 cache 中取得所有的 key 從最舊到最新依序排列 Len() int //取得 cahce 當前長度 Purge() //清除 cahce 所有資料 } 看完了 LRU 的 interface 後我們就可以往實作層面來看看，時做一個 LRU 需要什麼資料結構。\nstruct 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type EvictCallback func(key interface{}, value interface{}) //用於在 LRU 在 Evict entry 時call back 給使用者知道 // 實作了none thread safhe 的固定大小 LRU cahe type LRU struct { size int //cache size evictList *list.List //lru list items map[interface{}]*list.Element //紀錄 lru key 對應到的 value onEvict EvictCallback //當發生LRU Evict entry 時call back 給使用者知道 } // LRU cache 所記錄的物件型態 type entry struct { key interface{} value interface{} } 可能有人會問說為什麼會有一個 map 在 LRU 的資料結構中呢？\n原因是從 map 裡面找資料的速度是 O(1) 的，我們可以從 map 中判斷 key 是否存在，如果存在的話只要更新 map 中 key 所對應的 value ，並且將本來就存在 Doubly linked list 的 value 移動到 header 。\n簡單看完資料結構，我們可以看看要怎麼把這個物件叫起來，裡面有沒有偷偷藏什麼不為人知的初始化呢？\nNew Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // NewLRU constructs an LRU of the given size func NewLRU(size int, onEvict EvictCallback) (*LRU, error) { //由於是固定 size 的 cahce 所以 size 不能小於0 if size \u0026lt;= 0 { return nil, errors.New(\u0026#34;Must provide a positive size\u0026#34;) } c := \u0026amp;LRU{ size: size, evictList: list.New(), //使用 go 內建的 list 資料結構 items: make(map[interface{}]*list.Element), onEvict: onEvict, //傳入EvictCallback 當發生 Evict 的時候可以通知使用者處理 } return c, nil } 看完了怎麼建立一個 LRU 的物件之後我們來看看他怎麼實作 LRU 的 interface 吧！\nAdd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // Add adds a value to the cache. Returns true if an eviction occurred. func (c *LRU) Add(key, value interface{}) (evicted bool) { // Check for existing item //檢查物件是否存在於 map 中，若是存在的話我們需要把 LRU 中 物件所在的位置移動到 list 的 header //並且更新 物件 的值 if ent, ok := c.items[key]; ok { c.evictList.MoveToFront(ent) ent.Value.(*entry).value = value return false } //若是物件不存在於 map 當中，我們需要先建立物件在存放在 LRU 的 entry //並請把 entry 推送到 LRU list 的 header //最後更新 map 下次物件進來的時候可以直接從 map 判斷物件是否存在過，若是存在就直接更新 LRU list ent := \u0026amp;entry{key, value} entry := c.evictList.PushFront(ent) c.items[key] = entry //更新完 LRU 後需要判斷 LRU 的長度是否超過預設值，超過的話就需要刪除 LRU // list 最後一個 entry 並且更新對應的 map (過程都在c.removeOldest()，晚點會看到) evict := c.evictList.Len() \u0026gt; c.size if evict { c.removeOldest() } return evict } Get 1 2 3 4 5 6 7 8 9 10 11 // Get looks up a key\u0026#39;s value from the cache. func (c *LRU) Get(key interface{}) (value interface{}, ok bool) { // 會先從 map 搜尋物件是否存在，不存在就不用拿拉xD if ent, ok := c.items[key]; ok { // 根據 LRU 演算法最近使用過的物件要推到 list 的 header c.evictList.MoveToFront(ent) return ent.Value.(*entry).value, true } return } Contains 1 2 3 4 5 6 7 // Contains checks if a key is in the cache, without updating the recent-ness // or deleting it for being stale. func (c *LRU) Contains(key interface{}) (ok bool) { _, ok = c.items[key] //從 lru map 中查看有沒有我們指定的 key return ok } Peek 1 2 3 4 5 6 7 8 9 // Peek returns the key value (or undefined if not found) without updating // the \u0026#34;recently used\u0026#34;-ness of the key. func (c *LRU) Peek(key interface{}) (value interface{}, ok bool) { var ent *list.Element //先建立一個 element 等著承裝 LRU 的資料 if ent, ok = c.items[key]; ok { //從 LRU map 中找到對應的資料後回傳 return ent.Value.(*entry).value, true } return nil, ok } Remove 1 2 3 4 5 6 7 8 9 10 11 // Remove removes the provided key from the cache, returning if the // key was contained. func (c *LRU) Remove(key interface{}) (present bool) { // 會先從 map 搜尋物件是否存在，不存在就不用拿拉xD if ent, ok := c.items[key]; ok { // 直接從 LRU list 與 map 移除物件(過程都在c.removeElement()，晚點會看到) c.removeElement(ent) return true } return false } RemoveOldest 1 2 3 4 5 6 7 8 9 10 11 // RemoveOldest removes the oldest item from the cache. func (c *LRU) RemoveOldest() (key interface{}, value interface{}, ok bool) { ent := c.evictList.Back() //取得LRU list 最最舊的資料 //如果有找到資料就刪除最舊的資料 if ent != nil { c.removeElement(ent) kv := ent.Value.(*entry) return kv.key, kv.value, true } return nil, nil, false } GetOldest 1 2 3 4 5 6 7 8 9 10 // GetOldest returns the oldest entry func (c *LRU) GetOldest() (key interface{}, value interface{}, ok bool) { ent := c.evictList.Back() //取得LRU list 最最舊的資料 //如果有找到資料就回傳 if ent != nil { kv := ent.Value.(*entry) return kv.key, kv.value, true } return nil, nil, false } Keys 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Keys returns a slice of the keys in the cache, from oldest to newest. func (c *LRU) Keys() []interface{} { //先建立一個 slice 等等用來 copy LRU 資料用 keys := make([]interface{}, len(c.items)) i := 0 // 會從 LRU list 最後一個 entry 開始往前找，直到往前找到nil為止 // 過程中會把 entry 的 key 列出來放置到 keys 的 slice 中 // 但我有個疑問，不是有 map 嗎？直接拿 map 應該會比 LRU list 一個一格找要快吧？ for ent := c.evictList.Back(); ent != nil; ent = ent.Prev() { keys[i] = ent.Value.(*entry).key i++ } return keys } Len 1 2 3 4 // Len returns the number of items in the cache. func (c *LRU) Len() int { return c.evictList.Len() //回傳 LRU list 長度 } removeOldest/removeElement 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // removeOldest removes the oldest item from the cache. func (c *LRU) removeOldest() { ent := c.evictList.Back() //從 LRU list 中取得最舊的資料 //如果剛剛有找到存在的資料話就刪除物件 if ent != nil { c.removeElement(ent) } } // removeElement is used to remove a given list element from the cache func (c *LRU) removeElement(e *list.Element) { c.evictList.Remove(e) //刪除 LRU list 的資料 kv := e.Value.(*entry) //把list.Element轉成entry delete(c.items, kv.key) //透過 entry key 刪除 LRU map 對應的 物件 if c.onEvict != nil { c.onEvict(kv.key, kv.value) //因為有物件被刪除了需要通知使用者 } } 看完了 simplelru 表示我們可以往下一個地方邁進，那就是 hashicorp/golang-lru 的實作，這邊不難我們很快地看過去。\nhashicorp/golang-lru 先來看hashicorp/golang-lru的資料結構\nstruct 1 2 3 4 5 // Cache is a thread-safe fixed size LRU cache. type Cache struct { lru simplelru.LRUCache //組合了 simple lru 的功能 lock sync.RWMutex //多了讀寫鎖 } 其實很簡單 xDD 單純多了讀寫鎖而已，接著來看怎麼建構起這個物件的。\nNew function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // New creates an LRU of the given size. func New(size int) (*Cache, error) { return NewWithEvict(size, nil) //最簡單的方法就是直接就入 cahce size ，不指定 Evict call back function } // NewWithEvict constructs a fixed size cache with the given eviction // callback. func NewWithEvict(size int, onEvicted func(key interface{}, value interface{})) (*Cache, error) { lru, err := simplelru.NewLRU(size, simplelru.EvictCallback(onEvicted)) //這邊就是有指定 Evict call back function //當發生 Evict 事件時會透過 call back function 通知調用者。 if err != nil { return nil, err } c := \u0026amp;Cache{ lru: lru, } return c, nil } 看完了怎麼新增這個物件後我們來看看，加了讀寫鎖有什麼改變吧\nRWMutex impliemnt 這個部分相單的簡單，我只舉幾個作為例子有興趣的營有可以到 github 看其他的實作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // Purge is used to completely clear the cache. func (c *Cache) Purge() { c.lock.Lock() //全部移除前上鎖 c.lru.Purge() //lru 移除 c.lock.Unlock() //移除完解鎖 } // Add adds a value to the cache. Returns true if an eviction occurred. func (c *Cache) Add(key, value interface{}) (evicted bool) { c.lock.Lock() //加入資料前上鎖 evicted = c.lru.Add(key, value) //呼叫 simple lru add function c.lock.Unlock() //加完資料解鎖 return evicted } // Get looks up a key\u0026#39;s value from the cache. func (c *Cache) Get(key interface{}) (value interface{}, ok bool) { c.lock.Lock() //拿資料前上鎖 value, ok = c.lru.Get(key) //呼叫 simple lru get function c.lock.Unlock() //拿完資料解鎖 return value, ok } //這裡可能有人會問 Get 這個 function 不是拿資料嗎？為什麼用 lock 而不是 Rlock???? //別忘了 LRU 在拿資料的時候會更新資料的熱度，讓最近取得過的資料移到 LRU 的第一位！ // Contains checks if a key is in the cache, without updating the // recent-ness or deleting it for being stale. func (c *Cache) Contains(key interface{}) bool { c.lock.RLock() //單純拿資料，不會動到其他資料用 rlock containKey := c.lru.Contains(key) //呼叫 simple lru contain function c.lock.RUnlock() //拿完資料解鎖 return containKey } 幾本上就做了簡單的封裝也沒有做什麼特別的處理，其他的 function 可以到 hashicorp/golang-lru 的 github 去查看看。\n最後要進到重頭戲， kubernetes LRUExpireCache 好拉說白一點也只是封裝 hashicorp/golang-lru ，廢話就不多說了直接來看 kubernetes 如何封裝的吧！\nKubernetes LRUExpireCache Struct 資料結構的部分相當簡單，比剛剛看到的 hashicorp/golang-lru 多加了 clock 以及 sync Mutex ，不過為什麼不復用 hashicorp/golang-lru 就好了。\n1 2 3 4 5 6 7 8 9 // LRUExpireCache is a cache that ensures the mostly recently accessed keys are returned with // a ttl beyond which keys are forcibly expired. type LRUExpireCache struct { // clock is used to obtain the current time clock Clock //用來定時驅除 LRU 內的物件 cache *lru.Cache //用來定時驅除 LRU 內的物件 lock sync.Mutex //多了鎖 } 放入的物件除了 key 所代表的 value 之外還多加了 expireTime 來看， cache 是否過期。\n1 2 3 4 type cacheEntry struct { value interface{} expireTime time.Time } 簡單看完了資料結構後我們直接來看怎麼建立這個物件\nNew Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // NewLRUExpireCache creates an expiring cache with the given size func NewLRUExpireCache(maxSize int) *LRUExpireCache { //套入 kubernetes 的 real clock ，有機會再來看 clock ，這裡就當作他是 times就好了 return NewLRUExpireCacheWithClock(maxSize, realClock{}) } // NewLRUExpireCacheWithClock creates an expiring cache with the given size, using the specified clock to obtain the current time. func NewLRUExpireCacheWithClock(maxSize int, clock Clock) *LRUExpireCache { cache, err := lru.New(maxSize) //這裡直接重用了 hashicorp/golang-lru ，建立一個 cache if err != nil { // if called with an invalid size panic(err) } return \u0026amp;LRUExpireCache{clock: clock, cache: cache} } 看完了如何建立 LRUExpireCache 我們再接著來看一下 細節怎麼實作的。\nAdd 1 2 3 4 5 func (c *LRUExpireCache) Add(key interface{}, value interface{}, ttl time.Duration) { c.lock.Lock() //不重用hashicorp/golang-lru 的 lock 真難受xD defer c.lock.Unlock() c.cache.Add(key, \u0026amp;cacheEntry{value, c.clock.Now().Add(ttl)}) //重用hashicorp/golang-lru 的 add 放入的物件多了 ttl 的過期時間。 } Get 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func (c *LRUExpireCache) Get(key interface{}) (interface{}, bool) { c.lock.Lock() defer c.lock.Unlock() e, ok := c.cache.Get(key) //重用hashicorp/golang-lru 的 get 取得物件 //沒東西的話直接回傳就好 if !ok { return nil, false } //需要額外判斷物件 ttl 是否過期，若是過期需要移出 LRU 並回傳使用者 找不到 if c.clock.Now().After(e.(*cacheEntry).expireTime) { c.cache.Remove(key) return nil, false } //沒過期直接回傳 return e.(*cacheEntry).value, true } // Get looks up a key\u0026#39;s value from the cache. func (c *Cache) Get(key interface{}) (value interface{}, ok bool) { c.lock.Lock() value, ok = c.lru.Get(key) //重用hashicorp/golang-lru 的 get 取得物件 c.lock.Unlock() return value, ok } Remove 1 2 3 4 5 6 // Remove removes the specified key from the cache if it exists func (c *LRUExpireCache) Remove(key interface{}) { c.lock.Lock() defer c.lock.Unlock() c.cache.Remove(key) //重用hashicorp/golang-lru 的 remove 移除物件 } Keys 1 2 3 4 5 6 7 8 // Keys returns all the keys in the cache, even if they are expired. Subsequent calls to // get may return not found. It returns all keys from oldest to newest. func (c *LRUExpireCache) Keys() []interface{} { c.lock.Lock() defer c.lock.Unlock() return c.cache.Keys() //重用hashicorp/golang-lru 的 keys 取得所有 keys } 小結 kubernetes utils 裡面有許多有趣的工具，今天分享的工具關於 LRU 的 cache 比較簡單也非常容易上手，基於 github.com/hashicorp/golang-lru ，封裝成有 Expire 功能的 LRU 以及 基本的 LRU 由 hashicorp/golang-lru/simplelru 實作，若是文中有錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":23,"section":"posts","tags":["kubernetes","utils"],"title":"Kubernetes util tool 使用 cache LRU 定格一瞬間","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/utils/kubernetes-utils-tool-lru/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\nkubernetes 中封裝了非常多的基本的 library 例如 clock , channel , waits \u0026hellip;等等，我們從 source code 學習 kubernetes 的行為時常常會遇到這個 library 跟基本的 library 非常像但是又覺得跟基本的 library 使用起來有點不一樣，本篇文章會把我目前看到被封裝過的 library 進行整理。\nutils tool utilruntime 主要處理 runtime crash 的錯誤捕捉 wait.Group 主要處理 goroutine 的生命週期管理 signals 主要處理 linux signal 的訊號捕捉 sets 主要處理 set 類型的資料結構 rand 主要處理 go 語言中 random 的生成 cache 主要處理常用的 cache 方法，例如 LRU 以及 expiring diff 主要用來比較 string 或是 object 的異同處 之後若是在閱讀 kubernetes source code 有看到跟 utils tool 相關的 library 再補上來。\n小結 Kubernetes 為了開發者方便以及統一各個開發者使用基礎 library 的使用方式，對這些基礎的進行了封裝形成了 utils tool。\n我們若要了解 kubernetes 的運作原理，認識這些 utils tool 是不可避免的，後續會花幾個章節整理一下尚屬提到的幾個 utils tool，若是描述的不正確或是理解有錯希望觀看本文的大大嗎可以不吝嗇提出，感謝。\n","description":"","id":24,"section":"posts","tags":["kubernetes","utils"],"title":"Kubernetes util tool 從工具包裡找工具","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/utils/kubernetes-util-tool/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n前面講了很多 Indexer 的實作細節，不知道各位還不記不記得 kubernetes controller/operator 的架構，我們再來複習一下整個架構圖以及目前我們已經了解的元件～\n圖片來源：How to Create a Kubernetes Custom Controller Using client-go\n目前我們已經了解\nWork Queue Reflector ,Delta Queue 為生產者\n產出 object key 放入 work queue 按照特定規則排序（如delaying等） Controller/Handler 為消費者\n從 work queue 中拿出 object key Indexer key / value 形態的 local cache\n存放 object key 跟反序列化後的 obejct value（Pod, Deployment e.t.c） 還有許多元件是我們不了解的，例如 Reflector 中的 list watch 元件、Delta FIFO 元件 以及最後 Handler 如何從 work queue 中取出 object Key 到 indexer 查找資料。\n本章節先從 Reflector 中的 Delta FIFO 元件進行剖析，用以了解當 Reflector 中的 list watch 觀察到物件變化後如何存到 indexer 以及把 Object key 傳遞給 work queue，做到承上啟下的作用。\nDeltaFIFO kubernetes source code 設計得非常精美，我們可以先從 interface 定義了哪些方法來推敲實作這個 interface 的物件可能有什麼功能。\ninterface 由於 GO 語言判斷 struct 有沒有實作 interface 採用隱式的判斷，我先將DeltaFIFO 有實作的 Interface 列出，接再著一一剖析。\nDeltaFIFO 首先實作了 Queue interface\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type Queue interface { Store //上一篇 indexer 文章中有提過這裡不再贅述， DeltaFIFO 也實作了儲存。 Pop(PopProcessFunc) (interface{}, error) //用來把DeltaFIFO Object 彈出 AddIfNotPresent(interface{}) error //如果 Object 不再FIFO queue 就透過這個function 添加 HasSynced() bool //todo Close() //關閉 queue } DeltaFIFO 還實作了 KeyListerGetter interface ，對 ListKeys() 以及 GetByKey() 這兩個 function 還有印象的地方應該是在 Store interface 裡面，如果忘記了可以回去複習一下。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // A KeyListerGetter is anything that knows how to list its keys and look up by key. type KeyListerGetter interface { KeyLister //繼承 KeyLister KeyGetter //繼承 KeyGetter } // A KeyLister is anything that knows how to list its keys. type KeyLister interface { ListKeys() []string //回傳所有的 object key } // A KeyGetter is anything that knows how to get the value stored under a given key. type KeyGetter interface { // GetByKey returns the value associated with the key, or sets exists=false. GetByKey(key string) (value interface{}, exists bool, err error) //透過object key 取的 object } 看完了 DeltaFIFO 相關的 Interface 後我們來看一下 DeltaFIFO 的資料結構吧！\nstruct source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 type DeltaFIFO struct { // lock/cond protects access to \u0026#39;items\u0026#39; and \u0026#39;queue\u0026#39;. lock sync.RWMutex //當物件被加入、更新、刪除操作時，需要保證所有對 queue 的操作都是 atomic //因此需要鎖 cond sync.Cond //當 queue 中沒有資料 thread 會卡住，需要透過cond發送通知喚醒 thread items map[string]Deltas // object key對應object 的變化（等等會看到Deltas 是什麼） queue []string //儲存 Object key (fifo queue) populated bool //如果透過 Replace function 將第一批 object 放入 queue //或是第一次使用Delete/Add/Update/AddIfNotPresent 標記這個數值為true initialPopulationCount int //第一次使用 Replace function 放入的 object 數量 keyFunc KeyFunc //前一章有提過，計算Object key的方法 knownObjects KeyListerGetter //列出已知的 object key 或是列出已知的 object ，這裡應該是 Indexer closed bool //判斷queue是否關閉 // emitDeltaTypeReplaced is whether to emit the Replaced or Sync // DeltaType when Replace() is called (to preserve backwards compat). emitDeltaTypeReplaced bool //決定使用替換或是同步 } 看完了資料結構接著來看看如何把一個 DeltaFIFO 建立起來吧\nnew function 目前有兩種方式把 DeltaFIFO 的物件建立起來，第一種已經被 Deprecated 要使用的朋友需要注意一下。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // Deprecated: Equivalent to NewDeltaFIFOWithOptions(DeltaFIFOOptions{KeyFunction: keyFunc, KnownObjects: knownObjects}) //該方法已經被棄用了，需要特別注意！ func NewDeltaFIFO(keyFunc KeyFunc, knownObjects KeyListerGetter) *DeltaFIFO { return NewDeltaFIFOWithOptions(DeltaFIFOOptions{ //透過Options 封裝建構請求，接下來會看到 KeyFunction: keyFunc, //傳入計算 object key 的function KnownObjects: knownObjects, //傳入已知的有實作 KeyListerGetter 的物件，一般來說會傳入 indexer }) } //封裝 DeltaFIFO 的建構物件 type DeltaFIFOOptions struct { KeyFunction KeyFunc //計算 object key 的方法預設採用MetaNamespaceKeyFunc KnownObjects KeyListerGetter // 實作 KeyListerGetter 的物件 EmitDeltaTypeReplaced bool //支援 delta fifo 是否支援replace 方法 } // NewDeltaFIFOWithOptions returns a Queue which can be used to process changes to // items. See also the comment on DeltaFIFO. //目前都是使用 NewDeltaFIFOWithOptions 作為delta fifo 的初始化方法 func NewDeltaFIFOWithOptions(opts DeltaFIFOOptions) *DeltaFIFO { //如果 object key 計算方法沒有指定就用預設的 MetaNamespaceKeyFunc if opts.KeyFunction == nil { opts.KeyFunction = MetaNamespaceKeyFunc } // 建立 DeltaFIFO f := \u0026amp;DeltaFIFO{ items: map[string]Deltas{}, queue: []string{}, keyFunc: opts.KeyFunction, knownObjects: opts.KnownObjects, emitDeltaTypeReplaced: opts.EmitDeltaTypeReplaced, } // cond 復用 DeltaFIFO 的 sync lock ，提升效能？ f.cond.L = \u0026amp;f.lock return f } var ( _ = Queue(\u0026amp;DeltaFIFO{}) //在編譯期間或是在寫code的時候就能確認 DeltaFIFO 有沒有實作 Queue 了 ) 會看到 DeltaFIFO 的資料結構中有一個 items 的欄位資料型態為 Deltas ，我們先來看看 Deltas 是什麼\nDeltas source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 //Deltas 是一個 slice 的資料結構，再往下看 slice 裡面存了什麼 type Deltas []Delta // slice 裡面存了 DeltaType 以及 Object type Delta struct { Type DeltaType Object interface{} } // DeltaType 表示 Object 的變化型態為 string type DeltaType string //事件總共會有五種，分別是 Add 事件，updated 事件， Deleted 事件 ， Replaced 事件 以及 Sync事件。 const ( Added DeltaType = \u0026#34;Added\u0026#34; Updated DeltaType = \u0026#34;Updated\u0026#34; Deleted DeltaType = \u0026#34;Deleted\u0026#34; // Replaced is emitted when we encountered watch errors and had to do a // relist. We don\u0026#39;t know if the replaced object has changed. // // NOTE: Previous versions of DeltaFIFO would use Sync for Replace events // as well. Hence, Replaced is only emitted when the option // EmitDeltaTypeReplaced is true. Replaced DeltaType = \u0026#34;Replaced\u0026#34; // Sync is for synthetic events during a periodic resync. Sync DeltaType = \u0026#34;Sync\u0026#34; ) //取出最舊一筆資料 func (d Deltas) Oldest() *Delta { if len(d) \u0026gt; 0 { return \u0026amp;d[0] } return nil } //取出最新一筆資料 func (d Deltas) Newest() *Delta { if n := len(d); n \u0026gt; 0 { return \u0026amp;d[n-1] } return nil } //複製一組 deltas 的資料（因為是複製的所以就算更改了複製的資料也跟原先那組 deltas 沒有關係） func copyDeltas(d Deltas) Deltas { d2 := make(Deltas, len(d)) copy(d2, d) return d2 } default 的 DeltaFIFO 是以 MetaNamespaceKeyFunc 作為 Object Key 的計算方式，因此我們需要來看看 Object key 是如何計算的\nMetaNamespaceKeyFunc 這裏 source code 的註解寫得很棒，我直接引用原文，簡單來說 Object key 就是 namespace/name 如果沒有 namespace 的話 Object key 就只保留 name。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // MetaNamespaceKeyFunc is a convenient default KeyFunc which knows how to make // keys for API objects which implement meta.Interface. // The key uses the format \u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt; unless \u0026lt;namespace\u0026gt; is empty, then // it\u0026#39;s just \u0026lt;name\u0026gt;. func MetaNamespaceKeyFunc(obj interface{}) (string, error) { if key, ok := obj.(ExplicitKey); ok { return string(key), nil } meta, err := meta.Accessor(obj) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;object has no meta: %v\u0026#34;, err) } if len(meta.GetNamespace()) \u0026gt; 0 { return meta.GetNamespace() + \u0026#34;/\u0026#34; + meta.GetName(), nil } return meta.GetName(), nil } impliment Close 1 2 3 4 5 6 7 // Close the queue. func (f *DeltaFIFO) Close() { f.lock.Lock() //鎖不解釋 defer f.lock.Unlock() //退出解鎖不解釋 f.closed = true //設置關閉標記 f.cond.Broadcast() //通知所有等待的工人 } KeyOf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // KeyOf exposes f\u0026#39;s keyFunc, but also detects the key of a Deltas object or // DeletedFinalStateUnknown objects. // Keyof 就是把 DeltaFIFO 的私有變數 keyfunc 變形一個暴露，並且檢測 Object 轉換成Deltas 型態與 DeletedFinalStateUnknown 是否會出現問題 func (f *DeltaFIFO) KeyOf(obj interface{}) (string, error) { //嘗試轉換型態 if d, ok := obj.(Deltas); ok { if len(d) == 0 { return \u0026#34;\u0026#34;, KeyError{obj, ErrZeroLengthDeltasObject} } //取得 Deltas 中最新一筆資料 obj = d.Newest().Object } //嘗試轉換型態，如果是 DeletedFinalStateUnknown 型態的話，就直接回傳Object key，關於 DeletedFinalStateUnknown 是什麼後面會提到。 if d, ok := obj.(DeletedFinalStateUnknown); ok { return d.Key, nil } return f.keyFunc(obj) } 以下幾個 function 是對 store 的實作，因為 DeltaFIFO 同時也是一個儲存～\nAdd 1 2 3 4 5 6 7 8 // 把物件放入 FIFO queue 中 func (f *DeltaFIFO) Add(obj interface{}) error { f.lock.Lock() defer f.lock.Unlock() f.populated = true //標記有 object inqueue return f.queueActionLocked(Added, obj) //這個是重點，後面會看到 } Update 1 2 3 4 5 6 7 // 更新物件 func (f *DeltaFIFO) Update(obj interface{}) error { f.lock.Lock() defer f.lock.Unlock() f.populated = true //標記有 object inqueue return f.queueActionLocked(Updated, obj) //這個是重點，後面會看到 } Delete 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //刪除物件 func (f *DeltaFIFO) Delete(obj interface{}) error { id, err := f.KeyOf(obj) //透過 keyfunc 取得 object key if err != nil { return KeyError{obj, err} } f.lock.Lock() defer f.lock.Unlock() f.populated = true //標記有 object inqueue if f.knownObjects == nil { //如果沒有 indexer 的話就只要檢查該 object 在本地（detlas item map）有沒有資料 if _, exists := f.items[id]; !exists { return nil } } else { //有 indexer 的話就要確認 indexer 內有沒有資料 _, exists, err := f.knownObjects.GetByKey(id) _, itemsExist := f.items[id] if err == nil \u0026amp;\u0026amp; !exists \u0026amp;\u0026amp; !itemsExist { return nil } } return f.queueActionLocked(Deleted, obj) //這個是重點，後面會看到 } List 1 2 3 4 5 6 7 8 9 10 11 12 13 14 //列出所有 queue 的 object func (f *DeltaFIFO) List() []interface{} { f.lock.RLock() defer f.lock.RUnlock() return f.listLocked() //呼叫私有方法處理 } func (f *DeltaFIFO) listLocked() []interface{} { list := make([]interface{}, 0, len(f.items)) //建立slice for _, item := range f.items { //遞迴 delta item maps list = append(list, item.Newest().Object) //取出 delta 最新的資料，組合成slice } return list } ListKeys 1 2 3 4 5 6 7 8 9 10 //列出所有 queue 的 object key func (f *DeltaFIFO) ListKeys() []string { f.lock.RLock() defer f.lock.RUnlock() list := make([]string, 0, len(f.items)) //建立slice for key := range f.items { //遞迴 delta item maps list = append(list, key) //取出 delta item maps 每一個 key ，組合成slice } return list } Get 1 2 3 4 5 6 7 8 // 傳入 object ，透過 keyfunc 計算出 object key 再利用該 key 取的相對應 delta 的複製 slice func (f *DeltaFIFO) Get(obj interface{}) (item interface{}, exists bool, err error) { key, err := f.KeyOf(obj) //計算 object key if err != nil { return nil, false, KeyError{obj, err} } return f.GetByKey(key) //獲取對應的 delta slice } GetByKey 1 2 3 4 5 6 7 8 9 10 11 12 //以 object key 從 delta 紀錄中拿到相對的資料，取出的資料是 delta 的複製 slice func (f *DeltaFIFO) GetByKey(key string) (item interface{}, exists bool, err error) { f.lock.RLock() defer f.lock.RUnlock() d, exists := f.items[key] //從 deltas items map 中透過 object key 取的 deltas slice if exists { // Copy item\u0026#39;s slice so operations on this slice // won\u0026#39;t interfere with the object we return. d = copyDeltas(d) } return d, exists, nil } queueActionLocked 來到 store 最常出現的，我認為是比較重要的 queueActionLocked function ，可以看到上面許多function 如 Add Delete Update 都有用到這個 function ，我們就來好好看看他做了什麼事情吧！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 func (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface{}) error { //計算 object key id, err := f.KeyOf(obj) if err != nil { return KeyError{obj, err} } //從 deltas item map 中取出舊的 deltas 資料 oldDeltas := f.items[id] //把目標物件轉換成 delta 型態並且加入到既有的 deltas 資料後面 newDeltas := append(oldDeltas, Delta{actionType, obj}) //去除冗余的 deltas 資料 newDeltas = dedupDeltas(newDeltas) //去除冗余的 deltass 資料後，還要判斷 deltas 資料是不是被清空了（不過soure code 的註解寫不會發生這種情況XD） if len(newDeltas) \u0026gt; 0 { //判斷 deltas item map 有沒有資料，如果沒有資料的話，就要加到 fifo queue 中 if _, exists := f.items[id]; !exists { f.queue = append(f.queue, id) } // 如果 deltas item map 有資料表示 queue 裡面已經有了(還沒取走)，只要更新 delta item map就好 f.items[id] = newDeltas //發出果廣播告知卡在pop的人可以醒來取貨囉！ f.cond.Broadcast() } else { // This never happens, because dedupDeltas never returns an empty list // when given a non-empty list (as it is here). // If somehow it happens anyway, deal with it but complain. //上面原先的註解我就不拿掉，表示去除冗余資料後不會把 deltas slice 清空～如果真的發生的清空的話舊回報給社群然後還是要做處理xD if oldDeltas == nil { klog.Errorf(\u0026#34;Impossible dedupDeltas for id=%q: oldDeltas=%#+v, obj=%#+v; ignoring\u0026#34;, id, oldDeltas, obj) return nil } klog.Errorf(\u0026#34;Impossible dedupDeltas for id=%q: oldDeltas=%#+v, obj=%#+v; breaking invariant by storing empty Deltas\u0026#34;, id, oldDeltas, obj) f.items[id] = newDeltas return fmt.Errorf(\u0026#34;Impossible dedupDeltas for id=%q: oldDeltas=%#+v, obj=%#+v; broke DeltaFIFO invariant by storing empty Deltas\u0026#34;, id, oldDeltas, obj) } return nil } //去除 delta 冗余資料 func dedupDeltas(deltas Deltas) Deltas { //判斷 deltas slice 長度 ，若是小 2 沒什麼好去除的啊 n := len(deltas) if n \u0026lt; 2 { return deltas } //拿出倒數兩個來處理 a := \u0026amp;deltas[n-1] b := \u0026amp;deltas[n-2] //進行冗余比較 if out := isDup(a, b); out != nil { // `a` and `b` are duplicates. Only keep the one returned from isDup(). // TODO: This extra array allocation and copy seems unnecessary if all we do to dedup is compare the new delta with the last element in `items`, which could be done by mutating `items` directly. // Might be worth profiling and investigating if it is safe to optimize. d := append(Deltas{}, deltas[:n-2]...) return append(d, *out) } return deltas } //確認 a b delta 事件是否一樣，目前只能判斷 delete delta 事件 func isDup(a, b *Delta) *Delta { // 判斷 deletea delta 事件 if out := isDeletionDup(a, b); out != nil { return out } // TODO: Detect other duplicate situations? Are there any? return nil } // 判斷 delete delta 事件 func isDeletionDup(a, b *Delta) *Delta { //只要兩個其中一個不是 delete delta 事件，那就不是一樣的事件 if b.Type != Deleted || a.Type != Deleted { return nil } // 如果最後一個狀態是 DeletedFinalStateUnknown ，就回傳在奧數第二個狀態 a if _, ok := b.Object.(DeletedFinalStateUnknown); ok { return a } return b } 這裡有幾個部分直得我們思考\n為什麼 deltas 要做冗余合併？\n什麼樣的 deltas 資料可以進行冗余合併？\n我自己認為上述兩個問題得答案分別是\n由於加入 delta fifo 的速度與拿出 delta fifo 的速度是不一致的\nwatch kubernetes api （e.g. pod , deployment , service e.t.c）視為放入 delta fifo 處理這些資源變動的 controller 視為從 delta fifo 取出 如果某一個資源頻繁的更動 controller 還沒處理到，就會發生 object key 還在 queue 中，就要加入同一個 object key ，那這兩個動作就可以合併成一個。\n什麼樣的動作可以合併，就剛剛追蹤 source code 來看只有刪除的事件可以合併成一個，但為什麼是刪除事件呢？\n時間軸(time)\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;A\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;B\u0026mdash;\u0026mdash;\u0026mdash;-(time) 在Ａ時間點刪除物件，跟在B時間點刪除物件，可以將兩個事件聚合成如下圖所示 時間軸(time)\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-AB\u0026mdash;\u0026mdash;-(time) 就結論來看是一樣的 那為什麼只刪除可以，更新不行嗎？\n我是這樣認為的，我知道正不正確 時間軸(time)\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;A\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;B\u0026mdash;\u0026mdash;\u0026mdash;-(time) 在A時間點使用者改了replicas的數量，在B時間點使用者改了port number 要把兩個事件的內容聚合再一起，一定要有兩個事件更新的內容才可以聚合，不是不能做是不好做xD 你可能會問那新增呢，新增不香嗎，為什麼新增不能聚合？\n我認為可以，但就從 source code 來看 kubernetes 沒實作 時間軸(time)\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;A\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;B\u0026mdash;\u0026mdash;\u0026mdash;-(time) 在Ａ時間點新增物件，跟在B時間點新增物件，可以將兩個事件聚合成如下圖所示 時間軸(time)\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-AB\u0026mdash;\u0026mdash;-(time) 就結論來看是一樣的，我不知道為什麼 kubernetes 沒做 ，如果有知道的大大希望能說明一下原因 Replace 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 // Replace atomically does two things: (1) it adds the given objects using the Sync or Replace DeltaType and then (2) it does some deletions. // In particular: for every pre-existing key K that is not the key of // an object in `list` there is the effect of // `Delete(DeletedFinalStateUnknown{K, O})` where O is current object // of K. If `f.knownObjects == nil` then the pre-existing keys are // those in `f.items` and the current object of K is the `.Newest()` // of the Deltas associated with K. Otherwise the pre-existing keys // are those listed by `f.knownObjects` and the current object of K is // what `f.knownObjects.GetByKey(K)` returns. // replace做的有兩件事：（1）使用 Sync 或 Replace DeltaType 事件（2）刪除一些就的物件。 func (f *DeltaFIFO) Replace(list []interface{}, resourceVersion string) error { f.lock.Lock() defer f.lock.Unlock() //建立key set keys := make(sets.String, len(list)) // 使用 sync 標記 action := Sync //若是設定為可以用replace，那標記成replace if f.emitDeltaTypeReplaced { action = Replaced } // 為每個Object 轉換成 replace/sync delta 型態 for _, item := range list { //計算 object key key, err := f.KeyOf(item) if err != nil { return KeyError{item, err} } //插入object key keys.Insert(key) //處理 object key enqueue 以及設定 object ket map 的 delta object 對應 if err := f.queueActionLocked(action, item); err != nil { return fmt.Errorf(\u0026#34;couldn\u0026#39;t enqueue object: %v\u0026#34;, err) } } //如果沒有設定indexer的話，就要對自己儲存的進行檢查 if f.knownObjects == nil { // Do deletion detection against our own list. queuedDeletions := 0 //遞迴所有的 deltas items map for k, oldItem := range f.items { //如果新的 key set 裡面有 deltas map 所包含的 key 的話，就忽略。(deltas items map 有 新的 key set 的 key 下次只要更新就好) if keys.Has(k) { continue } // Delete pre-existing items not in the new list. // This could happen if watch deletion event was missed while // disconnected from apiserver. //刪除不再新的 object key sey 內的 object ，並將其標記成 DeletedFinalStateUnknown。 var deletedObj interface{} if n := oldItem.Newest(); n != nil { deletedObj = n.Object } //累加不在新的 object key sey 內的 object，總共有多少個 queuedDeletions++ //處理 object key enqueue 以及設定 object ket map 的 delta object 對應(設定成DeletedFinalStateUnknown) if err := f.queueActionLocked(Deleted, DeletedFinalStateUnknown{k, deletedObj}); err != nil { return err } } //如果populated 為 false 表示還沒有人操作（add delete update）過 delta fifo，這是第一次 replace。 if !f.populated { //標記inqueue f.populated = true // While there shouldn\u0026#39;t be any queued deletions in the initial population of the queue, it\u0026#39;s better to be on the safe side. //initialPopulationCount 表示初始化要pop多少數量是由queue的長度決定 //這邊比較弔詭的是如果沒有 delete 操作過（）也就是 populated=false 那應該不會有 queuedDeletions（也就是queuedDeletions＝0） ，註解是寫為了保險~ f.initialPopulationCount = len(list) + queuedDeletions } return nil } // 從 indexer 中取得所有的 object key knownKeys := f.knownObjects.ListKeys() queuedDeletions := 0 // 遞迴 indexer 中所有的 object key for _, k := range knownKeys { //如果新的object key set 有indexer object key 的話就忽略不處理(表示已經有delta item maps 已經有資料了) if keys.Has(k) { continue } // 新的 object key set 沒有 indexer object key 的話 // 從 indexer 透過 object key 取的 object deletedObj, exists, err := f.knownObjects.GetByKey(k) if err != nil { deletedObj = nil klog.Errorf(\u0026#34;Unexpected error %v during lookup of key %v, placing DeleteFinalStateUnknown marker without object\u0026#34;, err, k) } else if !exists { deletedObj = nil klog.Infof(\u0026#34;Key %v does not exist in known objects store, placing DeleteFinalStateUnknown marker without object\u0026#34;, k) } // 計數 indexer 有的 key ，但新的 object key set 沒有的 queuedDeletions++ //處理 object key enqueue 以及設定 object ket map 的 delta object 對應(設定成DeletedFinalStateUnknown) if err := f.queueActionLocked(Deleted, DeletedFinalStateUnknown{k, deletedObj}); err != nil { return err } } //如果populated 為 false 表示還沒有人操作（add delete update）過 delta fifo，這是第一次 replace。 if !f.populated { f.populated = true //這邊比較弔詭的是如果沒有 delete 操作過（）也就是 populated=false 那應該不會有 queuedDeletions（也就是queuedDeletions＝0） ，註解是寫為了保險~ f.initialPopulationCount = len(list) + queuedDeletions } return nil } 以上就是 store 的實作，說簡單不簡單說困難倒也還好。就是要花點時間整理思路了解到底儲存了什麼，怎麼存的已經批次取代要標記成DeletedFinalStateUnknown的狀態。\n接下來要來說說要怎麼把物件彈出 fifo queue～\nPop 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 func (f *DeltaFIFO) Pop(process PopProcessFunc) (interface{}, error) { f.lock.Lock() defer f.lock.Unlock() //不斷循環等有資料可以取 for { //如果 fifo queue 沒有資料了 for len(f.queue) == 0 { //需要先判斷 fifo queue 是不是已經關閉了 if f.closed { return nil, ErrFIFOClosed } //工人需要在這裡等有資料，在加入資料時工人會被喚醒 f.cond.Wait() } //把第一筆資料pop出來 id := f.queue[0] f.queue = f.queue[1:] //todo if f.initialPopulationCount \u0026gt; 0 { f.initialPopulationCount-- } //從 deltas item map 找到對應的 delta slice item, ok := f.items[id] //註解也寫了這不會發生xD，deltas items map一定找得到資料的意思（但也不處理錯誤讓他跳過去） if !ok { // This should never happen klog.Errorf(\u0026#34;Inconceivable! %q was in f.queue but not f.items; ignoring.\u0026#34;, id) continue } //pop出一個 delta 資料後 需要把 delta 從 deltas items map刪除 delete(f.items, id) //外部處理 delta 資料的 function err := process(item) //如果有錯就需要從新加入 queue 中 if e, ok := err.(ErrRequeue); ok { f.addIfNotPresent(id, item) err = e.Err } // Don\u0026#39;t need to copyDeltas here, because we\u0026#39;re transferring ownership to the caller. return item, err } } 在 process delta 資料的時候若是發生錯誤，需要再把資料加入 fifo queue 中，我們來看一下怎麼再把資料重新加到 fifo queue 中。\nAddIfNotPresent 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 // AddIfNotPresent inserts an item, and puts it in the queue. If the item is already // present in the set, it is neither enqueued nor added to the set. // // This is useful in a single producer/consumer scenario so that the consumer can // safely retry items without contending with the producer and potentially enqueueing // stale items. // // Important: obj must be a Deltas (the output of the Pop() function). Yes, this is // different from the Add/Update/Delete functions. func (f *DeltaFIFO) AddIfNotPresent(obj interface{}) error { //把物件轉換成 deltas 型態 deltas, ok := obj.(Deltas) if !ok { return fmt.Errorf(\u0026#34;object must be of type deltas, but got: %#v\u0026#34;, obj) } //取得 deltas 物件最新的 object key id, err := f.KeyOf(deltas.Newest().Object) if err != nil { return KeyError{obj, err} } f.lock.Lock() defer f.lock.Unlock() //透過私有方法重新enque物件 f.addIfNotPresent(id, deltas) return nil } // addIfNotPresent inserts deltas under id if it does not exist, and assumes the caller // already holds the fifo lock. func (f *DeltaFIFO) addIfNotPresent(id string, deltas Deltas) { f.populated = true //標記有 object inqueue if _, exists := f.items[id]; exists { //需要先確認 object 使否存在於 deltas map 中，如果存在表示 fifo queue 裡面還有資料 return } //重新加入 fifo queue f.queue = append(f.queue, id) //對應deltas 與 deltas map 關係 f.items[id] = deltas //通知pop工人可以來取資料 f.cond.Broadcast() } Resync 不是很確定用途，不過從 source code 分析結果來看是同步 indexer 裡面的資料？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 func (f *DeltaFIFO) Resync() error { f.lock.Lock() defer f.lock.Unlock() //如果沒有 indexer 就不需要處理 if f.knownObjects == nil { return nil } //從 indexer 中取出每一個 object key set keys := f.knownObjects.ListKeys() //遞迴 indexer object key set for _, k := range keys { //取出 object key 透過 syncKeyLocked ，下面會看到～ if err := f.syncKeyLocked(k); err != nil { return err } } return nil } //實作 object sync 的方法～ func (f *DeltaFIFO) syncKeyLocked(key string) error { //透過 indexer 使用 object 取得對應的 object key obj, exists, err := f.knownObjects.GetByKey(key) if err != nil { klog.Errorf(\u0026#34;Unexpected error %v during lookup of key %v, unable to queue object for sync\u0026#34;, err, key) return nil } else if !exists { klog.Infof(\u0026#34;Key %v does not exist in known objects store, unable to queue object for sync\u0026#34;, key) return nil } // If we are doing Resync() and there is already an event queued for that object, // we ignore the Resync for it. This is to avoid the race, in which the resync // comes with the previous value of object (since queueing an event for the object // doesn\u0026#39;t trigger changing the underlying store \u0026lt;knownObjects\u0026gt;. //透過key func 計算 object key id, err := f.KeyOf(obj) if err != nil { return KeyError{obj, err} } //如果 deltas item map 已經有資料了就不處理 if len(f.items[id]) \u0026gt; 0 { return nil } //標記已sync的方式加入fifo queue中 if err := f.queueActionLocked(Sync, obj); err != nil { return fmt.Errorf(\u0026#34;couldn\u0026#39;t queue object: %v\u0026#34;, err) } return nil } HasSynced 1 2 3 4 5 6 //主要是 object 放放入 DeltaFIFO 後，並且全部的 object 都被 Pop() ， HasSynced fucntion 才會回傳 true func (f *DeltaFIFO) HasSynced() bool { f.lock.Lock() defer f.lock.Unlock() return f.populated \u0026amp;\u0026amp; f.initialPopulationCount == 0 } 小整理 可以把使用者在呼叫 Delta FIFO 的 Add() 、 Delete() 、 Update() 、 Replaced() 、 Sync() 簡化成下圖的結構，應該會比較方便理解。\n而使用者呼叫 Delta FIFO 的 Pop（）則可以簡化成下圖的結構\n怎麼使用 真正的 Operator / Controller 使用 DeltaFIFO 還需要知道其他背景知識，我先以幾個簡單的 DeltaFIFO 的 unit test code 來做範例，讓大家了解delta fifo 使用起來大概是怎麼一回事！\npop 簡單的物件彈出，但是彈出之前 deltas fifo 內必須要有物件才行，接下來看看 test 是怎麼做的吧！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 func TestDeltaFIFO_requeueOnPop(t *testing.T) { //建立一個delta FIFO queue ，怎麼產生 object key 的不是很重要～ f := NewDeltaFIFO(testFifoObjectKeyFunc, nil) //加入了一個 object ，可以想像成 key = foo , value =10 f.Add(mkFifoObj(\u0026#34;foo\u0026#34;, 10)) //加入之後，就直接pop出來用～ _, err := f.Pop(func(obj interface{}) error { //只是簡單看一下物件是不是正常而已 if obj.(Deltas)[0].Object.(testFifoObject).name != \u0026#34;foo\u0026#34; { t.Fatalf(\u0026#34;unexpected object: %#v\u0026#34;, obj) } //這裡在處理過程故意讓他產生錯誤，再次重新放入 delta fifo （注意這裡的error一定回傳ErrRequeue，才有辦法requeued） return ErrRequeue{Err: nil} }) //檢查一下pop有沒有出現問題 if err != nil { t.Fatalf(\u0026#34;unexpected error: %v\u0026#34;, err) } //透過 KeyGetter interface 的 GetByKey 透過 foo key 拿物件 //這時候應該要拿的到，因為物件剛剛處理錯誤又重新 requeued 了（一般來說只要被pop出來 deltas item map 對應的 key 理論上要被清空） if _, ok, err := f.GetByKey(\u0026#34;foo\u0026#34;); !ok || err != nil { t.Fatalf(\u0026#34;object should have been requeued: %t %v\u0026#34;, ok, err) } //再次pop出來 _, err = f.Pop(func(obj interface{}) error { if obj.(Deltas)[0].Object.(testFifoObject).name != \u0026#34;foo\u0026#34; { t.Fatalf(\u0026#34;unexpected object: %#v\u0026#34;, obj) } //這裡在處理過程故意讓他產生錯誤，再次重新放入 delta fifo （注意這裡的error一定回傳ErrRequeue，才有辦法requeued） return ErrRequeue{Err: fmt.Errorf(\u0026#34;test error\u0026#34;)} }) // 檢查 pop 出來的錯誤訊息 if err == nil || err.Error() != \u0026#34;test error\u0026#34; { t.Fatalf(\u0026#34;unexpected error: %v\u0026#34;, err) } //透過 KeyGetter interface 的 GetByKey 透過 foo key 拿物件 //這時候應該要拿的到，因為物件剛剛處理錯誤又重新 requeued 了（一般來說只要被pop出來 deltas item map 對應的 key 理論上要被清空） if _, ok, err := f.GetByKey(\u0026#34;foo\u0026#34;); !ok || err != nil { t.Fatalf(\u0026#34;object should have been requeued: %t %v\u0026#34;, ok, err) } //再次pop出來 _, err = f.Pop(func(obj interface{}) error { if obj.(Deltas)[0].Object.(testFifoObject).name != \u0026#34;foo\u0026#34; { t.Fatalf(\u0026#34;unexpected object: %#v\u0026#34;, obj) } // 這次順利地處理完成～就不觸發 requeue return nil }) // 檢查 pop 出來的錯誤訊息 if err != nil { t.Fatalf(\u0026#34;unexpected error: %v\u0026#34;, err) } //這時候應該要拿不到，一般來說只要被pop出來 deltas item map 對應的 key 理論上要被清空。 if _, ok, err := f.GetByKey(\u0026#34;foo\u0026#34;); ok || err != nil { t.Fatalf(\u0026#34;object should have been removed: %t %v\u0026#34;, ok, err) } } resync 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 func TestDeltaFIFO_ResyncNonExisting(t *testing.T) { //先建立一個 delta fifo queue，怎麼計算 object key 的不重要，另外還放入了類似 indexer 的物件（實作 KeyListerGetter interface），這裡可以來看一下 literalListerGetter 這是什麼東西（幫我滾輪滾到下面一點看xD） f := NewDeltaFIFO( testFifoObjectKeyFunc, literalListerGetter(func() []testFifoObject { return []testFifoObject{mkFifoObj(\u0026#34;foo\u0026#34;, 5)} }), ) //這裡加入一個刪除的事件 f.Delete(mkFifoObj(\u0026#34;foo\u0026#34;, 10)) //觸發sync f.Resync() //看看 deltas item map foo 目前所代表的數值是什麼，這時候因為出發過resync //deltas 資料應該會是 event Sync ,key foo ,value 5 deltas := f.items[\u0026#34;foo\u0026#34;] if len(deltas) != 1 { t.Fatalf(\u0026#34;unexpected deltas length: %v\u0026#34;, deltas) } if deltas[0].Type != Deleted { t.Errorf(\u0026#34;unexpected delta: %v\u0026#34;, deltas[0]) } } //定義了一個 function type ，並且指名要回傳一個 testFifoObject 型態的slice，testFifoObject前面有提到他就是一個 key value。 type literalListerGetter func() []testFifoObject //在coding 階段就可以確認 literalListerGetter 有沒有實作 KeyListerGetter var _ KeyListerGetter = literalListerGetter(nil) //實作 literalListerGetter 就簡單的列出key func (kl literalListerGetter) ListKeys() []string { result := []string{} for _, fifoObj := range kl() { result = append(result, fifoObj.name) } return result } //實作 literalListerGetter 就透過 key簡單的列出value func (kl literalListerGetter) GetByKey(key string) (interface{}, bool, error) { for _, v := range kl() { if v.name == key { return v, true, nil } } return nil, false, nil } replace 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 func TestDeltaFIFO_ReplaceMakesDeletionsReplaced(t *testing.T) { //先建立一個 delta fifo queue，怎麼計算 object key 的不重要，另外還放入了類似 indexer 的物件（實作 KeyListerGetter interface） //上面有提過可以再回去看一下 //另外這邊打開了 EmitDeltaTypeReplaced f := NewDeltaFIFOWithOptions(DeltaFIFOOptions{ KeyFunction: testFifoObjectKeyFunc, KnownObjects: literalListerGetter(func() []testFifoObject { return []testFifoObject{mkFifoObj(\u0026#34;foo\u0026#34;, 5), mkFifoObj(\u0026#34;bar\u0026#34;, 6), mkFifoObj(\u0026#34;baz\u0026#34;, 7)} }), EmitDeltaTypeReplaced: true, }) //加入一個刪除的事件 f.Delete(mkFifoObj(\u0026#34;baz\u0026#34;, 10)) //觸發replace事件，帶入要replace的物件 f.Replace([]interface{}{mkFifoObj(\u0026#34;foo\u0026#34;, 6)}, \u0026#34;0\u0026#34;) //預期replace後的結果，基本上就是把指定要replace的物件 inqueue //indexer 內 replace 有的就標記成replaced //indexer 內 replace 沒有的就標記成deleted //中間如有 add 或是 update 過就會有兩筆資料，一筆是 add/update 一筆是 deleted～ expectedList := []Deltas{ {{Deleted, mkFifoObj(\u0026#34;baz\u0026#34;, 10)}}, {{Replaced, mkFifoObj(\u0026#34;foo\u0026#34;, 6)}}, // Since \u0026#34;bar\u0026#34; didn\u0026#39;t have a delete event and wasn\u0026#39;t in the Replace list // it should get a tombstone key with the right Obj. {{Deleted, DeletedFinalStateUnknown{Key: \u0026#34;bar\u0026#34;, Obj: mkFifoObj(\u0026#34;bar\u0026#34;, 6)}}}, } for _, expected := range expectedList { cur := Pop(f).(Deltas) if e, a := expected, cur; !reflect.DeepEqual(e, a) { t.Errorf(\u0026#34;Expected %#v, got %#v\u0026#34;, e, a) } } } 從上述幾個 test case ，可以看到 DeltaFIFO 基本上怎麼使用，需要承上啟下了解 DeltaFIFO 在 controler/operator 的作用還需要了解其他元件如 Reflector怎麼把資料送進來的，所以這裡怎麼使用 Delta FIFO 使用 test case 來呈現。\n小結 在本章節我們快速的了解了 Delta FIFO 的作用，除了可以作為一個 FIFO 的 Queue 之外，還有本地儲存的功能（儲存了物件的變化事件，如Add、Delete、Update等），一般來說還會把資料拋給 indexer 做儲存，大致上就是這些特性。\n了解了 Delta FIFO的這些特性後我們要繼續往上看Reflector怎麼把資料送進來，我認為這邊有點複雜牽扯到反序列化的過程，中間可能會有錯誤的見解希望各位在觀看文章的大大們可以指出哪裡有問題，讓我學習改進，謝謝。\n","description":"","id":25,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes DeltaFIFO 承上啟下","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/deltafifo/kubernetes-delta-fifo-queue/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\ncache 重新複習一次前一章Kubernetes Indexers localcache 之美（I）提到的 cache 相關的內容\nstruct 先來看他的 cache 的資料結構， cache 組合了 ThreadSafeStore 以及 KeyFunc\n1 2 3 4 5 6 7 8 9 // `*cache` implements Indexer in terms of a ThreadSafeStore and an // associated KeyFunc. type cache struct { // cacheStorage bears the burden of thread safety for the cache cacheStorage ThreadSafeStore //一個 thread safe 的 interface // keyFunc is used to make the key for objects stored in and retrieved from items, and // should be deterministic. keyFunc KeyFunc //計算object key的方法，稍後會解釋 } 複習完資料結構後，看看如何新增一個有實作 Store Interface 的 cache 物件。\nnew function 分別有兩種 function\n第一種為傳入 KeyFunc 也就是只傳入 Object Key 如何計算告訴 cache object key 如何計算，其他的 Indexers 以及 Indices 使用預設的。\n第二種為傳入 KeyFunc 以及 Indexers ，也就是告訴 cache Object Key 如何計算方法以及儲存 KeyFunc 的 Indexers 採用哪一種。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // NewStore returns a Store implemented simply with a map and a lock. func NewStore(keyFunc KeyFunc) Store { return \u0026amp;cache{ cacheStorage: NewThreadSafeStore(Indexers{}, Indices{}), keyFunc: keyFunc, } } // NewIndexer returns an Indexer implemented simply with a map and a lock. func NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer { return \u0026amp;cache{ cacheStorage: NewThreadSafeStore(indexers, Indices{}), keyFunc: keyFunc, } } 我們先來看看 cache 實作了哪些方法再來探討怎麼使用 cache\nimpliment 這裡的 Function 幾乎依賴 ThreadSafeStore 的實作，如果不了解 ThreadSafeStore 做了什麼的朋友可以回到前一章Kubernetes Indexers localcache 之美（I）複習一下！\nAdd 1 2 3 4 5 6 7 8 9 10 11 12 // Add inserts an item into the cache. func (c *cache) Add(obj interface{}) error { //透過 key function 計算出 object 所對應的 key key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore Add function 這裡的細節可以去上章節複習。 //簡單來說就更新 local cache 的物件以及更新 indices 以及 index c.cacheStorage.Add(key, obj) return nil } Update 1 2 3 4 5 6 7 8 9 10 11 12 // Update sets an item in the cache to its updated state. func (c *cache) Update(obj interface{}) error { //透過 key function 計算出 object 所對應的 key key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore Update function 這裡的細節可以去上一章節複習。 //簡單來說就更新 local cache 的物件以及更新 indices 以及 index c.cacheStorage.Update(key, obj) return nil } Delete 1 2 3 4 5 6 7 8 9 10 11 12 // Delete removes an item from the cache. func (c *cache) Delete(obj interface{}) error { //透過 key function 計算出 object 所對應的 key key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore Delete 這裡的細節可以去上一章節複習。 //簡單來說就刪除 local cache 的物件以及更新 indices 以及 index c.cacheStorage.Delete(key) return nil } List 1 2 3 4 5 6 7 // List returns a list of all the items. // List is completely threadsafe as long as you treat all items as immutable. func (c *cache) List() []interface{} { //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore List 這裡的細節可以去上一章節複習。 //簡單來說就是列出 local cache 所有 Object return c.cacheStorage.List() } ListKeys 1 2 3 4 5 6 7 // ListKeys returns a list of all the keys of the objects currently // in the cache. func (c *cache) ListKeys() []string { //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore ListKeys 這裡的細節可以去上一章節複習。 //簡單來說就是列出 local cache 所有 Object Key return c.cacheStorage.ListKeys() } GetIndexers 1 2 3 4 5 6 // GetIndexers returns the indexers of cache func (c *cache) GetIndexers() Indexers { //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore GetIndexers 這裡的細節可以去上一章節複習。 //簡單來說就是列出 loacl cache 目前有的所有 Indexer return c.cacheStorage.GetIndexers() } Index 1 2 3 4 5 6 7 // Index returns a list of items that match on the index function // Index is thread-safe so long as you treat all items as immutable func (c *cache) Index(indexName string, obj interface{}) ([]interface{}, error) { //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore Index 這裡的細節可以去上一章節複習。 //簡單來說就是透過以知道的index name 以及 Object 合作幫忙分類列出 loacl cache 相關的 Object return c.cacheStorage.Index(indexName, obj) } List 1 2 3 4 5 func (c *cache) IndexKeys(indexName, indexKey string) ([]string, error) { //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore IndexKeys 這裡的細節可以去上一章節複習。 //簡單來說就是透過已知道 index name 以及 indexkey 合作幫忙分類列出 loacl cache 上相關的 Objecy Key return c.cacheStorage.IndexKeys(indexName, indexKey) } List 1 2 3 4 5 6 // ListIndexFuncValues returns the list of generated values of an Index func func (c *cache) ListIndexFuncValues(indexName string) []string { //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore ListIndexFuncValues 這裡的細節可以去上一章節複習。 //簡單來說就是透過已知道 index name 幫忙分類列出 loacl cache 上負責計算Object key 的名字也就是 indexed function name return c.cacheStorage.ListIndexFuncValues(indexName) } ByIndex 1 2 3 4 5 func (c *cache) ByIndex(indexName, indexKey string) ([]interface{}, error) { //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore ByIndex 這裡的細節可以去上一章節複習。 //簡單來說就是透過已知道 index name 以及 index key 合作幫忙分類列出 loacl cache 上所有有關的 Object return c.cacheStorage.ByIndex(indexName, indexKey) } AddIndexers 1 2 3 4 5 func (c *cache) AddIndexers(newIndexers Indexers) error { //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore AddIndexers 這裡的細節可以去上一章節複習。 //簡單來說就是新增一個 Indexer return c.cacheStorage.AddIndexers(newIndexers) } Get 1 2 3 4 5 6 7 8 9 10 11 // Get returns the requested item, or sets exists=false. // Get is completely threadsafe as long as you treat all items as immutable. func (c *cache) Get(obj interface{}) (item interface{}, exists bool, err error) { //透過 key function 計算出 Object 所代表的 object key key, err := c.keyFunc(obj) if err != nil { return nil, false, KeyError{obj, err} } //使用該 Object key 來取得 Object return c.GetByKey(key) } GetByKey 1 2 3 4 5 6 7 8 // GetByKey returns the request item, or exists=false. // GetByKey is completely threadsafe as long as you treat all items as immutable. func (c *cache) GetByKey(key string) (item interface{}, exists bool, err error) { //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore Get 這裡的細節可以去上一章節複習。 //簡單來說就是使用 Object key 從 local stoarge 取得相關的 Object item, exists = c.cacheStorage.Get(key) return item, exists, nil } ReplaceList 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // Replace will delete the contents of \u0026#39;c\u0026#39;, using instead the given list. // \u0026#39;c\u0026#39; takes ownership of the list, you should not reference the list again // after calling this function. func (c *cache) Replace(list []interface{}, resourceVersion string) error { // 傳入一堆 object 以及這批物件的版本 //建立一個 item slice 進行遞迴計算他們的Object key items := make(map[string]interface{}, len(list)) for _, item := range list { key, err := c.keyFunc(item) if err != nil { return KeyError{item, err} } items[key] = item } //委任給 cacheStorage 也就是 threadSafeStore 去做處理，threadSafeStore Replace 這裡的細節可以去上一章節複習。 //簡單來說就是使用 這一坨 Object 直接對 原有的進行取代 c.cacheStorage.Replace(items, resourceVersion) return nil } Resync 1 2 3 4 5 // Resync is meaningless for one of these func (c *cache) Resync() error { //沒做事xD return nil } 快速地看完 cache 實作了什麼，絕大多數的 function 都是依賴著更加底層的 threadSafeStore 去完成，有了上一章節的邏輯整理 cache 實作的部分很快地就可以掃過去!\n接著讓我來看看外面怎麼來使用 cache 幫忙儲存以及取出資料吧！\nhow to use client go 官方有範例其中有一段用到 cache 的初始化 function NewIndexer ，我們先來看看怎麼用，以及他傳入什麼參數。\nsource code 1 2 3 4 5 6 7 8 9 10 //cache package 中的 NewIndexerInformer是我們要關注的重點 //傳入的參數我們先不要管那麼多菜後續的章節會慢慢的頗析到 indexer, informer := cache.NewIndexerInformer( podListWatcher, \u0026amp;v1.Pod{}, 0, cache.ResourceEventHandlerFuncs{ ... }, cache.Indexers{}) 上段 source code 中提到的 cache package 中的 NewIndexerInformer 是我們要關注的重點是一個重要的 function ，我們先來看看這個 function 裡面寫了什麼吧\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 //傳入數值目前不是很重要，我們需要把重要放到 NewIndexer 來 //可以看到 NewIndexer 傳入了 DeletionHandlingMetaNamespaceKeyFunc 以及 cache package 中預設的 indexers。 //表示所有的物件都會透過 DeletionHandlingMetaNamespaceKeyFunc 來計算Object key 並且存放在 local cache 中。 func NewIndexerInformer( lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration, h ResourceEventHandler, indexers Indexers, ) (Indexer, Controller) { // This will hold the client state, as we know it. clientState := NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers) return clientState, newInformer(lw, objType, resyncPeriod, h, clientState) } 我們再來看看其他用法，雖然這個方法在 Kubernetes 中非常少被用到但值得我們去了解。\n1 2 3 4 5 6 7 8 9 10 11 12 13 //傳入數值目前不是很重要，我們需要把重要放到 NewStore 來 //一樣這邊傳入DeletionHandlingMetaNamespaceKeyFunc 來處理 Object Key 的計算 func NewInformer( lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration, h ResourceEventHandler, ) (Store, Controller) { // This will hold the client state, as we know it. clientState := NewStore(DeletionHandlingMetaNamespaceKeyFunc) return clientState, newInformer(lw, objType, resyncPeriod, h, clientState) } 小結 大家可能比較不了解的地方在於 Object key 計算的方法，我在 Kubernetes source code 中挖掘了很久找到一個實作 keyFunc 的 function ，不確定還有沒有其他實作 keyFunc 的 function 。\nMetaNamespaceIndexFunc\nsource code 1 2 3 4 5 6 7 8 // MetaNamespaceIndexFunc is a default index function that indexes based on an object\u0026#39;s namespace func MetaNamespaceIndexFunc(obj interface{}) ([]string, error) { meta, err := meta.Accessor(obj) if err != nil { return []string{\u0026#34;\u0026#34;}, fmt.Errorf(\u0026#34;object has no meta: %v\u0026#34;, err) } return []string{meta.GetNamespace()}, nil } 我就不展開來細看裡面的實作，不就大致上能看出從 Object 拿到 metadata 中的 namespace ，以 namespace 作為 indexed name 非常簡單\n了解了 cache 的實作以及底層的 threadSafeMap 與如何計算 Object key 的 KeyFunc 後，下一篇我想探討 DeltaFIFO 的區塊， 可以看到 DeltaFiFo如何跟 cache 進行資料的交換，文章若寫錯誤的地方還請各位大大提出，感謝！\n","description":"","id":26,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes Indexers localcache 之美（II）","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/indexer/kubernetes-indexers-local-cache-2/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n上一篇Kubernetes Controller 設計挖掘之路 提到為什麼我們要了解 kubernetes 內部其他元件的運作方式，例如 work queue , indexer 或是informer， indexer 作為 controller 內保存物件資料的地方執得我們去了解它的設計思維與架構。\nIndexer Indexer 位於 client-go 的 cache package，從字面上來看他就是一個索引器，位於 infromer 之中作為 informer 的 local cache，至於為什麼會是使用 indexer 作為命名的方法，我猜是透過某些條件查詢儲存的物件加快整體的速度吧？（如果有錯麻煩大大糾正）\ninterface kubernetes source code 設計得非常精美，我們可以先從 interface 定義了哪些方法來推敲實作這個 interface 的物件可能有什麼功能。\nsource-code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // Indexer extends Store with multiple indices and restricts each // accumulator to simply hold the current object (and be empty after // Delete). // // There are three kinds of strings here: // 1. a storage key, as defined in the Store interface, // 2. a name of an index, and // 3. an \u0026#34;indexed value\u0026#34;, which is produced by an IndexFunc and // can be a field value or any other string computed from the object. type Indexer interface { //嵌入了stroe，擁有stroe全部的方法 Store // 回傳在indexName內與給定的Object有交集的所有object Index(indexName string, obj interface{}) ([]interface{}, error) // 回傳在indexname與透過indexed取的所有關聯的Object Key IndexKeys(indexName, indexedValue string) ([]string, error) // 回傳在indexName所有的indexed ListIndexFuncValues(indexName string) []string // 回傳在indexName與透過indexed回傳所有關聯的object ByIndex(indexName, indexedValue string) ([]interface{}, error) // 回傳indexer GetIndexers() Indexers // 新增indexer AddIndexers(newIndexers Indexers) error } 研究到這裡我真的很痛苦一下Index 一下 Indexer 一下Obejct 一下Obejct key 一下Indexed！！！神煩xD\n好AnyWay總之有五種東西分別是\nIndexer Indexed Obejct Obejct key Index 本系列會幫這些名稱進行一個分類與說明，希望能夠幫助到正在痛苦研究的夥伴\u0026hellip;.\n回歸正題可以看到 Indexer 嵌入了 Store Interface ，就名稱來看 Indexer 有基礎的儲存能力，至於實際上有什麼儲存能力我們繼續看 Store Interface 定義了什麼方法。\nsource-code\n可以從 Store interface 中簡單的看到有新增、刪除、修改等動作，由於操作的事Key Value store所以會有，列出Key 列出物件等動作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 type Store interface { //新增物件 Add(obj interface{}) error //更新物件 Update(obj interface{}) error //刪除物件 Delete(obj interface{}) error //列出所有物件 List() []interface{} //列出所有物件的key ListKeys() []string // 透過物件取得object key Get(obj interface{}) (item interface{}, exists bool, err error) //透過object key取得object GetByKey(key string) (item interface{}, exists bool, err error) //一次性替換Object key 所代表的所有Object Replace([]interface{}, string) error //重新同步 Resync() error } 看完了抽象的定義之後，必須要回過來看 cache 實際物件定義了哪些屬性\nstruct 只有 cache 這個物件實作了 Indexer Interface，我們先來分析這傢伙！\ncache source-code\n1 2 3 4 5 6 7 type cache struct { // cacheStorage bears the burden of thread safety for the cache cacheStorage ThreadSafeStore //一個 thread safe 的 interface ，稍後會解釋。 // keyFunc is used to make the key for objects stored in and retrieved from items, and // should be deterministic. keyFunc KeyFunc //計算object key的方法，稍後會解釋 } cache 組合 ThreadSafeStore 以及 KeyFunc ，我們繼續展開來看這兩個是什麼東西！\nThreadSafeStore 先來看 ThreadSafeStore interface 定義了什麼東西，從中來了解實作他的物件可能有什麼行為。\ninterface 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type ThreadSafeStore interface { Add(key string, obj interface{}) Update(key string, obj interface{}) Delete(key string) Get(key string) (item interface{}, exists bool) List() []interface{} ListKeys() []string Replace(map[string]interface{}, string) Index(indexName string, obj interface{}) ([]interface{}, error) IndexKeys(indexName, indexKey string) ([]string, error) ListIndexFuncValues(name string) []string ByIndex(indexName, indexKey string) ([]interface{}, error) GetIndexers() Indexers // AddIndexers adds more indexers to this store. If you call this after you already have data // in the store, the results are undefined. AddIndexers(newIndexers Indexers) error // Resync is a no-op and is deprecated Resync() error } 快速地看過一次應該會覺得跟上面提到的 Store Interface 非常相似，我認為大致上的差異在於 ThreadSafeStroe 主要針對 Index 做處理，我們看一下UML會更加清楚。\n在使用者面向的應該是 Store 這個 Interface ， cache 把功能委任（委託）給 cacheStorage Interface 有Thread safe 特性且有索引的 storage 。\n看完了抽象的定義之後，必須要回過來看 threadSafeMap 實際物件定義了哪些屬性\nstruct 只有 threadSafeMap 這個物件實作了 cacheStorage Interface，我們接著來分析threadSafeMap！\nsource-code\n1 2 3 4 5 6 7 8 9 10 // threadSafeMap implements ThreadSafeStore type threadSafeMap struct { lock sync.RWMutex //讀寫鎖（可能讀多寫少？） items map[string]interface{} //儲存Object Key:Object // indexers maps a name to an IndexFunc indexers Indexers //用以計算indexed的function // indices maps a name to an Index indices Indices //indexed map透過這個表獲取Onject Key } 我知道頭很痛，又出現 Index ， indexed 這一坨東西，我會盡力講解些東西的關聯，先看看在 Indexers Indices 這些東西在 client go 裡面的定義吧，從中我們可以獲取一些靈感。\nsource-code\n1 2 3 4 5 6 7 8 9 10 11 12 13 // IndexFunc knows how to compute the set of indexed values for an object. type IndexFunc func(obj interface{}) ([]string, error) //傳入Obaject會幫你算出多種[]indexed-name // Index maps the indexed value to a set of keys in the store that match on that value type Index map[string]sets.String //每個indexed-name會對應到一個[set]object key set //inedxed-\u0026gt;object key set // Indexers maps a name to a IndexFunc type Indexers map[string]IndexFunc //計算 indexed 的 function 們（透過 name 去分類） //index name-\u0026gt;index function-\u0026gt;index // Indices maps a name to an Index type Indices map[string]Index //index name(function name)-\u0026gt;index[indexed-name]-\u0026gt;[set]object key 把上面的source code轉換成圖形應該會變成這樣子，\n從最上面的部分往下看\nIndexer 裡面有許多 index-name 分別對應著不同的 index function index function 算出多個 indexed-name 從 Indices 內透過 index-name 可以拿到一個 Index-name 所對應的 index 從 Index 內透過 inexed-name 可以獲得一組 Object key set 好了解到這裡後，應該會有一點概念吧?XD\n這一邊我讀了很久才看懂一點，如果分析有錯希望各位大大可以指出！感謝\n接著我們來看 New Function 要怎麼把 ThreadSafeStore 建立出來\nnew Function 可以看到 ThreadSafeStore 要求傳入一個 indexer 以及 indeices，這邊可以對照著 client go 中有使用到 NewThreadSafeStore 的人傳入什麼進去做為參考。\nsource-code\n1 2 3 4 5 6 7 func NewThreadSafeStore(indexers Indexers, indices Indices) ThreadSafeStore { return \u0026amp;threadSafeMap{ items: map[string]interface{}{}, indexers: indexers, indices: indices, } } 可以看到 NewStore 這個 function 有使用到 NewThreadSafeStore 方法，並且傳入 Indexers{} 以及 Indices{} 。\nsource-code\n1 2 3 4 5 6 7 // NewStore returns a Store implemented simply with a map and a lock. func NewStore(keyFunc KeyFunc) Store { return \u0026amp;cache{ cacheStorage: NewThreadSafeStore(Indexers{}, Indices{}), keyFunc: keyFunc, } } 看完了如何建立 ThreadSafeStore 的物件之後，我們接著來分析該物件每個function 實作了什麼事情吧！\nimplement function 單純的把 Object 放入 map，以 Object key 作為 map 的索引，重點在於 updateIndices 的動作。\nAdd source-code\n1 2 3 4 5 6 7 func (c *threadSafeMap) Add(key string, obj interface{}) { c.lock.Lock() //鎖不解釋 defer c.lock.Unlock() //退出解鎖 oldObject := c.items[key] //取出舊的物件 c.items[key] = obj //寫入新的物件 c.updateIndices(oldObject, obj, key) //更新indexed索引 ，稍後會解釋更新 indexed 索引的動作 } Update 單純的更新物件，透過 object key 作為索引找到物件並且更新，重點在於 updateIndices 的動作。\nsource-code\n1 2 3 4 5 6 7 8 //跟Add不是一模一樣...為什麼不重用add不明白 func (c *threadSafeMap) Update(key string, obj interface{}) { c.lock.Lock() //鎖不解釋 defer c.lock.Unlock() //退出解鎖 oldObject := c.items[key] //透過Object key 取出舊的物件 c.items[key] = obj //透過Object key寫入新的物件 c.updateIndices(oldObject, obj, key) //更新indexed索引 } Delete 刪除物件，透過 Object key作為索引找到物件並且刪除，重點在於 deleteFromIndices 的動作。\nsource-code\n1 2 3 4 5 6 7 8 func (c *threadSafeMap) Delete(key string) { c.lock.Lock() //鎖不解釋 defer c.lock.Unlock() //退出解鎖 if obj, exists := c.items[key]; exists { //如果物件存在 c.deleteFromIndices(obj, key) //刪除indexed索引 delete(c.items, key) //刪除物件 } } Get 取得物件，透過 Object key 作為索引取的物件\nsource-code\n1 2 3 4 5 6 7 func (c *threadSafeMap) Get(key string) (item interface{}, exists bool) { c.lock.RLock() //讀操作，用讀鎖 defer c.lock.RUnlock() //解鎖 item, exists = c.items[key] //透過Object key拿取物件 return item, exists //回傳 } List 列出所有物件\nsource-code\n1 2 3 4 5 6 7 8 9 10 11 func (c *threadSafeMap) List() []interface{} { c.lock.RLock() //讀操作，用讀鎖 defer c.lock.RUnlock() //解鎖 //for跑一次map全部加入到slice就好 list := make([]interface{}, 0, len(c.items)) for _, item := range c.items { list = append(list, item) } return list //回傳slice } ListKeys 列出所有物件的 Object key\nsource-code\n1 2 3 4 5 6 7 8 9 10 11 12 // ListKeys returns a list of all the keys of the objects currently // in the threadSafeMap. func (c *threadSafeMap) ListKeys() []string { c.lock.RLock() //讀操作，用讀鎖 defer c.lock.RUnlock() //解鎖 //for跑一次map全部加入到slice就好 list := make([]string, 0, len(c.items)) for key := range c.items { list = append(list, key) } return list //回傳slice } Replace 一次替換整個儲存庫，重點在於會重建整個indices以及更新index。\nsource-code\n1 2 3 4 5 6 7 8 9 10 11 12 func (c *threadSafeMap) Replace(items map[string]interface{}, resourceVersion string) { c.lock.Lock() //鎖不解釋 defer c.lock.Unlock() //退出解鎖 c.items = items //覆蓋整個紀錄 // rebuild any index c.indices = Indices{} //indexed 索引重建 for key, item := range c.items { //更新indexed索引列表 c.updateIndices(nil, item, key) } } 接下來的部分會著重在 indexed 的計算\nIndex 透過 index-name 取得indexer中的 object key 計算 function 算出對應的 indexed value ， 透過對應的 indices-name 像 indices 取的對應的 index 。\n使用對應的 index 以及計算出來的 indexed value取得 object key set ，遞迴跑一是 key set 從 map 中一一取得對應的物件。\nsource-code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 // Index returns a list of items that match the given object on the index function. // Index is thread-safe so long as you treat all items as immutable. //透過index name取得indexed後 func (c *threadSafeMap) Index(indexName string, obj interface{}) ([]interface{}, error) { c.lock.Lock() //鎖不解釋 defer c.lock.Unlock() //退出解鎖 indexFunc := c.indexers[indexName] //透過index name從indexer中取得對應的infexed function if indexFunc == nil { return nil, fmt.Errorf(\u0026#34;Index with name %s does not exist\u0026#34;, indexName) } indexedValues, err := indexFunc(obj) //透過indexed function 計算indexed if err != nil { return nil, err } index := c.indices[indexName] //透過index name 從 indices 拿對應的 index var storeKeySet sets.String //object key的集合 if len(indexedValues) == 1 { //大多數情狂indexed 只會有一個 // In majority of cases, there is exactly one value matching. // Optimize the most common path - deduping is not needed here. storeKeySet = index[indexedValues[0]] //透過indexed從index拿到對應的 object key set } else { //不知道什麼情況會跑進去這裡，我用dlv跑測試沒有跑進這裡過...以下是看註解猜測的 // Need to de-dupe the return list. // Since multiple keys are allowed, this can happen. storeKeySet = sets.String{} for _, indexedValue := range indexedValues { //遞迴index所有的indexed,並且將數值插入set中 for key := range index[indexedValue] { storeKeySet.Insert(key) } } } list := make([]interface{}, 0, storeKeySet.Len()) //Objest set 取出所有的 Object key //將map有對應到的 Object 放入list 回傳 for storeKey := range storeKeySet { list = append(list, c.items[storeKey]) } return list, nil } 看完了Index的實作，我是這樣猜的透過一個物件算出 Indexed-name，在 Indices 中 index name 對應到一組 Index，最後再從這個 Index \u0008中透過 \u0008Indexed name拿到關聯的Object。\ndlv 實際拿到的數值 \u0026hellip; todo\nIndexKeys 透過 index name 從 indexer 中拿到計算 indexe value 的 function ，接著從 indices 使用 index name 拿到到對應的 index 。\n使用算出來的 index value 從 index 拿到所有的 object key set 。\nsource-code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // IndexKeys returns a list of the Store keys of the objects whose indexed values in the given index include the given indexed value. // IndexKeys is thread-safe so long as you treat all items as immutable. func (c *threadSafeMap) IndexKeys(indexName, indexedValue string) ([]string, error) { c.lock.RLock() //讀操作，用讀鎖 defer c.lock.RUnlock() //解鎖 indexFunc := c.indexers[indexName] //透過 index name從indexer中取得對應的infexed function if indexFunc == nil { return nil, fmt.Errorf(\u0026#34;Index with name %s does not exist\u0026#34;, indexName) } index := c.indices[indexName] //透過 index name 從 indices 拿對應的 index set := index[indexedValue] //透過indexed 從 index 拿到所有的 Object key return set.List(), nil } ListIndexFuncValues 透過 index name 從 indices 取得對應的 index，遞迴跑過 index 取得所有的 index key。\nsource-code\n1 2 3 4 5 6 7 8 9 10 11 12 func (c *threadSafeMap) ListIndexFuncValues(indexName string) []string { c.lock.RLock() //讀操作，用讀鎖 defer c.lock.RUnlock() //解鎖 index := c.indices[indexName] //透過 index name 從 indices 拿對應的 index names := make([]string, 0, len(index)) //拿出 index 中所有的 indexed key for key := range index { names = append(names, key) } return names } ByIndex 透過 index name 從 indexer 取得計算 indexed value 的 function ，以及使用 index name 從 indices 中拿到對應的 index 。\n使用對應的 index 以及計算出來的 indexed value 取得對應的 object key set 。\n遞迴object key set 把 object key set 對應到的 object 回傳出來。\nsource-code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // ByIndex returns a list of the items whose indexed values in the given index include the given indexed value func (c *threadSafeMap) ByIndex(indexName, indexedValue string) ([]interface{}, error) { c.lock.RLock() //讀操作，用讀鎖 defer c.lock.RUnlock() //解鎖 indexFunc := c.indexers[indexName] //透過index name從indexer 中取的 indexed function if indexFunc == nil { return nil, fmt.Errorf(\u0026#34;Index with name %s does not exist\u0026#34;, indexName) } index := c.indices[indexName] //透過index name從Indices 中取得 index set := index[indexedValue] //透過indexed name從index取的 Object key set list := make([]interface{}, 0, set.Len()) //遞迴儲存的object key set 對應的object倒出 for key := range set { list = append(list, c.items[key]) } return list, nil } GetIndexers 回傳 indexer\nsource-code\n1 2 3 func (c *threadSafeMap) GetIndexers() Indexers { return c.indexers //沒啥好說的回傳一個indexer } AddIndexers 加入新的 indexer ，前提是不能有資料才可以加入 indexer 。\nsource-code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func (c *threadSafeMap) AddIndexers(newIndexers Indexers) error { c.lock.Lock() //鎖不解釋 defer c.lock.Unlock() //退出解鎖 if len(c.items) \u0026gt; 0 { //現在有存資料了，不能加入indexer return fmt.Errorf(\u0026#34;cannot add indexers to running index\u0026#34;) } oldKeys := sets.StringKeySet(c.indexers) //把舊的indexer轉乘一個可以比對的資料 newKeys := sets.StringKeySet(newIndexers) //把新的indexer轉乘一個可以比對的資料 if oldKeys.HasAny(newKeys.List()...) { //比對兩個indexer 是否有重複 return fmt.Errorf(\u0026#34;indexer conflict: %v\u0026#34;, oldKeys.Intersection(newKeys)) } //上述不引響核心邏輯，不展開解析 //把新的indexer加入indexer中 for k, v := range newIndexers { c.indexers[k] = v } return nil } Resync 沒幹啥事\nsource-code\n1 2 3 4 func (c *threadSafeMap) Resync() error { // Nothing to do return nil //沒幹啥 } 最後幾個 function 加油！\nupdateIndices 更新 indices ，如果有就的物件就需要刪除舊的 index\n遞迴所有 indexer 拿到計算 indexed value 的 function ，透過 indexed function 計算出 indexed value。\n使用 index name 從 indices 拿到 index。\n如果 index 不存在建立新的index，並且建立 index name 與 indices 的連結\n透過 indexed function 計算出 indexed value\n遞迴所有 indexed value ，從 index 中到對應的 Object key set ，若是找不到 Set ，則建立一個 Object ket set 並且讓 indexed value 與 Object key set 建立連結。\nsource-code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 // updateIndices modifies the objects location in the managed indexes, if this is an update, you must provide an oldObj // updateIndices must be called from a function that already has a lock on the cache //傳入舊的 object 為了確認有沒有這個 index （更新用） //傳入新的 object //私有方法只能從add 或是 update呼叫，add/update 有上鎖，保證thread safe func (c *threadSafeMap) updateIndices(oldObj interface{}, newObj interface{}, key string) { // if we got an old object, we need to remove it before we add it again //如果有代舊的object的話需要先刪除舊的index，下一個段落會說到如何刪除 if oldObj != nil { c.deleteFromIndices(oldObj, key) } //將所有的indexer跑過一次，拿到所有的index name以及indexed function for name, indexFunc := range c.indexers { //透過indexed function 算出indexed indexValues, err := indexFunc(newObj) if err != nil { panic(fmt.Errorf(\u0026#34;unable to calculate an index entry for key %q on index %q: %v\u0026#34;, key, name, err)) } //透過 index name 從 indices 拿到對應的 index index := c.indices[name] //確認 index 是否存在，不存在表示這個 indices 還沒有這個 index //建立 index 並且對應到 indecies 上 if index == nil { index = Index{} c.indices[name] = index } for _, indexValue := range indexValues { //透過 indexed 檢查 index 對應的 Object key set set := index[indexValue] //若是set 為空表示 index 還沒建立object key set //建立 set 並且對應到 index 上 if set == nil { set = sets.String{} index[indexValue] = set } //object set set 插入 object key set.Insert(key) } } } deleteFromIndices 當更新物件或是刪除物件的時候會被呼叫為了更新 Indices\n遞迴 indexer 取出所有的 indexed value function ，並且從 indices 確認 index 是否存在\n若是不存在，表示物件已經被清理且 indices 已更新\n若是存在，需要進一步確認 indexed value function 算出來的所有 indexed value 在 index 中 Object key set 的情況\n若是 Object key set 存在就刪除 Object 內對應的 Object key，另外 Obeset set 若是為空 需要回收 Object 減少記憶體使用。\nsource-code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 // deleteFromIndices removes the object from each of the managed indexes // it is intended to be called from a function that already has a lock on the cache func (c *threadSafeMap) deleteFromIndices(obj interface{}, key string) { //將indexer 中全部 indexed function 取出 for name, indexFunc := range c.indexers { //透過indexed function 算出 indexed indexValues, err := indexFunc(obj) if err != nil { panic(fmt.Errorf(\u0026#34;unable to calculate an index entry for key %q on index %q: %v\u0026#34;, key, name, err)) } //透過index name 從indices拿到對應的index index := c.indices[name] //有可能 index name 在indeces中找不到（不太可能...） if index == nil { continue } //將indexed 全部跑一次 for _, indexValue := range indexValues { //從index拿到對應的object key set set := index[indexValue] //如果set 存在的話就刪除對應的 Object key if set != nil { set.Delete(key) //刪除 object key 為零的 set ，為了避免佔用空間（kubernetes 上有相關的issuekubernetes/kubernetes/issues/84959） if len(set) == 0 { delete(index, indexValue) } } } } } 小結 我想先介紹到這邊把 threadSafeMap 的部分介紹完，讓大家先了解 cache 底層是怎麼做的，以及 indexer 、 indexed-name 、 Indices 、 index 以及 Object set 之間的關聯。\n因為 cahce 是基於 threadSafeMap 往上蓋的，所以先建立最底層的概念是非常重要，下一篇我將會繼續介紹 cache 的部分。\n","description":"","id":27,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes Indexers local cache 之美 （I）","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/indexer/kubernetes-indexers-local-cache-1/"},{"content":" 今年挺廢的許多目標都沒有完成\n平時覺得自己總是忙忙碌碌的每天都有好多安排在清單裡面要完成，到了年末回顧的時候回首這些清單覺得沒有完成什麼重要的事情，做了很多雜事呵，期許明年可以更好完成更多目標。\n就來年度回顧審視過去一年來好的壞的\n回顧 2020 是一個不平凡的一年，從去年就開始的國軍 online ， Kobe Bryant 要上帝幫他搶籃板， Chadwick Boseman 去到瓦甘達 ，全球疫情大肆虐各地都充滿了災情。希望2021 愛我的與我愛的以及世界各地都能得到一些小確幸。\nR.O.C airforce 登出\n國軍嘛\u0026hellip;.，說實話我在那一段時間身體變得滿健康的，還記得研究所快要畢業前的那段時間寫論文、做實驗、調參數、優化架構。作息極度不正常，在裡面短短四個月肝應該軟了不少吧～哈！ 另外認識了許多小夥伴，雖然每個人做的領域都不同 e.g. 會計、設計、養微生物，自己創業的 \u0026hellip;，希望未來有時間我們還能再聚聚 blog 經營\nblog 大概寫了三十三篇文章，雖然大多數都在記錄一些在開發(Dev)與維運(Ops)的心得，以及一些 kubernetes source tracing 扔然覺得寫得不到位，詞不達意需要多磨練。 此外有跟社群的小夥伴討論要不要同步到 medium 大家一起寫比較不孤單xD ，每天有太多太多各式各樣的開發、維運、底層與演算法等資訊出現在我們身邊，有人可以一起學習比較不會放棄。 很推薦 hwchiu 的每日技術文章(矽谷牛的耕田筆記) 每天陪我度過吃早餐的時間 參與社群\n2020年初到年終這段時間，家裡、疫情以及當兵三大因素參加社群的時間大幅減低，最近研究的東西變少了也沒辦法拿到社群說嘴哈哈 主要參與了三個社群 SRE TW 的 round 4 讀書會 CNTUG 線上線下 meetup Golang TW 線上線下 meeup 運動\n在接近年尾的時間學長 Kyle 帶我去萬華-原岩攀岩館，體驗了人生第一次的攀岩，第一次體驗就過了 LV2 覺得這項運動真的還不錯挑戰自我與小臂肌肉爆炸的感覺非常棒！ 開始拉同事到南港附近的攀岩館運動，真是個不錯的社交運動呢(其實我不擅長社交ＱＱ) 找不到合適的健身房運動（家裡跟公司與健身房找不到一個平衡點），在家裡徒手健身，2019 研究所畢業到 2020 的現在肌肉掉了不少 ＱＱ 煮飯\n開發了兩道菜，普普通通沒特別好吃 清燉牛肉佐九層塔 炸馬鈴薯燉南洋咖哩辛辣苦味 線上課程\n上了一門多益線上課程，一個禮拜大概花了五到六個小時上課（新的課程加複習） 主要是看 Cindy老師的教學影片，內容大多數是單字以及考試用的詞彙，心得是我單字量有變多但是比較針對考試非日常的口說與寫作。 上課之前我有做過模擬測驗大概五百八十分左右，上完課再做其他的模擬試題大概進步到六百七十，多多少少有幫助xDD 通勤時間勁量保持英聽的習慣，cindy 老師的新聞簡報 的或是 好想講英文 都是滿不錯的題材 說說我覺得比較雖但是印象深刻的事情吧\n新換的手機不到四個月，螢幕就被我摔爆（前後玻璃都破了），慘 台北一個多月的大雨，放讓我的小雷霆在外面風吹日曬雨淋 電池沒電 汽油幫浦壞了 皮帶龜裂 停在外面還被撞了，後土除莫名其妙歪一邊zZZ 我家的小瓜球（貓咪）吃了一堆的橡皮筋跟塑膠帶，可能是小名取的太好吧（小名叫傻眼） 喉嚨痛了一個月，我都懷疑自己是不是中了COVID-19，每天吃飯都要仔細品嚐\u0026hellip;.味覺xDD 展望 2021 年希望疫情好轉，自己的也能一起成長。\n多運動，朔造更好的體態（期待一週能有 3 ~ 4 次，每次 1.5 hr 的時間） 產出有深度與廣度的技術文章，透過閱讀 source code 與 blog 等方式更加貼近 cloud native 與 linux kernel 的世界。 保持閱讀的習慣，不論是雜書或是論文，訓練自己的耐心。 持續加強英文聽說能力（期待一週能在聽力花上 6 ~ 8 hr 靜下心好好聽懂新聞、 ted 演講、影集的日常對話，口說花上 5 ~ 6 hr 找到正確且適合的教材跟讀與訓練發音） 沒事就煮煮飯給貓吃，讓貓貓吃好一點不要再吃塑膠袋根橡皮筋了zZ 小結 生活總有波折，2020 雖然許多被奪走寶貴的生命也發生了經濟危機，但雨過了總會天晴。\n但願 2021 年不只是個人的能力增強，也希望週遭的一切都能一掃過去的不快迎接美好的未來，創造更多快樂的記憶。\n","description":"","id":28,"section":"posts","tags":["life"],"title":"2020回顧總結","uri":"https://blog.jjmengze.website/zh-tw/posts/record/2020/"},{"content":" 首先本文所有的 source code 基於 kubernetes 1.19 版本，所有 source code 為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\n換換口味今天我們來分析一下 kubernetes 如何處理錯誤， golang 的錯誤處理總是令人詬病，看看開源的大專案面對這種語言本身的缺陷（算嗎xD?）是怎麼處理的，從中學習到一些妙招～\n本篇只涵蓋到 kubernetes 錯誤處理的一部分，還有許多錯誤處理的方法等待我們去挖掘～未來會持續解析相關議題！\nerrors kubernetes 定義了一個 interface ，來描述錯誤訊息要如何被封裝，先來看看這個 interface 是如何定義的。\ninterface source code\n1 2 3 4 5 6 // 聚合 error type Aggregate interface { error //繼承了golang builtin package 的 error interface Errors() []error //列出聚合所有的 error Is(error) bool //檢查聚合的內容有沒有包含指定的 error } 看完了 interface 後可以來看看哪個 struct 實作了這個 interface\nstruct 聚合的意思就是把一群東西放在一起，error 的聚合就是把 error 放在一起很自然地就會想到 slice ~\n沒錯！kubernetes也是這麼做的，透過 error slice 把 error 聚合再一起。\nsource code\n1 type aggregate []error 看完了資料結構後我們來看看要怎麼把這個物件建立起來\nNew function 傳入一組 error slice 並且轉換成 Aggregate 物件回傳出去\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func NewAggregate(errlist []error) Aggregate { // error 是空的表示沒有錯誤要處理 if len(errlist) == 0 { return nil } // In case of input error list contains nil var errs []error // 把 error list 的東西全部倒出來，檢查有沒有偷藏 nil // 沒有 nil 的加入到 errs 的 slice 中 for _, e := range errlist { if e != nil { errs = append(errs, e) } } //檢查轉換完的 errs 是不是空的 if len(errs) == 0 { return nil } //轉換物件型態～轉成有實作 Aggregate interface 的物件 return aggregate(errs) } impliment 看完了 aggregate 的資料結構以及如何建立一個實作 Aggregate interface 的物件後，我們來接著看實作得部分。\nError Error 的部分是實作 golang builtin package 的 error\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 // Error is part of the error interface. func (agg aggregate) Error() string { //確認一下 error slice 長度，在 new function 的時候就確認過理論上不會出錯 if len(agg) == 0 { // This should never happen, really. return \u0026#34;\u0026#34; } //如果錯誤只有一個的話就回傳那一個 if len(agg) == 1 { return agg[0].Error() } // new 一個errs set seenerrs := sets.NewString() result := \u0026#34;\u0026#34; //需要搭配下面一點的 visit function 來看～ //visit 代表訪問每一個 item agg.visit(func(err error) bool { // 印出錯誤 msg := err.Error() // 如果errs set 有包含這個錯誤的話就回傳 false（表示錯誤重複了） if seenerrs.Has(msg) { return false } //把錯誤加入 error set seenerrs.Insert(msg) //如果錯誤大於一個需要用,分割錯誤 if len(seenerrs) \u0026gt; 1 { result += \u0026#34;, \u0026#34; } result += msg return false }) //只有一個錯誤，直接回傳錯誤不需要加[] if len(seenerrs) == 1 { return result } //錯誤是一個陣列，需要加[] return \u0026#34;[\u0026#34; + result + \u0026#34;]\u0026#34; } //訪問每一個錯誤透過傳入的 function 來處理每個錯誤 func (agg aggregate) visit(f func(err error) bool) bool { // 遞迴全部的錯誤 for _, err := range agg { //判斷錯誤類型 switch err := err.(type) { //如錯錯誤是一個 aggregate 的物件的話 case aggregate: //表示這個錯誤裡面包了其他錯誤，需要再展開遞迴的處理 if match := err.visit(f); match { return match } //如錯錯誤是一個實作 aggregate interface 的物件的話 case Aggregate: //等等會看到 Errors ，簡單來說就是把錯誤展開成一個slice //檢查每一個錯誤 for _, nestedErr := range err.Errors() { if match := f(nestedErr); match { return match } } //如果是一般錯誤的話 default: //檢查個錯誤 if match := f(err); match { return match } } } return false } Is 透過golang errors packages 的 is 來輔助判斷 aggregate slice 內有無相同的error\nsource code\n1 2 3 4 5 6 7 8 func (agg aggregate) Is(target error) bool { //需要搭配剛剛提到的visit function 來看～ //visit 代表訪問每一個 item return agg.visit(func(err error) bool { //透過errors packages 來輔助判斷 aggregate slice 內有無相同的error return errors.Is(err, target) }) } Errors 顯示所有error，簡單來說就是把 aggregate 轉換成 []error\nsource code\n1 2 3 4 // Errors is part of the Aggregate interface. func (agg aggregate) Errors() []error { return []error(agg) } 以上透過 aggregate 簡單的封裝了 error，讓 golang 的錯誤處理有較好的處理方式。\n我認為 kubernetes 處理 error 的靖華在於下面這幾個 function\nFlatten 比如有一個巢狀的 aggregate 可以用 Flatten 把 aggregate 攤平。\n1 2 3 4 5 6 7 8 aggregate{ aggregate{ fmt.Errorf(\u0026#34;abc\u0026#34;), aggregate{ fmt.Errorf(\u0026#34;def\u0026#34;) } } } 我們想把上面這個結構攤平，就可以透過 Flatten 來幫忙輸出的結果會變成\n1 2 3 aggregate{ fmt.Errorf(\u0026#34;abc\u0026#34;), fmt.Errorf(\u0026#34;def\u0026#34;) } 我們來看看他怎麼實作的\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // Flatten takes an Aggregate, which may hold other Aggregates in arbitrary // nesting, and flattens them all into a single Aggregate, recursively. func Flatten(agg Aggregate) Aggregate { //建立一個 error slice result := []error{} //判斷 Aggregate 存不存在，不存在可以直接回傳 if agg == nil { return nil } //把 Aggregate 展開成 errors slice 並且遞迴所有error for _, err := range agg.Errors() { //如果判斷到 error 是 Aggregate 就要繼續遞迴展開 if a, ok := err.(Aggregate); ok { //遞迴展開的error結果加入到result內 r := Flatten(a) if r != nil { result = append(result, r.Errors()...) } } else { //error結果加入到result內 if err != nil { result = append(result, err) } } } //回傳一個 Aggregate error slice return NewAggregate(result) } CreateAggregateFromMessageCountMap 有些情況我們會紀錄錯誤訊息發生的次數，我們可以透過 CreateAggregateFromMessageCountMap 幫助我們把這些訊息轉成 Aggregate error slice 。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // MessageCountMap contains occurrence for each error message. // 計數錯誤訊息出現次數 type MessageCountMap map[string]int // CreateAggregateFromMessageCountMap converts MessageCountMap Aggregate // 輸入計數訊息錯誤次數的 map 轉換成 Aggregate 輸出 func CreateAggregateFromMessageCountMap(m MessageCountMap) Aggregate { //簡單的判斷一下輸入，沒有輸入就不做事 if m == nil { return nil } //建立一個error slice 長度為 MessageCountMap 的大小 result := make([]error, 0, len(m)) //遞迴 MessageCountMap for errStr, count := range m { var countStr string //錯誤出現超過一次的才放入 error slice 中，並且在錯誤訊息中標示 錯誤訊息與錯誤次數 if count \u0026gt; 1 { countStr = fmt.Sprintf(\u0026#34; (repeated %v times)\u0026#34;, count) } result = append(result, fmt.Errorf(\u0026#34;%v%v\u0026#34;, errStr, countStr)) } //回傳一個 Aggregate error slice return NewAggregate(result) } FilterOut 把錯誤進行過濾～有通過 Matcher function 才能輸出\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 type Matcher func(error) bool func FilterOut(err error, fns ...Matcher) error { //沒有輸入就不需要做事直接退出 if err == nil { return nil } //如果 error 有實作 Aggregate if agg, ok := err.(Aggregate); ok { //把 error 展開成 error slice //再把 Matcher 與 error slice 交給 filterErrors 處理（下面有解釋～） return NewAggregate(filterErrors(agg.Errors(), fns...)) } // 判斷error 有沒有通過 matcher function if !matchesError(err, fns...) { return err } return nil } // matchesError returns true if any Matcher returns true //透過 matcher function 檢查輸入的error func matchesError(err error, fns ...Matcher) bool { //遞迴全部的 Matcher function for _, fn := range fns { //檢查所有error if fn(err) { return true } } return false } //檢查errors slice func filterErrors(list []error, fns ...Matcher) []error { //建立result error slice result := []error{} //遞迴輸入的 error slice for _, err := range list { //這裡會回到 FilterOut 確認 err 是不是有實作 Aggregate //如果有實作 Aggregate 還需要繼續的迴展開 r := FilterOut(err, fns...) //遞迴展開後結果不是空數就可以合併到 result error slice if r != nil { result = append(result, r) } } return result } AggregateGoroutines 假設有很多 function 會併發執行且我們需要收集她的 error 做聚合組裡的時候這個 AggregateGoroutines function 就很好用～\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // AggregateGoroutines runs the provided functions in parallel, stuffing all // non-nil errors into the returned Aggregate. // Returns nil if all the functions complete successfully. func AggregateGoroutines(funcs ...func() error) Aggregate { //建立有 n 個 buffer 的 channel errChan := make(chan error, len(funcs)) //遞迴執行function for _, f := range funcs { //透過 goroutine 在背景執行，把執行的結果丟在 buffer channel go func(f func() error) { errChan \u0026lt;- f() }(f) } // 建立 error slice errs := make([]error, 0) //接收全部的 channel 結果 for i := 0; i \u0026lt; cap(errChan); i++ { //如果 error 不是 nil 就加入到 error slice 內 if err := \u0026lt;-errChan; err != nil { errs = append(errs, err) } } // 將error 轉換成 Aggregate return NewAggregate(errs) } Reduce 我不確定 Reduce function 的用途\u0026hellip;.看不是很懂\u0026hellip;\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Reduce will return err or, if err is an Aggregate and only has one item, the first item in the aggregate. //輸入一個error func Reduce(err error) error { //如過error 有實作 Aggregate 的話 ，要接著判斷 errors slice 的長度 if agg, ok := err.(Aggregate); ok \u0026amp;\u0026amp; err != nil { switch len(agg.Errors()) { //如果長度是1回傳第一個錯誤 case 1: return agg.Errors()[0] //不然就是nil case 0: return nil } } return err } 小結 kubernetes 設計了很多很棒的架構，我們可以以 kubernetes 為借鏡設計模式出適合公司的框架，從中還可以學到不少神奇有趣的方法。\n感恩開源讚嘆開源，讓我學習到更多！文中如果有見解錯誤的或是寫的不正確的還希望觀看此文的大大們可以不吝嗇地提出讓我修正與學習！感謝～\n","description":"","id":29,"section":"posts","tags":["kubernetes","source-code"],"title":"Kubernetes error handle 好吃驚","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/errors/kubernetes-errors-handle/"},{"content":" 前幾章節，我針對 Kubernetes work queue 進行了大部分解，回頭看看這幾篇文之後我發現一個問題，讀者可能沒辦法體會為什麼我們要了解 kubernetes work queue 的原理與機制以及要 work queue 怎麼使用，不是按照 example controller 抄一抄改一改就好了嗎？或是採用某一種框架與架構例如非常有名的 operator framework 或是 metacontroller 還有很多不同的框架族繁不及備載。\n本篇文章將會提出為什麼需要了解 kubernetes work queue，以及其他在撰寫 controller 時候需要了解的元件。\n如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\ncontroller example 這是透過 client go 撰寫 kubernetes controller 官方所提供的範例，從中我們可以了解搭建一個最基本的 controller 需要什麼元件。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Controller demonstrates how to implement a controller with client-go. type Controller struct { indexer cache.Indexer //幫忙儲放資料的元件 queue workqueue.RateLimitingInterface //幫忙資料排隊的元件 informer cache.Controller //幫忙向api server 拿資料的元件 } //上述三個組合於 Controller 資料結構都是 interface ， 只要傳入有實作的物件就好 // NewController creates a new Controller. //傳入實作 Rate Limiting queue interface 的物件 //傳入實作 interface 的物件 //傳入實作 Controller interface 的物件 //那麼就得到了一個堪用的 kubernetes controller 了 func NewController(queue workqueue.RateLimitingInterface, indexer cache.Indexer, informer cache.Controller) *Controller { return \u0026amp;Controller{ informer: informer, indexer: indexer, queue: queue, } } 架構圖 看到官方提供的範例，再回頭看前些日子我針對 kubernetes controller 其中的 queue 的分析，我們可以對應著架構圖來看，會方便我們理解！\n大致可以拆成三塊來看\nwork queue\nindexer\ninformer\n上述三個拆分的方式是我個人的理解，如果有誤希望有大大可以指點QQ\ninformer\nReflector\n從 api server 拉到 kubernetes 的資料（Pod, Deployment e.t.c）進行反序列化丟給Delta queue處理 Handle Deltas\n拿到 Queue 吐出的資料（Object Key）進行處理，分派給其他 work 等 Get By Key\nWorker 透過 Obejct Key 像 Indexer 取得資料（Pod, Deployment e.t.c） work queue\nReflector為生產者 產出object key 放入work queue按照特定規則排序（如delaying等） Controller為消費者 從work queue 中拿出 object key Indexer\nkey / value形態的local cache 存放object key跟反序列化後的obejct value（Pod, Deployment e.t.c） 小結 從架構圖上我們可以看到有幾好幾個部分需要了解例如 Work queue 、 Indexer 、 Informer 。\n先前的章節我們了解了 Work queue 的各種變化，例如最基礎的 Common work queue 、 Delaying work queue 、 Ratelimite work queue 等。\n後續會逐步介紹 Indexer 其中的奧妙與設計理念， Kubernetes 如何在本地端進行 cache 以及提供其他元件透過 Object key 進行查詢。\n個人認為複雜與難以理解的 informer 放到最後一個章節，這邊牽扯的東西非常多也很雜論需要前面幾張的鋪成，可能會比較好理解吧 xD\n最後檢視官方的的 deployment controller 是如何撰寫得，把整篇文章的思路與脈絡打通。\n文章中若有錯誤或是有疑問的地方歡迎提出，讓我們互相學習成長，謝謝！\n","description":"","id":30,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes Controller 設計挖掘之路","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/dig-kubernetes-controller/"},{"content":" 首先本文所以 source code 基於 kubernetes 1.19 版本，所有 source code 的為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\nkubernetes work queue Kubernetes controller/operator 是一個非常精彩的設計模式，在了解Kubernetes controller/operator 怎麼撰寫之前，了解kubernetes work queue的實作模式是非常重要的，下面引用了How to Create a Kubernetes Custom Controller Using client-go的 controller 架構圖可以看到在 sharedindexinformer 內有引用到這個元件，這個元件實際被定義在 kubernetes 的 client-go library 中。\n圖片來源：How to Create a Kubernetes Custom Controller Using client-go\nKubernetes 為什麼要實踐一個 work queue 呢？就我們所知 kubernetes 是用 go 撰寫應該可以使用 channel 的機制直接將物件送給要用的元件(thread)啊，原因其實非常簡單，go channel 的設計功能非常單一無法滿足 kubernetes 所要的場景，例如帶有延遲時間物件需要根據延遲時間排序的queue，例如限制物件取出速度的queue。\n之先前的章節有提到 common work queue 、 delaying work queue 、 ratelimiting queue，但 ratelimiting queue 有組合 ratelimiter ，本章節將有實作 ratelimiter 同時也滿常被用到的 BucketRateLimiter 展開解說。\nBucketRateLimiter 先來看看 BucketRateLimiter 的 UML 圖，很清楚可以看得出來他實作的 RateLimiter interface 已經嵌入了一個 golang rate package 的 Limiter(也就是固定速度qps限速器，有興趣的朋友可以自行深入閱讀golang的實作方式)\ninterface kubernetes source code 設計得非常精美，我們可以先從 interface 定義了哪些方法來推敲實作這個 interface 的物件可能有什麼功能。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type RateLimiter interface { // When gets an item and gets to decide how long that item should wait //當一個物件放入的時候，需要回傳延遲多久（可自定義規則，等等會看到） When(item interface{}) time.Duration // Forget indicates that an item is finished being retried. Doesn\u0026#39;t matter whether its for perm failing\t// or for success, we\u0026#39;ll stop tracking it //當一個物件完成的時候可以，要忘記曾經延遲過（重新計算） Forget(item interface{}) // NumRequeues returns back how many failures the item has had // 回傳物件已經放入幾次（重試了幾次，白話一點呼叫NumRequeues幾次） NumRequeues(item interface{}) int } 看完了抽象的定義之後，必須要回過來看 Bucket Rate Limiter queue 實際物件定義了哪些屬性\nstruct source code\n1 2 3 4 5 // BucketRateLimiter adapts a standard bucket to the workqueue ratelimiter API type BucketRateLimiter struct { *rate.Limiter //BucketRateLimiter嵌入了golang.org.x.time.rate.Limiter //也就是固定速度qps限速器，有興趣的朋友可以自行深入閱讀golang的實作方式 } 看完了資料結構我們接著來看 BucketRateLimiter 實作的方法，與初始化方法。\nnew function source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //不知道為什麼要設定一個空的變數 var _ RateLimiter = \u0026amp;BucketRateLimiter{} // DefaultControllerRateLimiter is a no-arg constructor for a default rate limiter for a workqueue. It has // both overall and per-item rate limiting. The overall is a token bucket and the per-item is exponential func DefaultControllerRateLimiter() RateLimiter { //這裡使用到上一章節提到得MaxOfRateLimiter，這裡以兩個RateLimiter為主 //一個是ItemExponentialFailureRateLimiter，例外一個是本篇的主角BucketRateLimiter return BucketRateLimiter( //前幾篇有轉們講解有興趣的朋友歡迎回到前幾章節複習 NewItemExponentialFailureRateLimiter(5*time.Millisecond, 1000*time.Second), // 10 qps, 100 bucket size. This is only for retry speed and its only the overall factor (not per item) //這裏設定了golang golang.org.x.time.rate.Limiter 的吞吐速度 // 設定了 10 個 qps 以及 100 個 bucket \u0026amp;BucketRateLimiter{Limiter: rate.NewLimiter(rate.Limit(10), 100)}, ) } implement function 看完了初始化 BucketRateLimiter 後接下來看看核心的功能。\nWhen source code\n1 2 3 func (r *BucketRateLimiter) When(item interface{}) time.Duration { return r.Limiter.Reserve().Delay() // golang.org.x.time.rate.Limiter實作的這個延遲會是個固定的周期（依照qps以及bucket而定） } NumRequeues 當我們需要知道物件已經重試了幾次可以透過NumRequeues function 得知物件重是的次數。\nsource code\n1 2 3 func (r *BucketRateLimiter) NumRequeues(item interface{}) int { return 0 //因為是固定的頻率，所以我們不需要管物件重次了幾次 } Forget 當物件做完時需要重新計算放延遲時間與放入次數，需要透過Forget function完成。\nsource code\n1 2 3 func (r *BucketRateLimiter) Forget(item interface{}) { //因為是固定速率所以我們也不需要去管要不要重新處理物件 } 怎麼使用 對於 BucketRateLimiter 物件而言，他只是實作了 RateLimiter interface，使用者要怎麼用這個 Rate Limiter queue 呢？\n上一篇有提到 RateLimiter 的初始化方法\n1 2 3 4 5 6 func NewRateLimitingQueue(rateLimiter RateLimiter) RateLimitingInterface { return \u0026amp;rateLimitingType{ DelayingInterface: NewDelayingQueue(), //前一小節有提到過delating work queue的newfunction rateLimiter: rateLimiter, //自行實作的rateLimiter } } 使用者可以在傳入參數帶入實作 RateLimiter interface的 ItemExponentialFailureRateLimiter 物件\n1 NewRateLimitingQueue(DefaultControllerRateLimiter()) 表示使用者要求的 Rate Limiter queue 用了 DelayingQueue 與 BucketRateLimiter、ItemExponentialFailureRateLimiter。\n物件延遲時間由 ItemExponentialFailureRateLimiter 與 BucketRateLimiter 決定 物件延遲的排序方式由 DelayingQueue 決定（之前有提過用heap加上clock來觸發） 存放物件的 queue 由 common queue 決定（之前有提過用 processing set 加上 dirty set 合力完成） 大致上流程是這樣，不清楚的地方可以回去複習之前提到過的元件\n小結 終於把 kubernetes work queue 的部分梳理完，小小一個 work queue 有如此多實作細節與方式\n從 common work queue 如何保證下一次 add 進來的物件就有被取走還沒做完，以及還沒被取走的事情要考慮。\n以及 delaying work queue 如何排序一個 延遲物件 ，讓延遲時間最短的物件排在最前面\n另外 RateLimiter work queue 展現了設計模式代理了 delaying work queue 以及組合了 rateLimiter ，讓邏輯分離 rateLimiter 產出物件需要延遲多久，交給 delaying work queue 進行排序。\nkubernetes底層設計的非常精美，透過閱讀程式碼的方式提升自己對kubernetes的了解，若文章有錯的部分希望大大們指出，謝謝！\n","description":"","id":31,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes Bucket Rate Limiter 設計真d不錯","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/queue/bucket-ratelimiter/"},{"content":" 首先本文以 source code 基於 kubernetes 1.19 版本，所有 source code 的為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\nkubernetes work queue Kubernetes controller/operator 是一個非常精彩的設計模式，在了解Kubernetes controller/operator 怎麼撰寫之前，了解kubernetes work queue的實作模式是非常重要的，下面引用了How to Create a Kubernetes Custom Controller Using client-go的 controller 架構圖可以看到在 sharedindexinformer 內有引用到這個元件，這個元件實際被定義在 kubernetes 的 client-go library 中。\n圖片來源：How to Create a Kubernetes Custom Controller Using client-go\nKubernetes 為什麼要實踐一個 work queue 呢？就我們所知 kubernetes 是用 go 撰寫應該可以使用 channel 的機制直接將物件送給要用的元件(thread)啊，原因其實非常簡單，go channel 的設計功能非常單一無法滿足 kubernetes 所要的場景，例如帶有延遲時間物件需要根據延遲時間排序的queue，例如限制物件取出速度的queue。\n之先前的章節有提到 common work queue 、 delaying work queue 、 ratelimiting queue，但 ratelimiting queue 有組合 ratelimiter ，本章節將有實作 ratelimiter 同時也滿常被用到的 MaxOfRateLimiter 展開解說。\nMaxOfRateLimiter interface kubernetes source code 設計得非常精美，我們可以先從 interface 定義了哪些方法來推敲實作這個 interface 的物件可能有什麼功能。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type RateLimiter interface { // When gets an item and gets to decide how long that item should wait //當一個物件放入的時候，需要回傳延遲多久（可自定義規則，等等會看到） When(item interface{}) time.Duration // Forget indicates that an item is finished being retried. Doesn\u0026#39;t matter whether its for perm failing\t// or for success, we\u0026#39;ll stop tracking it //當一個物件完成的時候可以，要忘記曾經延遲過（重新計算） Forget(item interface{}) // NumRequeues returns back how many failures the item has had // 回傳物件已經放入幾次（重試了幾次，白話一點呼叫NumRequeues幾次） NumRequeues(item interface{}) int } 看完了抽象的定義之後，必須要回過來看 Max Of RateLimiter queue 實際物件定義了哪些屬性\nstruct source code\n1 2 3 4 5 6 // MaxOfRateLimiter calls every RateLimiter and returns the worst case response // When used with a token bucket limiter, the burst could be apparently exceeded in cases where particular items // were separately delayed a longer time. type MaxOfRateLimiter struct { limiters []RateLimiter //定義一個實作集合RateLimiter比如前幾個章節有提到的 ItemExponentialFailureRateLimiter、ItemFastSlowRateLimiter都可以放到這個集合內 } 看完了資料結構我們接著來看 MaxOfRateLimiter 實作的方法，與初始化方法。\nnew function source code\n1 2 3 4 //傳入實作集合RateLimiter比如前幾個章節有提到的 ItemExponentialFailureRateLimiter、ItemFastSlowRateLimiter都可以放到這個集合內 func NewMaxOfRateLimiter(limiters ...RateLimiter) RateLimiter { return \u0026amp;MaxOfRateLimiter{limiters: limiters} } implement function 看完了初始化 MaxOfRateLimiter 後接下來看看核心的功能。\nWhen source code\n1 2 3 4 5 6 7 8 9 10 11 func (r *MaxOfRateLimiter) When(item interface{}) time.Duration { ret := time.Duration(0) for _, limiter := range r.limiters { curr := limiter.When(item) //把集合內所有的when都跑跑看，看看延遲時間哪個比較大，用大的延遲時間為主。 if curr \u0026gt; ret { ret = curr } } return ret } NumRequeues 當我們需要知道物件已經重試了幾次可以透過NumRequeues function 得知物件重是的次數。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 func (r *MaxOfRateLimiter) NumRequeues(item interface{}) int { ret := 0 for _, limiter := range r.limiters { curr := limiter.NumRequeues(item) //把集合內所有的NumRequeues都跑跑看，看看哪個用了最多次，以最多次那個為主。 if curr \u0026gt; ret { ret = curr } } return ret } Forget 當物件做完時需要重新計算放延遲時間與放入次數，需要透過Forget function完成。\nsource code\n1 2 3 4 5 func (r *MaxOfRateLimiter) Forget(item interface{}) { for _, limiter := range r.limiters { //做完了，讓集合內所有的人重新處理這個物件。 limiter.Forget(item) } } 怎麼使用 對於 MaxOfRateLimiter 物件而言，他只是實作了 RateLimiter interface，使用者要怎麼用這個 Rate Limiter queue 呢？\n上一篇有提到 RateLimiter 的初始化方法\n1 2 3 4 5 6 func NewRateLimitingQueue(rateLimiter RateLimiter) RateLimitingInterface { return \u0026amp;rateLimitingType{ DelayingInterface: NewDelayingQueue(), //前一小節有提到過delating work queue的newfunction rateLimiter: rateLimiter, //自行實作的rateLimiter } } 使用者可以在傳入參數帶入實作 RateLimiter interface的 ItemExponentialFailureRateLimiter 物件\n1 2 3 4 NewRateLimitingQueue( NNewItemFastSlowRateLimiter(5*time.Millisecond, 3*time.Second, 3), NewItemExponentialFailureRateLimiter(1*time.Millisecond, 1*time.Second) ) 表示使用者要求的 Rate Limiter queue 用了 DelayingQueue 與 ItemExponentialFailureRateLimiter以及ItemFastSlowRateLimiter。\n物件延遲時間由ItemExponentialFailureRateLimiter與ItemFastSlowRateLimiter決定 物件延遲的排序方式由 DelayingQueue 決定（之前有提過用heap加上clock來觸發） 存放物件的 queue 由 common queue 決定（之前有提過用 processing set 加上 dirty set 合力完成） 大致上流程是這樣，不清楚的地方可以回去複習之前提到過的元件\n小結 下一章節將介紹另外一個實作 rateLimiter interface的物件BucketRateLimiter，文章有錯的部分希望大大們指出，謝謝！\n","description":"","id":32,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes Max Of Rate Limiter 設計真d不錯","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/queue/item-max-of-ratelimiter/"},{"content":" 先本文所以 source code 基於 kubernetes 1.19 版本，所有 source code 的為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\nkubernetes work queue Kubernetes controller/operator 是一個非常精彩的設計模式，在了解Kubernetes controller/operator 怎麼撰寫之前，了解kubernetes work queue的實作模式是非常重要的，下面引用了How to Create a Kubernetes Custom Controller Using client-go的 controller 架構圖可以看到在 sharedindexinformer 內有引用到這個元件，這個元件實際被定義在 kubernetes 的 client-go library 中。\n圖片來源：How to Create a Kubernetes Custom Controller Using client-go\nKubernetes 為什麼要實踐一個 work queue 呢？就我們所知 kubernetes 是用 go 撰寫應該可以使用 channel 的機制直接將物件送給要用的元件(thread)啊，原因其實非常簡單，go channel 的設計功能非常單一無法滿足 kubernetes 所要的場景，例如帶有延遲時間物件需要根據延遲時間排序的queue，例如限制物件取出速度的queue。\n之先前的章節有提到 common work queue 、 delaying work queue 、 ratelimiting queue，但 ratelimiting queue 有組合 ratelimiter ，本章節將有實作 ratelimiter 的 ItemFastSlowRateLimiter 展開解說。\nItemFastSlowRateLimiter interface kubernetes source code 設計得非常精美，我們可以先從 interface 定義了哪些方法來推敲實作這個 interface 的物件可能有什麼功能。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type RateLimiter interface { // When gets an item and gets to decide how long that item should wait //當一個物件放入的時候，需要回傳延遲多久（可自定義規則，等等會看到） When(item interface{}) time.Duration // Forget indicates that an item is finished being retried. Doesn\u0026#39;t matter whether its for perm failing\t// or for success, we\u0026#39;ll stop tracking it //當一個物件完成的時候可以，要忘記曾經延遲過（重新計算） Forget(item interface{}) // NumRequeues returns back how many failures the item has had // 回傳物件已經放入幾次（重試了幾次，白話一點呼叫NumRequeues幾次） NumRequeues(item interface{}) int } 看完了抽象的定義之後，必須要回過來看 Item Fast Slow Rate Limiter queue 實際物件定義了哪些屬性\nstruct source code\n1 2 3 4 5 6 7 8 9 // ItemFastSlowRateLimiter does a quick retry for a certain number of attempts, then a slow retry after that type ItemFastSlowRateLimiter struct { failuresLock sync.Mutex //鎖，防止資源競爭 failures map[interface{}]int //計算某個物件呼叫了延遲的次數 maxFastAttempts int //最大短嘗試次數 fastDelay time.Duration //短延遲時間 slowDelay time.Duration //長延遲時間 } 看完了資料結構我們接著來看 Item Fast Slow Rate Limiter 實作的方法，與初始化方法。\nnew function source code\n1 2 3 4 5 6 7 8 9 10 11 12 //不知道為什麼要設定一個空的變數 var _ RateLimiter = \u0026amp;ItemFastSlowRateLimiter{} //傳入基礎要最長等待時間，最短等待時間，最大短嘗試次數，以及初始化failures map資料集 func NewItemFastSlowRateLimiter(fastDelay, slowDelay time.Duration, maxFastAttempts int) RateLimiter { return \u0026amp;ItemFastSlowRateLimiter{ failures: map[interface{}]int{}, fastDelay: fastDelay, slowDelay: slowDelay, maxFastAttempts: maxFastAttempts, } } implement function 看完了初始化 Item Fast Slow Rate Limiter 後接下來看看核心的功能。\nWhen source code\n1 2 3 4 5 6 7 8 9 10 11 12 func (r *ItemFastSlowRateLimiter) When(item interface{}) time.Duration { r.failuresLock.Lock() //鎖不做解釋 defer r.failuresLock.Unlock() //解鎖不做解釋 r.failures[item] = r.failures[item] + 1 //放入次數＋1 if r.failures[item] \u0026lt;= r.maxFastAttempts { //如果放入次數小於短嘗試次數最大值就回傳短延遲時間 return r.fastDelay } return r.slowDelay //回傳常延遲時間 } NumRequeues 當我們需要知道物件已經重試了幾次可以透過 NumRequeues function 得知物重試的次數。\nsource code\n1 2 3 4 5 6 func (r *ItemFastSlowRateLimiter) NumRequeues(item interface{}) int { r.failuresLock.Lock() //鎖不做解釋 defer r.failuresLock.Unlock() //解鎖不做解釋 return r.failures[item] //回傳重試次數 } Forget 當物件做完時需要重新計算放延遲時間與放入次數，需要透過Forget function完成。\nsource code\n1 2 3 4 5 6 func (r *ItemFastSlowRateLimiter) Forget(item interface{}) { r.failuresLock.Lock() //鎖不做解釋 defer r.failuresLock.Unlock() //解鎖不做解釋 delete(r.failures, item) //刪除map裡的物件（重新計算） } 基本上看起來重點在於 When function 的實作，我們可以看到當重試了 N 次若超過設定的次數（maxFastAttempts）就用用慢延遲，不然就用快延遲。\n用以達到短時間內多試幾次，超過嘗試次數就慢慢來。（有點像人類的心態，他壞了要趕快修好，修了好幾次沒好欸，那就慢慢做試試看吧xD)\n怎麼使用 對於 ItemFastSlowRateLimiter 物件而言，他只是實作了 RateLimiter interface，使用者要怎麼用以這個物件為底的 Rate Limiter queue 呢？\n前幾篇篇有提到 RateLimiter 的初始化方法\n1 2 3 4 5 6 func NewRateLimitingQueue(rateLimiter RateLimiter) RateLimitingInterface { return \u0026amp;rateLimitingType{ DelayingInterface: NewDelayingQueue(), //前一小節有提到過delating work queue的newfunction rateLimiter: rateLimiter, //自行實作的rateLimiter } } 使用者可以在傳入參數帶入實作 RateLimiter interface的 ItemFastSlowRateLimiter 物件\n1 NewRateLimitingQueue(NewItemFastSlowRateLimiter(5*time.Millisecond, 3*time.Second, 3) 表示使用者要求的 Rate Limiter queue 用了 DelayingQueue 與 ItemFastSlowRateLimiter 。\n物件延遲時間由 ItemFastSlowRateLimiter 決定 物件延遲的排序方式由 DelayingQueue 決定（之前有提過用heap加上clock來觸發） 存放物件的 queue 由 common queue 決定（之前有提過用 processing set 加上 dirty set 合力完成） 大致上流程是這樣，不清楚的地方可以回去複習之前提到過的元件\n小結 下一章節將介紹另外一個實作 rateLimiter interface的物件 MaxOfRateLimiter ，文章有錯的部分希望大大們指出，謝謝！\n","description":"","id":33,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes Item Fast Slow RateLimiter work queue 設計真d不錯","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/queue/item-fast-slow-ratelimiter/"},{"content":" 先本文所以 source code 基於 kubernetes 1.19 版本，所有 source code 的為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\nkubernetes work queue Kubernetes controller/operator 是一個非常精彩的設計模式，在了解Kubernetes controller/operator 怎麼撰寫之前，了解kubernetes work queue的實作模式是非常重要的，下面引用了How to Create a Kubernetes Custom Controller Using client-go的 controller 架構圖可以看到在 sharedindexinformer 內有引用到這個元件，這個元件實際被定義在 kubernetes 的 client-go library 中。\n圖片來源：How to Create a Kubernetes Custom Controller Using client-go\nKubernetes 為什麼要實踐一個 work queue 呢？就我們所知 kubernetes 是用 go 撰寫應該可以使用 channel 的機制直接將物件送給要用的元件(thread)啊，原因其實非常簡單，go channel 的設計功能非常單一無法滿足 kubernetes 所要的場景，例如帶有延遲時間物件需要根據延遲時間排序的queue，例如限制物件取出速度的queue。\n之先前的章節有提到 common work queue 、 delaying work queue 、 ratelimiting queue，但 ratelimiting queue 有組合 ratelimiter ，本章節將有實作 ratelimiter 同時也滿常被用到的 ItemExponentialFailureRateLimiter 展開解說。\nItemExponentialFailureRateLimiter interface kubernetes source code 設計得非常精美，我們可以先從 interface 定義了哪些方法來推敲實作這個 interface 的物件可能有什麼功能。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type RateLimiter interface { // When gets an item and gets to decide how long that item should wait //當一個物件放入的時候，需要回傳延遲多久（可自定義規則，等等會看到） When(item interface{}) time.Duration // Forget indicates that an item is finished being retried. Doesn\u0026#39;t matter whether its for perm failing\t// or for success, we\u0026#39;ll stop tracking it //當一個物件完成的時候可以，要忘記曾經延遲過（重新計算） Forget(item interface{}) // NumRequeues returns back how many failures the item has had // 回傳物件已經放入幾次（重試了幾次，白話一點呼叫NumRequeues幾次） NumRequeues(item interface{}) int } 看完了抽象的定義之後，必須要回過來看 Item Exponential Failure RateLimiter queue 實際物件定義了哪些屬性\nstruct source code\n1 2 3 4 5 6 7 8 9 // ItemExponentialFailureRateLimiter does a simple baseDelay*2^\u0026lt;num-failures\u0026gt; limit // dealing with max failures and expiration are up to the caller type ItemExponentialFailureRateLimiter struct { failuresLock sync.Mutex //鎖，防止資源競爭 failures map[interface{}]int //計算某個物件呼叫了延遲的次數 baseDelay time.Duration //基礎延遲的時間 maxDelay time.Duration //最多要延遲多久 } 看完了資料結構我們接著來看 ItemExponentialFailureRateLimiter 實作的方法，與初始化方法。\nnew function source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 //不知道為什麼要設定一個空的變數 var _ RateLimiter = \u0026amp;ItemExponentialFailureRateLimiter{} //傳入基礎要等待的時間，最大等待時間，以及初始化failures map資料集 func NewItemExponentialFailureRateLimiter(baseDelay time.Duration, maxDelay time.Duration) RateLimiter { return \u0026amp;ItemExponentialFailureRateLimiter{ failures: map[interface{}]int{}, baseDelay: baseDelay, maxDelay: maxDelay, } } //預設等待時間為 1 Millisecond，最大等待時間為1000秒 func DefaultItemBasedRateLimiter() RateLimiter { return NewItemExponentialFailureRateLimiter(time.Millisecond, 1000*time.Second) } implement function 看完了初始化 ItemExponentialFailureRateLimiter 後接下來看看核心的功能。\nWhen source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func (r *ItemExponentialFailureRateLimiter) When(item interface{}) time.Duration { r.failuresLock.Lock() //鎖不做解釋 defer r.failuresLock.Unlock() //解鎖不做解釋 exp := r.failures[item] //查看map裡面物件放入的次數 r.failures[item] = r.failures[item] + 1 //放入次數＋1 // The backoff is capped such that \u0026#39;calculated\u0026#39; value never overflows. // backoff=base * 2^（物件放入的次數），代表延遲時間為指數型成長。 backoff := float64(r.baseDelay.Nanoseconds()) * math.Pow(2, float64(exp)) //如果backoff 大於int64（溢位）就回傳最大延遲時間 if backoff \u0026gt; math.MaxInt64 { return r.maxDelay } //封裝為延遲時間 calculated := time.Duration(backoff) //延遲時間超過最大延遲時間就回傳最大延遲時間 if calculated \u0026gt; r.maxDelay { return r.maxDelay } return calculated } NumRequeues 當我們需要知道物件已經重試了幾次可以透過NumRequeues function 得知物件重是的次數。\nsource code\n1 2 3 4 5 6 func (r *ItemExponentialFailureRateLimiter) NumRequeues(item interface{}) int { r.failuresLock.Lock() //鎖不做解釋 defer r.failuresLock.Unlock() //解鎖不做解釋 return r.failures[item] //回傳重試次數 } Forget 當物件做完時需要重新計算放延遲時間與放入次數，需要透過Forget function完成。\nsource code\n1 2 3 4 5 6 func (r *ItemExponentialFailureRateLimiter) Forget(item interface{}) { r.failuresLock.Lock() //鎖不做解釋 defer r.failuresLock.Unlock() //解鎖不做解釋 delete(r.failures, item) //刪除map裡的物件（重新計算） } 怎麼使用 對於 ItemExponentialFailureRateLimiter 物件而言，他只是實作了 RateLimiter interface，使用者要怎麼用這個 Rate Limiter queue 呢？\n上一篇有提到 RateLimiter 的初始化方法\n1 2 3 4 5 6 func NewRateLimitingQueue(rateLimiter RateLimiter) RateLimitingInterface { return \u0026amp;rateLimitingType{ DelayingInterface: NewDelayingQueue(), //前一小節有提到過delating work queue的newfunction rateLimiter: rateLimiter, //自行實作的rateLimiter } } 使用者可以在傳入參數帶入實作 RateLimiter interface的 ItemExponentialFailureRateLimiter 物件\n1 NewRateLimitingQueue(NewItemExponentialFailureRateLimiter(time.Millisecond, 1000*time.Second) 表示使用者要求的 Rate Limiter queue 用了 DelayingQueue 與 ItemExponentialFailureRateLimiter。\n物件延遲時間由ItemExponentialFailureRateLimiter決定 物件延遲的排序方式由 DelayingQueue 決定（之前有提過用heap加上clock來觸發） 存放物件的 queue 由 common queue 決定（之前有提過用 processing set 加上 dirty set 合力完成） 大致上流程是這樣，不清楚的地方可以回去複習之前提到過的元件\n小結 下一章節將介紹另外一個實作 rateLimiter interface的物件ItemFastSlowRateLimiter，文章有錯的部分希望大大們指出，謝謝！\n","description":"","id":34,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes Item Exponential Failure RateLimiter work queue 設計真d不錯","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/queue/item-exponential-failure-ratelimiter/"},{"content":" 首先本文所以 source code 基於 kubernetes 1.19 版本，所有 source code 的為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\nkubernetes work queue 在前一篇 kubernetes delaying work queue 設計真d不錯一文中分享 kubernetes work queue最基礎的實作方式，再複習一次！Kubernetes 為什麼要實踐一個 work queue 呢？\n就我們所知 kubernetes 是用 go 撰寫應該可以使用 channel 的機制直接將物件送給要用的元件(thread)啊，原因其實非常簡單，go channel 的設計功能非常單一無法滿足 kubernetes 所要的場景，例如帶有延遲時間物件需要根據延遲時間排序的queue，例如限制物件取出速度的queue。\n圖片來源：How to Create a Kubernetes Custom Controller Using client-go\n上圖引用了How to Create a Kubernetes Custom Controller Using client-go的 controller 架構圖可以看到在 sharedindexinformer 內有引用到這個元件，這個元件實際被定義在 kubernetes 的 client-go library 中。\n在第二步驟與第三步驟之間透過 queue 不只解偶了上下層的耦合關係同時 Queue 有達到了消峰填谷的作用，當觀察的物件一直送資料進來不會因為我們業務邏輯處理得太慢而卡住，資料會保留在 queue 中直到被取出。\n之前有提到了兩種 queue ，分別是 rate limiters queue 以及 delaying queue ，上一章節介紹完 Kubernetes delaying queue ，本篇文章將介紹最後一個 rate limiters queue ！\nRateLimiter queue kubernetes source code 設計得非常精美，我們可以先從 interface 定義了哪些方法來推敲實作這個 interface 的物件可能有什麼功能。\ninterface source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // RateLimitingInterface is an interface that rate limits items being added to the queue. //RateLimiting interface 結合了 delaying work queue ，透過一些方法計算 物件的延遲速率 // 再交由 delaying work queue 丟入Heap 最後放入 common queue中。 type RateLimitingInterface interface { DelayingInterface // delaying work queue 前一篇有介紹過 // AddRateLimited adds an item to the workqueue after the rate limiter says it\u0026#39;s ok AddRateLimited(item interface{}) // 對某個物件加入延遲 // Forget indicates that an item is finished being retried. Doesn\u0026#39;t matter whether it\u0026#39;s for perm failing // or for success, we\u0026#39;ll stop the rate limiter from tracking it. This only clears the `rateLimiter`, you // still have to call `Done` on the queue. Forget(item interface{}) // 表示某個物件已經做了 // NumRequeues returns back how many times the item was requeued NumRequeues(item interface{}) int //計算某個物件調用queueAddRateLimited的次數 } 看完了抽象的定義之後，必須要回過來看 delaying queue 實際物件定義了哪些屬性\nstruct source code\n1 2 3 4 5 6 7 // rateLimitingType wraps an Interface and provides rateLimited re-enquing type rateLimitingType struct { DelayingInterface // 嵌入 delaying work queue 表示 rateLimitingType 也有 delay work queue的能力 //（不一定需要傳入實作的物件，本身就是DelayingInterface的一種） rateLimiter RateLimiter //組合了一個 RateLimiter interface，需要明確傳入實作的物件。 } 剛剛上面有一個疑點那就 type RateLimiter 到底是什麼\n我們先來看看 type RateLimiter 的結構\nRateLimiter 由於 rateLimitingType 組合了一個 RateLimiter 我們來看看 RateLimiter 是什麼吧！\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type RateLimiter interface { // When gets an item and gets to decide how long that item should wait //當一個物件放入的時候，需要回傳延遲多久（可自定義規則，等等會看到） When(item interface{}) time.Duration // Forget indicates that an item is finished being retried. Doesn\u0026#39;t matter whether its for perm failing\t// or for success, we\u0026#39;ll stop tracking it //當一個物件完成的時候可以，要忘記曾經延遲過（重新計算） Forget(item interface{}) // NumRequeues returns back how many failures the item has had // 回傳物件已經放入幾次（重試了幾次，白話一點呼叫whem幾次） NumRequeues(item interface{}) int } 看到這裡想必大家一定亂了，我幫大家整理一下個類別之間的關係。\n看圖說故事的時間到了\nInterface 繼承關係\n1. Interface 為 common work queue 的介面（先用介面表示抽象方法，這裡就不用Interface會搞混....） 2. DelayingInterface 為 delaying work queue 的介面，這個介面繼承了Interface 3. RateLimitingInterface 繼承了 delaying work queue 的介面 rateLimitingType 有什麼能力\n- rateLimitingType 嵌入(embedding)了 DelayingInterface ，表示 `rateLimitingType` 有 DelayingInterface 的能力（用嵌入委託給其他人） - rateLimitingType 實作了 RateLimitingInterface ，表示 `rateLimitingType` 有 RateLimitingInterface 的能力（這個能力要自己實作） rateLimitingType 依賴\n- 由於 rateLimitingType 組合了(Composite) rateLimiter ，所以在 UML 上呈現的是依賴 rateLimiter - rateLimiter 被 ItemExponentialFailureRateLimiter 實作（還由其他物件實作，我這裡沒畫出來） struct function rateLimitingType 實作了 RateLimitingInterface 我們來看一下他實作了什麼\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // AddRateLimited AddAfter\u0026#39;s the item based on the time when the rate limiter says it\u0026#39;s ok // rateLimitingType 本身實作了 RateLimitingInterface func (q *rateLimitingType) AddRateLimited(item interface{}) { // 由於嵌入了 DelayingInterface 可以直接使用 delay work queue的抽象方法 // 這裡就是說放入的物件要延遲多久，延遲多久就由組合的 rateLimiter 抽象方法實作。 q.DelayingInterface.AddAfter(item, q.rateLimiter.When(item)) } // rateLimitingType 本身實作了 RateLimitingInterface func (q *rateLimitingType) NumRequeues(item interface{}) int { // 這裡就是說放入的物件已經被重複放入多少次了，放入多少次就由組合的 rateLimiter 抽象方法實作。 return q.rateLimiter.NumRequeues(item) } // rateLimitingType 本身實作了 RateLimitingInterface func (q *rateLimitingType) Forget(item interface{}) { // 這裡就是說放入的物件已經被做完了，可以重新計算了，從新計算的方法就交給組合的 rateLimiter 抽象方法實作。 q.rateLimiter.Forget(item) } 其實看到這裡，我覺得 rateLimitingType 有點像是 proxy 設計模式，只要有人實作了 RateLimiter 就可以放進來給上層做使，反正上層只會呼叫 AddRateLimited 、NumRequeues 、Forget。\nKubernetes 這邊做的鬆偶做得很不錯， 有新的計算延遲的方法 rateLimitingType 根本不用動，透過 rateLimitingType 代理呼叫 RateLimiter 就好，更上層的使用者也只會使用 RateLimitingInterface 而已。\nnew function source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // NewRateLimitingQueue constructs a new workqueue with rateLimited queuing ability // Remember to call Forget! If you don\u0026#39;t, you may end up tracking failures forever. //使用者在使用RateLimiting的時候可以傳入自己實作的RateLimiter，此時使用預設的delay work queue。 func NewRateLimitingQueue(rateLimiter RateLimiter) RateLimitingInterface { return \u0026amp;rateLimitingType{ DelayingInterface: NewDelayingQueue(), //前一小節有提到過delating work queue的newfunction rateLimiter: rateLimiter, //自行實作的rateLimiter } } //使用者在使用RateLimiting的時候可以傳入自己實作的RateLimiter，此時使用預設的delaying work queue並且可以設定delaying work queue的metric name。 func NewNamedRateLimitingQueue(rateLimiter RateLimiter, name string) RateLimitingInterface { return \u0026amp;rateLimitingType{ DelayingInterface: NewNamedDelayingQueue(name), //前一小節有提到delating work queue的newfunction（可以設定metric name） rateLimiter: rateLimiter, //自行實作的rateLimiter } } 下一章節會講解實作rateLimiter介面的物件Item Exponential Failure RateLimiter，我們來看看 kubernetes 怎麼限制物件進入 queue 的速度！\n","description":"","id":35,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes RateLimite work queue 設計真d不錯","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/queue/rating/"},{"content":" 首先本文所以 source code 基於 kubernetes 1.19 版本，所有 source code 的為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\nkubernetes work queue 在前一篇 kubernetes common work queue 設計真d不錯一文中分享 kubernetes work queue最基礎的實作方式，再複習一次！Kubernetes 為什麼要實踐一個 work queue 呢？\n就我們所知 kubernetes 是用 go 撰寫應該可以使用 channel 的機制直接將物件送給要用的元件(thread)啊，原因其實非常簡單，go channel 的設計功能非常單一無法滿足 kubernetes 所要的場景，例如帶有延遲時間物件需要根據延遲時間排序的 queue ，例如限制物件取出速度的 queue 。\n圖片來源：How to Create a Kubernetes Custom Controller Using client-go\n上圖引用了How to Create a Kubernetes Custom Controller Using client-go的 controller 架構圖可以看到在 sharedindexinformer 內有引用到這個元件，這個元件實際被定義在 kubernetes 的 client-go library 中。\n在第五步驟與第六步驟之間透過 queue 不只解偶了上下層的耦合關係同時 Queue 有達到了消峰填谷的作用，當觀察的物件一直送資料進來不會因為我們業務邏輯處理得太慢而卡住，資料會保留在 queue 中直到被取出。\n之前有提到了兩種 queue ，分別是 rate limiters queue 以及 delaying queue ，上一章節介紹完 Kubernetes 通用的 common queue ，本篇文章會從 delaying queue 開始探討。\ndelaying queue kubernetes source code 設計得非常精美，我們可以先從 interface 定義了哪些方法來推敲實作這個 interface 的物件可能有什麼功能。\ninterface source code\n1 2 3 4 5 6 7 8 9 // DelayingInterface is an Interface that can Add an item at a later time. This makes it easier to // requeue items after failures without ending up in a hot-loop. // 原生的註解寫得非常棒了，大致上意思為一個物件處理失敗，如果很快地物件在被處理一次失敗的可能性還是很高會造成 // hot-loop，所以讓物件等待一下在排隊進入 queue 就是 delaying queue 的用意 type DelayingInterface interface { Interface //嵌入了common work queue的interface，delaying queue 也是common queue 的一種 AddAfter(item interface{}, duration time.Duration) //表示物件需要等待多久才能被放入 queue 中 } 看完了抽象的定義之後，必須要回過來看 delaying queue 實際物件定義了哪些屬性\nstruct source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // delayingType wraps an Interface and provides delayed re-enquing type delayingType struct { Interface //嵌入了一個common queue // clock tracks time for delayed firing clock clock.Clock //用來比對物件延遲時間 // stopCh lets us signal a shutdown to the waiting loop stopCh chan struct{} //異步退出用 // stopOnce guarantees we only signal shutdown a single time stopOnce sync.Once //異步退出用，保證退出只會被呼叫一次 // heartbeat ensures we wait no more than maxWait before firing heartbeat clock.Ticker //定時器，定時喚醒thread處理物件 // waitingForAddCh is a buffered channel that feeds waitingForAdd waitingForAddCh chan *waitFor //用以添加延遲物件的channel // metrics counts the number of retries metrics retryMetrics //用以紀錄重試的metric } 剛剛上面有一個疑點那就 type waitFor 到底是什麼\n我們先來看看 type waitFor 的結構\nsource code\n1 2 3 4 5 6 7 8 // waitFor holds the data to add and the time it should be added // 如果需要延遲的物件都會被轉換成這個類型 type waitFor struct { data t // t 在common queue介紹過，為一個泛行表示什麼都接受的物件 readyAt time.Time //在什麼時間加入到queue中的 // index in the priority queue (heap) index int // index會用在後面的排序，延遲時間較小的排前面（用heap排序） } heap 排序 由於放入 dealying queue 的物件，有的可能要延遲 1s 有的可能要延遲 2ms等等， dealying queue 如何保證延遲較小的物件先放入 queue 呢？\ndelaying queue 透過 heap 進行排序，底下讓展開排序的實作方式。\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 // waitForPriorityQueue implements a priority queue for waitFor items. // // waitForPriorityQueue implements heap.Interface. The item occurring next in // time (i.e., the item with the smallest readyAt) is at the root (index 0). // Peek returns this minimum item at index 0. Pop returns the minimum item after // it has been removed from the queue and placed at index Len()-1 by // container/heap. Push adds an item at index Len(), and container/heap // percolates it into the correct location. type waitForPriorityQueue []*waitFor //waitForPriorityQueue 這個類型實作了 heap interface ，排序的物件為 waitFor //實作heap interface 的len方法，取出heap當前的長度。 func (pq waitForPriorityQueue) Len() int { return len(pq) } //實作 heap interface 的 Less 方法，確認在 waitForPriorityQueue 的第 i 個元素是否比第 j 個元素小 // 若是第 i 個元素比第 j 個元素小就交換，因為我們希望，因為我們希望越小的排越前面。 func (pq waitForPriorityQueue) Less(i, j int) bool { return pq[i].readyAt.Before(pq[j].readyAt) // 比的是時間 } //實作 heap interface 的 swap ，實作 i j 交換 func (pq waitForPriorityQueue) Swap(i, j int) { pq[i], pq[j] = pq[j], pq[i] pq[i].index = i pq[j].index = j } //實作 heap interface 的 Push ，向 heap 添加物件 func (pq *waitForPriorityQueue) Push(x interface{}) { n := len(*pq) item := x.(*waitFor) item.index = n //新加入的物件會記錄當前自己的位置 *pq = append(*pq, item) //新加入的物件排到heap的最後面 } //實作 heap interface 的 Pop ，從 heap 的尾巴彈出最後一個物件。 func (pq *waitForPriorityQueue) Pop() interface{} { n := len(*pq) item := (*pq)[n-1] item.index = -1 *pq = (*pq)[0:(n - 1)] //縮小heap，移除最後一個物件 return item } //回傳heap第一個物件，延遲時間最短的那一個物件 func (pq waitForPriorityQueue) Peek() interface{} { return pq[0] } 看完了資料結構我們接著來看 delaying queue 實作的方法，與初始化方法！\nnew function source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 //給心跳用最久10秒delaying queue會檢查一次 const maxWait = 10 * time.Second //傳入clock與common queue以及紀錄這個物件metric的名字，這是一個不公開的方法，有其他封裝好的 new function 可以用 func newDelayingQueue(clock clock.Clock, q Interface, name string) *delayingType { ret := \u0026amp;delayingType{ Interface: q, clock: clock, heartbeat: clock.NewTicker(maxWait), stopCh: make(chan struct{}), waitingForAddCh: make(chan *waitFor, 1000), metrics: newRetryMetrics(name), } // 啟動一個 thread 檢測有沒有 wiatfor 物件在等待進入 queue，稍後會展開分析。 go ret.waitingLoop() return ret } // 不同的封裝方式，不提 （設定 metric 的 name 為空） func NewDelayingQueue() DelayingInterface { return NewDelayingQueueWithCustomClock(clock.RealClock{}, \u0026#34;\u0026#34;) } //不同的封裝方式，不提 （設定 metric 的 name 為空，可傳入自己實作的 common queue ） func NewDelayingQueueWithCustomQueue(q Interface, name string) DelayingInterface { return newDelayingQueue(clock.RealClock{}, q, name) } //不同的封裝方式，不提 （可傳入 metric 的 name ） func NewNamedDelayingQueue(name string) DelayingInterface { return NewDelayingQueueWithCustomClock(clock.RealClock{}, name) } //不同的封裝方式，不提 （可傳入 metric 的 name 以及 clock ） func NewDelayingQueueWithCustomClock(clock clock.Clock, name string) DelayingInterface { // NewNamed 是前一章節提到建立 common queue的方法 return newDelayingQueue(clock, NewNamed(name), name) } implement function 看完了初始化 delaying queue function 後接下來看看核心的功能。\nsource code\nAddAfter 使用者要放入有延遲的物件需要呼叫這個function，帶入要延遲的物件，以及該物件要延遲多久。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // AddAfter adds the given item to the work queue after the given delay func (q *delayingType) AddAfter(item interface{}, duration time.Duration) { // don\u0026#39;t add if we\u0026#39;re already shutting down if q.ShuttingDown() { //如果queue關閉了就不能放入 return } q.metrics.retry() //metric不解釋 // immediately add things with no delay if duration \u0026lt;= 0 { //如果延遲時間小於等於0表示不用延遲 q.Add(item) //直接丟入common queue中 return } select { case \u0026lt;-q.stopCh: //因為可能會組塞在 waitingForAddCh 透過 stop 保證退出？ // unblock if ShutDown() is called // 要延遲的物件會封裝成 waitFor 型態並且方入 channel 等待處理 case q.waitingForAddCh \u0026lt;- \u0026amp;waitFor{data: item, readyAt: q.clock.Now().Add(duration)}: } } waitingLoop 這邊是delaying queue的主要邏輯，執行檢查 waitingForAddCh channel 有沒有延遲物件，取出延遲物件看延遲時間是達到選擇加入 Heap 或是 queue，以及接收心跳包。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 // waitingLoop runs until the workqueue is shutdown and keeps a check on the list of items to be added. func (q *delayingType) waitingLoop() { defer utilruntime.HandleCrash() // Make a placeholder channel to use when there are no items in our list never := make(\u0026lt;-chan time.Time) //我不是很定他的用意...可以看到 nerver channel 又換一個別名表示 // Make a timer that expires when the item at the head of the waiting queue is ready var nextReadyAtTimer clock.Timer //當 heap 吐出一個延遲物件時透過這個 timer 延遲 waitingForQueue := \u0026amp;waitForPriorityQueue{} // heap 物件 heap.Init(waitingForQueue) // heap初始化 waitingEntryByData := map[t]*waitFor{} //用來防止同一個物件重複放入，如果有重複的物件就更新延遲時間 for { //如果queue關閉就離開 if q.Interface.ShuttingDown() { return } //標記現在時間 now := q.clock.Now() // 如果在 heap 裡面有東西 for waitingForQueue.Len() \u0026gt; 0 { //拿出第一個在 heap 的物件 entry := waitingForQueue.Peek().(*waitFor) //如果現在時間還沒達到物件要等待的時間就退出 if entry.readyAt.After(now) { break } //如果現在時間達到物件要等到的時間，將物件從heap彈出 entry = heap.Pop(waitingForQueue).(*waitFor) //加到queue中 q.Add(entry.data) //刪除set儲存的物件 delete(waitingEntryByData, entry.data) } // Set up a wait for the first item\u0026#39;s readyAt (if one exists) nextReadyAt := never //在上面有提到過nerver channel 只換成這個名字，不知道用意為何 // 如果在 heap 裡面有東西 if waitingForQueue.Len() \u0026gt; 0 { // 若是前一個物件的計時器有殘留物就清除前一個物件的計時器 if nextReadyAtTimer != nil { nextReadyAtTimer.Stop() } //拿出第一個在 heap 的物件 entry := waitingForQueue.Peek().(*waitFor) //看物件延遲多久 nextReadyAtTimer = q.clock.NewTimer(entry.readyAt.Sub(now)) //當物件延遲時間到了發通知 nextReadyAt = nextReadyAtTimer.C() } select { // queue關閉 case \u0026lt;-q.stopCh: return // 定時被心跳喚醒 case \u0026lt;-q.heartbeat.C(): // continue the loop, which will add ready items // 當收到物件延遲時間到了發通知 case \u0026lt;-nextReadyAt: // continue the loop, which will add ready items //當有人放需要延遲的物件進queue中 case waitEntry := \u0026lt;-q.waitingForAddCh: //如果新放入的物件還沒超過延遲時間 if waitEntry.readyAt.After(q.clock.Now()) { //放入heap中 insert(waitingForQueue, waitingEntryByData, waitEntry) } else { //已經到了延遲時間直接放入queue q.Add(waitEntry.data) } //一次取光用 drained := false for !drained { select { // 一次把把延遲物件的channel取乾淨 case waitEntry := \u0026lt;-q.waitingForAddCh: //如果新放入的物件還沒超過延遲時間 if waitEntry.readyAt.After(q.clock.Now()) { //放入heap中 insert(waitingForQueue, waitingEntryByData, waitEntry) } else { //已經到了延遲時間直接放入queue q.Add(waitEntry.data) } default: // 保證會退出這個取光的loop drained = true } } } } } insert 在 waitingLoop 有使用到 insert 這個 function ，他的實作也相當的簡單，簡單的來說就是把 waitfor 的物件放到 Heap 中，我們來看看他是如何實作的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // insert adds the entry to the priority queue, or updates the readyAt if it already exists in the queue func insert(q *waitForPriorityQueue, knownEntries map[t]*waitFor, entry *waitFor) { //先判斷加入的物件有沒有重複的 existing, exists := knownEntries[entry.data] //若是有重複的話 if exists { // 跟之前放入的物件比較哪個延遲時間比較短 // 若是現在要放入的物件比較短的話就更新 set 中的物件延遲時間 if existing.readyAt.After(entry.readyAt) { existing.readyAt = entry.readyAt heap.Fix(q, existing.index) } return } //如果 set 沒有重複的話就直接加到 heap 中，以及使用 set 紀錄 heap 有這個物件。 heap.Push(q, entry) knownEntries[entry.data] = entry } 小結 本章講述了 kubernetes delaying work queue 的底層實作方法，接下來還會有幾篇介紹基於 common work queue 的 rate limiters work queue 以及 其他類型的 work queue ，從中我們可以了解 kubernetes controller 監聽到 etcd 變化的物件後如何把 變化的物件丟入 queue 中等待其他人取出並處理，相關業務邏輯，如果文中有錯希望大家不吝嗇提出，讓我們互相交流學習。\n","description":"","id":36,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes delaying work queue 設計真d不錯","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/queue/delaying/"},{"content":" 首先本文所以 source code 基於 kubernetes 1.19 版本，所有 source code 的為了版面的整潔會精簡掉部分 log 相關的程式碼，僅保留核心邏輯，如果有見解錯誤的地方，還麻煩觀看本文的大大們提出，感謝！\nkubernetes work queue Kubernetes controller/operator 是一個非常精彩的設計模式，在了解Kubernetes controller/operator 怎麼撰寫之前，了解kubernetes work queue的實作模式是非常重要的，下面引用了How to Create a Kubernetes Custom Controller Using client-go的 controller 架構圖可以看到在 sharedindexinformer 內有引用到這個元件，這個元件實際被定義在 kubernetes 的 client-go library 中。\n圖片來源：How to Create a Kubernetes Custom Controller Using client-go\nKubernetes 為什麼要實踐一個 work queue 呢？就我們所知 kubernetes 是用 go 撰寫應該可以使用 channel 的機制直接將物件送給要用的元件(thread)啊，原因其實非常簡單，go channel 的設計功能非常單一無法滿足 kubernetes 所要的場景，例如帶有延遲時間物件 queue 就需要根據延遲時間排序 ，例如限制物件取出速度的 queue 。\n剛剛提到了兩種 queue ，kubernetes 實作上稱為 rate limiters queue 以及 delaying queue ，此外還有一種通用的 common queue ，本篇文章會先從 common queue 開始探討。\ncommon queue kubernetes source code 設計得非常精美，我們可以先從 interface 定義了哪些方法來推敲實作這個 interface 的物件可能有什麼功能。\ninterface source code\n1 2 3 4 5 6 7 8 type Interface interface { Add(item interface{}) //向 queue 送出 interface 類型的物件 Len() int //拿到目前儲存在 queue 中的物件的數量 Get() (item interface{}, shutdown bool) //拿到 queue 中第 0 個 item，另外 queue 是否已經關閉 Done(item interface{}) //告知 queue 某個 item 已經處理完成 ShutDown() //關閉 queue ShuttingDown() bool //查詢 queue 是否關閉 } 看完了抽象的定義之後，必須要回過來看 common queue 實際物件定義了哪些屬性\nstruct source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // Type is a work queue (see the package comment). type Type struct { // queue defines the order in which we will work on items. Every // element of queue should be in the dirty set and not in the // processing set. queue []t // queue 的 item 用 slice 儲存，item 的類型為 t 等等會看到 t 是什麼 // dirty defines all of the items that need to be processed. dirty set // 用來判斷我們哪些 item 還沒處理，是一個 set 的集合 // Things that are currently being processed are in the processing set. // These things may be simultaneously in the dirty set. When we finish // processing something and remove it from this set, we\u0026#39;ll check if // it\u0026#39;s in the dirty set, and if so, add it to the queue. processing set // 儲存我們現在哪些 item 正在處理，是一個 set 的集合。 cond *sync.Cond //同步鎖，用來通知其他thread可以取item shuttingDown bool //標記 queue是否關閉 metrics queueMetrics // metric不解釋 unfinishedWorkUpdatePeriod time.Duration // 給 metric 用確認 queue 是否還活著 clock clock.Clock // 給 metric 用確認 queue 是否還活 } 剛剛上面有一個疑點那就 type t 到底是什麼，以及 type set 到底是什麼。\n我們先來看看 type t 的結構\nsource code\n1 type t interface{} //其實也沒什麼好說的，就是一個泛形表示什麼類型的物件我都能吃 接下來看看 type set 的結構\nsource code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type empty struct{} type set map[t]empty // type set 就是一個 map ，key 是一個泛型物件 值就是空結構 // set 資料結構的，判斷有沒有 t 類型的物件 func (s set) has(item t) bool { _, exists := s[item] return exists } // set 資料結構的，插入 t 類型的物件 func (s set) insert(item t) { s[item] = empty{} } // set 資料結構的，刪除 t 類型的物件 func (s set) delete(item t) { delete(s, item) } 看完了資料結構我們接著來看 common work queue 實作的方法，與初始化方法。（看到common work queue的型態為Type讓我一直搞混\u0026hellip;.）\nnew function source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 //透過 NewQueue function 建立一個common work queue。 //其中需要帶入 clock , metric , updatePeriod 參數 func newQueue(c clock.Clock, metrics queueMetrics, updatePeriod time.Duration) *Type { t := \u0026amp;Type{ clock: c, dirty: set{}, processing: set{}, cond: sync.NewCond(\u0026amp;sync.Mutex{}), //metric物件 metrics: metrics, //定期檢測時間 unfinishedWorkUpdatePeriod: updatePeriod, } // 啟動一個 thread 檢測 queue 是否關閉,並且定期回報 metric go t.updateUnfinishedWorkLoop() return t } const defaultUnfinishedWorkUpdatePeriod = 500 * time.Millisecond // 不同的封裝方式，不提 （設定 metric 的 name 為空） func New() *Type { return NewNamed(\u0026#34;\u0026#34;) } // 不同的封裝方式，提供 name （設定 metric 的 name ） func NewNamed(name string) *Type { rc := clock.RealClock{} return newQueue( rc, globalMetricsFactory.newQueueMetrics(name, rc), defaultUnfinishedWorkUpdatePeriod, ) } // func (q *Type) updateUnfinishedWorkLoop() { //設置了一個定時器 t := q.clock.NewTicker(q.unfinishedWorkUpdatePeriod) //在function結束的時候停止計時器 defer t.Stop() //當受到計時器訊號時 for range t.C() { if !func() bool { // Lock 鎖 q.cond.L.Lock() // 當離開時解鎖 defer q.cond.L.Unlock() // 判斷 queue 是否關閉（沒有關閉） if !q.shuttingDown { //告訴 metric work queue 還沒關閉 q.metrics.updateUnfinishedWork() // 回傳繼續等在 計時器訊號 return true } // queue 關閉了 跳出整個 updateUnfinishedWorkLoop() function return false }() { // queue 關閉了 跳出整個 updateUnfinishedWorkLoop() function return } } } implement function 看完了初始化common work queue function 後接下來看看核心的功能。\nsource code\nAdd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // Add marks item as needing processing. func (q *Type) Add(item interface{}) { // lock鎖 q.cond.L.Lock() // 離開時解鎖 defer q.cond.L.Unlock() // 如果queue 關閉就離開 if q.shuttingDown { return } // 如果 dirty set 裡面已經有了表示物件已經儲存過但還沒被處理那就離開 if q.dirty.has(item) { return } // metric 不解釋 q.metrics.add(item) // 放入 dirty set ，表示物件等待處理 q.dirty.insert(item) // 物件如果在 processing set 就退出 if q.processing.has(item) { return } //加入queue q.queue = append(q.queue, item) //告知其他thread可以來取物件了 q.cond.Signal() } 這邊看似很簡單實際上有點複雜，有幾種狀態需要特別用圖片來解釋。\n如果 queue 關閉了就不能放入物件 如果 dirty set 內有這個物件，表示誰都還沒處理過物件，那這個物件就不能放到 dirty 裡面裡面 換句話說 diry set 表示物件放進去 queue 過但還沒有被處理 如果 processing set 內有這個物件表示，有人正在處理這個物件，新進來的就先丟到 dirty set 裡面吧 其中 2 , 3 較難理解，我透過圖片來幫助我們了解狀況。\n基本上 2 , 3 的邏輯如圖所示，比較有問題的應該是在 processing set 有東西的時候 ，還要放入 item 為什麼不會放到 queue 裡面去呢？這會在後面的分析中看到！\nGet source code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // Get blocks until it can return an item to be processed. If shutdown = true, // the caller should end their goroutine. You must call Done with item when you // have finished processing it. func (q *Type) Get() (item interface{}, shutdown bool) { // lock 鎖 q.cond.L.Lock() // 離開function 還回lock defer q.cond.L.Unlock() // 如果 queue 長度=0代表 queue 中沒東西，而且 queue 狀態為啟動的狀態，這裡會開始睡覺等別人通知說可以來取貨再醒來 for len(q.queue) == 0 \u0026amp;\u0026amp; !q.shuttingDown { q.cond.Wait() } //如果醒來後發現 queue 中長度為0而且 queue 狀態為關閉的那就退出告訴 caller 說 queue 已經關閉了 if len(q.queue) == 0 { // We must be shutting down. return nil, true } //取出 queue 列隊中第一個元素，並且讓後面的元素往前推進一個 item, q.queue = q.queue[0], q.queue[1:] //metric不解釋 q.metrics.get(item) // 把拿出來item丟入processing set 裡面，表示正在處理。 q.processing.insert(item) // dirty set 刪除 item 表示有人已經把 item 拿走，准許再把相同的 item 放進入 q.dirty.delete(item) // 回傳 queue 列隊中第一個元素，並且告知使用者 queue 有沒有關閉 return item, false } 這裏 Get 的邏輯十分簡單，使用者透過 Get 就能拿到 queue 裡面第一筆資料，若是 queue 中沒有資料會 pedding 等到有人通知來拿資料才會醒來 ，此外若是 Get function 告訴使用者說 queue 已經關閉了，那麼 使用者應該把處理這段的 thread 關掉。\nDone 這邊是告訴 common work queue 說我們已經做完了，可以把 processing set 所標記的資料刪除囉，那我們就深入 source code 來看看 kubernetes 是怎麼處理的吧！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Done marks item as done processing, and if it has been marked as dirty again // while it was being processed, it will be re-added to the queue for // re-processing. func (q *Type) Done(item interface{}) { // lock 鎖 q.cond.L.Lock() // 離開function 還回lock defer q.cond.L.Unlock() //metric不進行說明 q.metrics.done(item) //從processing set 刪除，表示我們已經做完了！ q.processing.delete(item) //如果 dirty set 裡面還有的話我們就把他加入 queue 中讓他回去排隊等待處理 if q.dirty.has(item) { q.queue = append(q.queue, item) // 告訴其他thread該醒了，來拿新的item囉！ q.cond.Signal() } } 流程大致上如下圖所示\nLen 主要回傳給使用者，現在 queue 長度為多少。\n1 2 3 4 5 6 7 8 9 10 11 12 13 // Len returns the current queue length, for informational purposes only. You // shouldn\u0026#39;t e.g. gate a call to Add() or Get() on Len() being a particular // value, that can\u0026#39;t be synchronized properly. func (q *Type) Len() int { // lock 鎖 q.cond.L.Lock() // 離開function 還回lock defer q.cond.L.Unlock() //回傳目前 queue 的長度 return len(q.queue) } ShutDown 關閉 queue ， queue 不再接受 add() ，使用者只能從 queue 中取值直到取完為止。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // ShutDown will cause q to ignore all new items added to it. As soon as the // worker goroutines have drained the existing items in the queue, they will be // instructed to exit. func (q *Type) ShutDown() { // lock 鎖 q.cond.L.Lock() // 離開function 還回lock defer q.cond.L.Unlock() //設定queue為關閉狀態 q.shuttingDown = true //告知所有要取 queue 的人說，我要關了，快來取值。 q.cond.Broadcast() } ShuttingDown 查詢當前 queue 是否關閉\n1 2 3 4 5 6 7 8 9 10 func (q *Type) ShuttingDown() bool { // lock 鎖 q.cond.L.Lock() // 離開function 還回lock defer q.cond.L.Unlock() // 回傳queue是否關閉 return q.shuttingDown } 小結 本章講述了 kubernetes common work queue 的底層實作方法，接下來還會有兩篇介紹基於 common work queue的 rate limiters queue 以及 delaying queue ，從中我們可以了解 kubernetes controller 監聽到 etcd 變化的物件後如何把 變化的物件丟入 queue 中等待其他人取出並處理，相關業務邏輯，如果文中有錯希望大家不吝嗇提出，讓我們互相交流學習。\n","description":"","id":37,"section":"posts","tags":["kubernetes","controller","source-code"],"title":"Kubernetes common work queue 設計真d不錯","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/controller/queue/common/"},{"content":" 我們會在 consumer 設定 max.poll.interval.ms ，例如這個數值設定為 300 表示300 毫秒要去向 kafka brcoker 拉取資料，若是 consumer 消費訊息花了太多時間就會噴出 CommitFailedException 這個錯誤訊息。\n社群解決方案 增加拉取時間 比如消費 100 筆資料我們要花費 400ms 的時間，那麼設定max.poll.interval.ms為300就不符合當前場景的需求。\n顯示的增加這個數值很直接的能解這個錯誤的產生，相對的 TPS 就會下降。（花的時間一樣，我們只是跟Kafka說我們拉資料的間隔時間拉長而已，有種掩耳盜鈴的感覺）\n減少 poll 消息數量 max.poll.records 是設定 cunsumer 向 kafka brocker 一次拉下來的量是多少，假設是1000筆資料，那我們在 max.poll.interval.ms 300 ms 單位時間內最多只能處理 800 筆資料，那這個數值設定為 1000 似乎也不符合我們的應用場景，酌量下降設定為 750 可能會是一個比較好的做法，但是也會造成 TPS 下降。（花的時間一樣，我們只是跟 Kafka 說我們一次垃少一點的資料，一樣有種掩耳盜鈴的感覺）\n還可以怎麼做 社群解決方案需要對自身系統足夠了解，這裡我舉一個例子。\n我們需要搞清楚你的 consumer 系統消費每條訊息的平均花費時間是多少。\n比如我們的 consumer 邏輯是從 Kafka brocker 拉取消息後，進行一些業務處理最後再寫入下游的永久儲存 MySQL 中。我們可以觀測在最後一哩路存到 MySQL 的平均花費時間不超過 1 秒，我們大致上可以認為這筆訊息只要 1 秒的時間被消費。\n如果 consumer 的 max.poll.records 設定為 400，那麼消費這些訊息要大概花上 1000 秒的時間，因此我們 Consumer 的 max.poll.interval.ms 數值就不能設定 400 秒。了解了自身系統後可以針對這兩個數值去調整減少CommitFailedException的錯誤發生，另外社群給出調整這兩項數值是傷害TPS的。\n我推薦這麼做 縮短資料處理時間\n很直觀的若是一條若是一條訊息原始處理時間為 50ms , 現在經過優化過後處理時間變為 40 ms 速度上直接提升了1.25倍，但是這也是最難做的部分需要優化處理流程（演算法、資料結構）。 使用Muti Thread 來處理訊息 Tips: 不能在多個 Thread 中共享同一個 KafkaConsumer instance ，否則會產生 ConcurrentModificationException Error\n我們向kafka brocker 說拉取了資料，這時候要啟動多條Thread 來處理這些訊息。\n我們假假設每條訊息都開一條Thread來處理（先不考慮thread pool），如果有1000條訊息就會有1000條thread，如下圖所示。\n1 2 3 4 5 6 7 message(1000筆－－－－－－－├─thread(1) ├─thread(2) ├─thread(3) ├─thread(.) ├─thread(..) └─thread(1000) 我認為困難點是我要怎麼知道Thread全部做完了，什麼時候要commit 要怎麼commit 用async還是用sync開開Thread很簡單，但commit才是最麻煩的地方。\n缺點 實作起來非常複雜且麻煩，在 Thread 中維護資料確定哪些沒有 commit 哪些 commit 了 非常的費心思，另外 如果用等冪的 topic 資料在 partaction 會是有順序的，例如 partation 的資料順序為 1 2 3 4 5 6 7 8 9 10 ，如果採用 MutiThread的方案會在造成 處理的順序是隨機的，可能處理的順序變為 1 4 3 2 10 9 7 8 之類的。\n","description":"","id":38,"section":"posts","tags":["kafka","queue"],"title":"Kafka consumer CommitFailedException 好煩","uri":"https://blog.jjmengze.website/zh-tw/posts/queue/kafka/kafka-commitfailed/"},{"content":" 紀錄使用Kafka開發隊友與我的採坑之旅並如何解決問題的，如果有錯的地方希望大家能夠指出，謝謝。\nJVM GC調整 Kafka 是用 Scala 語言撰寫的最後在 JVM 上運行，因此 JVM 參數的設定對於 Kafka 的重要性可想而知。\nBroker 所在節點的 CPU 資源還不錯的話，建議可以使用 CMS 垃圾回收方法。\nUseCurrentMarkSweepGC,另外 kafka runtime 是 java 8 的話可以使用 G1 垃圾回收方法，會有較好的 GC 效能。\n1 -XX:+UseCurrentMarkSweepGC 反之，CPU 資源匱乏請使用UseParallelGC。\n1 -XX:+UseParallelGC HEAP調整 預設 Kafka 啟動時只有用 1G 的 HEAP 對於大多數情境非常不友好．．．會造成吞吐量的問題。\n如果環境如果環境的記憶體空間許可，我建議可以設定到6G以上\n1 KAFKA_HEAP_OPTS=--Xms6g --Xmx6g Topic調整 log.retention.\u0026lt;hours|minutes|ms\u0026gt;\n基本上這個數值要依照kafka的單日傳輸量來決定，保存太久會引響io速度，以及硬碟的大小 message.max.bytes\nkafka預設的數值為1000012，我定睛一看連1MB都不到．．．大多數的 kafka 使用場景都會超過 1MB 的，這個需要根據自身使用情境做調整！ unclean.leader.election.enable\n每個版本預設的數值都不同，至於有什麼影響如果設置為 true 的話，當跑得慢的 partition 變成 leader ，那所有中間那一段資料就遺失了。設值為flase的話這些跑得慢的partition就不讓他來選幾囉！ leader 掛掉了 Kafka 會從 ISR 剩下的副本中選擇一個當 leader ，但如果 ISR 也沒有副本了， leader 就選不出來了。如果設置 unclean.leader.election.enable=true，則允許 Kafka 從那些不在 ISR 但依然存活的副本中選擇一個出來當 leader 。此時是有資料丟失的風險。 auto.leader.rebalance.enable\n堅決設定為flase沒什麼好說的，重新選舉的代價很大！ auto.create.topics.enable\n預設為 true ，需要調整為 false ，不然 cluster 會有一堆不知名的 topic ，可能你上 code 不小心打錯字\u0026hellip; topic 就建立了，滿浪費資源的。 OS 調整 Disk-Flush 根據爬了將近三週kafka官方說明以及許多關於kafka雜談，replicas 如何判定資料確實收到？ 只要資料被寫入 system 的 cache 就算了完成了（buffer / cache 差異請自行google） 系統會根據 LRU 定期地（預設是五秒鐘）將 cache 的資料寫入硬碟（stoarge device） 需要依照情況調整 LRU 定期寫入硬碟的時間 可能會有人說在這五秒鐘或是更長的時間資料還沒落到硬碟就死機了，那資料不就遺失了？？ 確實是遺失了，但一般來說副本會複製三份（一份掉了，會被同步回來）。 加長這個時間是避免io頻繁的操作，用效能換可用性 File-system 根據官方網站 XFS 的性能要強於 ext4 https://www.confluent.io/kafka-summit-sf18/kafka-on-zfs/ Swap 不建議關閉Swap（裸機情況） 之前用裸機跑的時候在可用的記憶體空間變少時會發生 oomkill 造成 brocker 直接死掉 在k8s上運行 沒什麼好說的 k8s 不給用swap FD Too many open files 的錯誤 在k8s上我沒有遇到這個錯誤，還沒去深究原因 在裸機的情況下，我會調整成ulimit -n 1000000 Producer 最多一次（at most once）：訊息在傳送的過程中可能會遺失，但是絕對不會重送。\n至少一次（at least once）：訊息在傳送的過程中不會遺失，但可能會重送。\n精确一次（exactly once）：訊息在傳送的過程中不會遺失，也不會重送。\n我認為精確性提交比較符合場景，以下我將針對exactly once模式說明與探討相關設定。\n設定 acks = all\n有副本 replicas 都要接收到消息，該訊息才算是commited。這是符合exactly once定義 ISR 中只有 1 replicas 時，acks=all 也就相當於 acks=1，注意ISR會變動。 當 replicas=3 時，ack=all 則需要 3 個副本都同步後才算commited，當有一個副本掛掉的時候，此時 ISR 會調整為 2 ，由於 ack=all 的設定這時候只需要 2 個副本同步後就認為“commited” 設定 retries 為一個較大的數值\n網路的瞬間抖動可能會導致訊息發送失敗，若是設定 retries \u0026gt; 0 的 Producer 能夠自動重新發送訊息。 設定 min.insync.replicas \u0026gt; 1\n除了 ISR 需要全部都寫入外，還要保證寫入的 ISR 數量要大於等於 min.insync.replicas ，注意ISR會變動。\n一旦指定 min.insync.replicas=xxx，不管什麼情況下 ISR 同步的數量最少都要跟設定xxx數量一樣，如果 ISR 當前數量小於 xxxx 那 commited 一定失敗。\nConsumer 確保訊息消費完成再提交。 Consumer 有個參數 enable.auto.commit，最好把它設置成 false，並採用手動提交位移的方式。\n手動提交又分成 sync 跟 async 兩種方法，需要這兩種方法結合使用才能確保 consumer 端不掉訊息，這裡採用一個小技巧。\npoll 資料後先使用 async 異步提交 commit 若有異常可以使用處理異常的方法，可以透過 Kafka 重送的機制 等 poll 所有資料做完可以使用 sync 同步提交 commit 確保本次 poll 的東西都有提交上去 另外本來要去嘗試更細粒度的提交方式，例如每次poll下來是1000筆資料，但我想每100筆去提交一次。\n看到Kafka有提供兩種方法去使用（我這裡沒去驗證，趕時間ＱＱ）。\ncommitSync(Map\u0026lt;TopicPartition, OffsetAndMetadata\u0026gt;) commitAsync(Map\u0026lt;TopicPartition, OffsetAndMetadata\u0026gt;) 這個也非常簡單，看 samplecode 應該是先建立一個 map 放入訊息從哪個 Partition 來目前處理到第幾個。\n假設目前處理到第100個，那就應該提交給 kafka ，以此達到較細粒度的提交。\n增加拉取時間 比如消費 100 筆資料我們要花費 400ms 的時間，那麼設定max.poll.interval.ms為300就不符合當前場景的需求。\n顯示的增加這個數值很直接的能解這個錯誤的產生，相對的 TPS 就會下降。\n減少 poll 消息數量 max.poll.records 是設定 cunsumer 向 kafka brocker 一次拉下來的量是多少，假設是1000筆資料，那我們在 max.poll.interval.ms 300 ms 單位時間內最多只能處理 800 筆資料，那這個數值設定為 1000 似乎也不符合我們的應用場景，酌量下降設定為 750 可能會是一個比較好的做法，但是也會造成 TPS 下降。\nkafka架設在私有機房經驗 注意本章節使用的單位\n另外本章節推估 kafka server 數量不是非常嚴謹，但符合我們的業務需求與場景。需要針對不同的場景做微調與優化並且小心服用。\n機房架設 kafka cluster switch 以及 baremetal 提供的頻寬為 1Gbps，我們業務的目標( SLA )是一個小時內處理 1TB 的資料，我們要設計多少台 kafka Server 來滿足這個業務需求呢？\n每台機器只有安裝 Kafka Server (排除固定會安裝的 ssh-server 等)，換句話說每台機器沒有混合部署其他服務。\n假設 Kafka 滿載的情況下會使用到 80 % 頻寬（為其他常駐的服務保留一點資源），另外過去的實驗經驗告訴我超過 80 % 的使用頻寬 kafka 就會有網路調包的可能，因此可以說單台 Kafka Server 最多大改能使用 800 Mb 的頻寬。（同時避免高峰直流量的情況作些微的保留）\n要記得 Kafka Server 之間有互相備援的機制，這大概會佔用掉 1/3 的頻寬（800Mb的3分之1），那我們可以計算實際上 Kafka 提供給使用者的頻寬大概是多少 ≈ 533Mbps。\n業務上 SLA 一個小時要求我們要處理 1TB 的資料，所以表示每秒的資料量為 10001000/(6060)≈2222Mb(注意！這邊有Byte換成bits)。\n計算需要多少台 Kafka Server ，每秒需要處理 2222Mb 的業務量一台 kafka 一秒頻寬最大只能使用 533 Mb ，用 2222 除與 533 就可以換換算出 4 台左右的 kafka server 。\nKafka 一般來說不太吃 CPU ，從過去監控的紀錄來看 CPU 很少吃滿常態的情況之下 CPU 只用到一半不到（producer 與 consumer 需要用同一個壓縮算法喔！）\n","description":"","id":39,"section":"posts","tags":["kafka","queue"],"title":"微紀錄Kafka參數調整","uri":"https://blog.jjmengze.website/zh-tw/posts/queue/kafka/kafka-tunning/"},{"content":" 當系統中一個元件有多個副本，這些副本同時要競爭成為領導人會透過許多不同的方法，例如zk(zookeeper), raft, redis\u0026hellip;等等，這些方法都有一個共同的特色誰先搶到誰就當領導人。\n那如果我的元件執行在 Kubernetes 叢集裡，有沒有什麼方式可以不依靠第三方系統讓我們也能做到分散式資源鎖呢？\n答案是有的，可以透過Kubernetes的ResourceLock來達成，ResourceLock 大致上可以分為endpoints,configmaps,leases三種。\n如果我們有手動搭建過HA(High Availability)環境的Kubernetes的話，會發現Controller Manger 與 Scheduler\ntodo\n這是因為 Controller Manger以及 scheduler 這兩個元件採用 Lease 又或是稱為 ResourceLock 的機制來從所有的 instance 會去競爭一個資源鎖，只有競爭到資源鎖的 instance 有能力去繼續動作，沒有競爭到的則是繼續等待資源鎖的釋放，只有持有資源鎖的 instance 發生故障沒有 renew 資源鎖，其他 instacne 才會繼續競爭資源鎖並接手原來的工作。\n底下會透過Kubernetes controller manger source code 來簡單的介紹 Kubernetes ResourceLock。\nresourcelock 在Controller Manger source code 中有使用到resourcelock.New，resourcelock.New使用到工廠模式在new的過程中會產出我們指定的resourcelock。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // Lock required for leader election rl, err := resourcelock.New( //在config會指定說要新增哪一種resource loack c.ComponentConfig.Generic.LeaderElection.ResourceLock, //在config會指定說resource lock要綁在哪一個namespace c.ComponentConfig.Generic.LeaderElection.ResourceNamespace, //在config會指定說resource lock的名稱 c.ComponentConfig.Generic.LeaderElection.ResourceName, //帶入建立configmap,endpoint client object c.LeaderElectionClient.CoreV1(), //帶入建立leases client object c.LeaderElectionClient.CoordinationV1(), //Resource lock 內容 resourcelock.ResourceLockConfig{ //Resource lock 內容 ID 目前會以hostname_uuid Identity: id, //EventRecorder 會紀錄 resource lock 事件 EventRecorder: c.EventRecorder, }) if err != nil { klog.Fatalf(\u0026#34;error creating lock: %v\u0026#34;, err) } configmaplock,endpointlock,leaselock,MultiLock這幾個需要實作以下這個interface。\n我們來簡單看一下這些是什麼功能\nGet\n從 ConfigMap、enpoint 或 lease 的 Annotation 拿到 LeaderElectionRecord 的資料。 Create\n建立 LeaderElectionRecord 的資料 到 ConfigMap、enpoint 或 lease 的 Annotation。 Update\n更新現有的 LeaderElectionRecord 資料到 ConfigMap、enpoint 或 lease 的 Annotation。 RecordEvent\n當加入 LeaderElectionRecord 觸發紀錄的event 。 Identity\n拿到 resource lock 的 id 。 Describe\n拿到 resource name/resource meta-data name 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type Interface interface { // Get returns the LeaderElectionRecord Get(ctx context.Context) (*LeaderElectionRecord, []byte, error) // Create attempts to create a LeaderElectionRecord Create(ctx context.Context, ler LeaderElectionRecord) error // Update will update and existing LeaderElectionRecord Update(ctx context.Context, ler LeaderElectionRecord) error // RecordEvent is used to record events RecordEvent(string) // Identity will return the locks Identity Identity() string // Describe is used to convert details on current resource lock // into a string Describe() string } configmap lock/lease lock 物件 這邊舉兩個例子ConfigmapLock與LeaseLock，xxxMeta主要紀錄namespace name以及resource name。\nClient 主要是用來操作 Kubernetes configmap / endpoint 的client api。\nLockConfig 主要操作 Kubernetes Lease 的 Client api。\ncm 操作 Kubernetes configmap 實際物件，不會透露給使用者。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type ConfigMapLock struct { // ConfigMapMeta should contain a Name and a Namespace of a // ConfigMapMeta object that the LeaderElector will attempt to lead. ConfigMapMeta metav1.ObjectMeta Client corev1client.ConfigMapsGetter LockConfig ResourceLockConfig cm *v1.ConfigMap } ... type LeaseLock struct { // LeaseMeta should contain a Name and a Namespace of a // LeaseMeta object that the LeaderElector will attempt to lead. LeaseMeta metav1.ObjectMeta Client coordinationv1client.LeasesGetter LockConfig ResourceLockConfig lease *coordinationv1.Lease } ... configmap lock/lease lock 工廠 kubernetes lock resource 透過工廠模式建立不同的物件\n一開始一次性建立EndpointsLock,ConfigMapLock,LeaseLock再由lockType去選擇要回傳什麼給使用者。 例如 type 輸入 endpoints 最後回傳給使用者的就是 endpointslock ，type 輸入 configmaps 最後回傳使用者 configmaplock 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // Manufacture will create a lock of a given type according to the input parameters func New(lockType string, ns string, name string, coreClient corev1.CoreV1Interface, coordinationClient coordinationv1.CoordinationV1Interface, rlc ResourceLockConfig) (Interface, error) { endpointsLock := \u0026amp;EndpointsLock{ EndpointsMeta: metav1.ObjectMeta{ Namespace: ns, Name: name, }, Client: coreClient, LockConfig: rlc, } configmapLock := \u0026amp;ConfigMapLock{ ConfigMapMeta: metav1.ObjectMeta{ Namespace: ns, Name: name, }, Client: coreClient, LockConfig: rlc, } leaseLock := \u0026amp;LeaseLock{ LeaseMeta: metav1.ObjectMeta{ Namespace: ns, Name: name, }, Client: coordinationClient, LockConfig: rlc, } switch lockType { case EndpointsResourceLock: return endpointsLock, nil case ConfigMapsResourceLock: return configmapLock, nil case LeasesResourceLock: return leaseLock, nil case EndpointsLeasesResourceLock: return \u0026amp;MultiLock{ Primary: endpointsLock, Secondary: leaseLock, }, nil case ConfigMapsLeasesResourceLock: return \u0026amp;MultiLock{ Primary: configmapLock, Secondary: leaseLock, }, nil default: return nil, fmt.Errorf(\u0026#34;Invalid lock-type %s\u0026#34;, lockType) } } resource lock 怎麼使用 在 kubernetes controller manger code 裡面使用了 leaderelection package 的 RunOrDie package 這個方法會在後面解析。\n這邊簡單說明一下怎麼使用 RunOrDie ，非常簡單。\nlock\n在上面定義的resource lock 會在這裡被使用，例如上面定義了，例如上面定義了 configmap resource lock 會在這裡被注入（可能會被create）。 leaseduration\n要佔領這個resource lock 多久 renewdeadline\n多久要刷新一次resource lock retryperiod\n多久要嘗試建立resource lock callback OnStartedLeading:\n當成功拿到resource lock 後要執行的動作 OnStoppedLeading:\n拿取resource lock 失敗後要執行的動作 watchdog\n健康檢查相關的Object name todo\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Try and become the leader and start cloud controller manager loops //使用 leaderelection package 裡的 RunOrDie function 會按照你設定的 LeaseDuration, RenewDeadline , RetryPeriod 三個時間嘗試成為領導人。 leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig{ Lock: rl, LeaseDuration: c.ComponentConfig.Generic.LeaderElection.LeaseDuration.Duration, RenewDeadline: c.ComponentConfig.Generic.LeaderElection.RenewDeadline.Duration, RetryPeriod: c.ComponentConfig.Generic.LeaderElection.RetryPeriod.Duration, Callbacks: leaderelection.LeaderCallbacks{ // 在 controller manger 裡面就是執行一個run 的function 當作controller 的進入點 // 也可以理解為controller manger 程式的進入點 OnStartedLeading: run, OnStoppedLeading: func() { klog.Fatalf(\u0026#34;leaderelection lost\u0026#34;) }, }, WatchDog: electionChecker, Name: \u0026#34;cloud-controller-manager\u0026#34;, }) RunOrDie背後的機制 對於使用者很方便的 RunOrDie，我們只要簡單的使用這個function 就可以定期的幫我們去競爭resource lock 勢必要去理解 RunOrDie 背後的機制。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // RunOrDie starts a client with the provided config or panics if the config // fails to validate. RunOrDie blocks until leader election loop is // stopped by ctx or it has stopped holding the leader lease func RunOrDie(ctx context.Context, lec LeaderElectionConfig) { //這裡很簡單的去new一個LeaderElector物件，主要是檢查使用者輸入的config有沒有問題。 le, err := NewLeaderElector(lec) if err != nil { panic(err) } //todo還不是很清楚watch dog的作用 if lec.WatchDog != nil { lec.WatchDog.SetLeaderElection(le) } //主要在這個run function 等等會來解密這個function 做了什麼事 le.Run(ctx) } LeaderElector run 解密 剛剛有提到 RunOrDie 背後的機制主要是透過 LeaderElector 物件底下的 run 方法去執行的，底下就開始講解 LeaderElector run做什麼吧！\n大致上主要是執行 建立以及續約 resourcelock ，當沒拿到 resourcelock 就對退出，拿到 resourcelock就執行續約。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Run starts the leader election loop. Run will not return // before leader election loop is stopped by ctx or it has // stopped holding the leader lease func (le *LeaderElector) Run(ctx context.Context) { //kubernetes 內置處理 crash 的方法本篇不討論。 defer runtime.HandleCrash() //當使用者觸發了 context cancel 時會呼叫 OnStoppedLeading 方法。 defer func() { le.config.Callbacks.OnStoppedLeading() }() //在這裡會試圖取得會是創建 resource lock,沒有拿到資源鎖的話就會卡在這個function裡面喔！ if !le.acquire(ctx) { return // ctx signalled done } ctx, cancel := context.WithCancel(ctx) defer cancel() //啟動一個 goroutine 去執行 controller manger 的進入點 run function go le.config.Callbacks.OnStartedLeading(ctx) //執行 resource lock 的續約動作。 le.renew(ctx) } LeaderElector acquire 這邊會簡單的說明一下獲取 resource lock 的流程，這裡主要各個process會嘗試向kubernetes 要 resource lock 如果不成功就再嘗試看看（嘗試到process死掉或是拿到resource lock為止）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // acquire loops calling tryAcquireOrRenew and returns true immediately when tryAcquireOrRenew succeeds. // Returns false if ctx signals done. func (le *LeaderElector) acquire(ctx context.Context) bool { ctx, cancel := context.WithCancel(ctx) defer cancel() succeeded := false desc := le.config.Lock.Describe() klog.Infof(\u0026#34;attempting to acquire leader lease %v...\u0026#34;, desc) // kubernetes封裝了一個類似timmer的function比較，細節不去探討。 // 我們只要知道他是一個定時觸發的function就好。 wait.JitterUntil(func() { //嘗試獲取 resource lock 或是續約resource lock succeeded = le.tryAcquireOrRenew(ctx) //不知道實際用途，追code感覺是有人定義Callbacks.OnNewLeader的話會callback告知 leader換了。 le.maybeReportTransition() // 獲取 resource lock 失敗，會等下次timmer到了再嘗試獲取租約。 if !succeeded { klog.V(4).Infof(\u0026#34;failed to acquire lease %v\u0026#34;, desc) return } //獲取租約成功紀錄事件 le.config.Lock.RecordEvent(\u0026#34;became leader\u0026#34;) //獲取租約成功紀錄事件 le.metrics.leaderOn(le.config.Name) //獲取租約成功紀錄事件 klog.Infof(\u0026#34;successfully acquired lease %v\u0026#34;, desc) //只要獲取成功後就不用再跑timmer，（這個Timmer用意就是讓沒有拿到resource lock的人可以定時去拿拿看） cancel() }, le.config.RetryPeriod, JitterFactor, true, ctx.Done()) return succeeded } LeaderElector renew LeaderElector renew 的行為可能我是說可能有一點點複雜，對其他大大們來說可能不會 xD。\n當通過一關 LeaderElector acquire 的考驗 process 已經拿到 resource lock 接下來只要不斷的更新租約就好了 ～\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 // renew loops calling tryAcquireOrRenew and returns immediately when tryAcquireOrRenew fails or ctx signals done. func (le *LeaderElector) renew(ctx context.Context) { ctx, cancel := context.WithCancel(ctx) defer cancel() // kubernetes封裝了一個類似timmer的function比較，細節不去探討。 // 可能有人會問跟wait.JitterUntil有什麼區別，這邊之後會另開戰場去討論（先挖洞給自己跳） wait.Until(func() { timeoutCtx, timeoutCancel := context.WithCancel(ctx) defer timeoutCancel() // PollImmediateUntil 會等 ConditionFunc 執行完畢 // 若是 ConditionFunc 執行失敗(是false)就會等 RetryPeriod 時間在執行一次。 // 如果 ConditionFunc 執行的結果是error或是true的話就不會重新嘗試 ConditionFunc 。 err := wait.PollImmediateUntil(le.config.RetryPeriod, func() (bool, error) { // ConditionFunc // 嘗試建立resource lock 或是更新resource lock 租約 return le.tryAcquireOrRenew(timeoutCtx), nil }, timeoutCtx.Done()) //不知道實際用途，追code感覺是有人定義Callbacks.OnNewLeader的話會callback告知 leader換了。 le.maybeReportTransition() // 拿到 resource lock 以configmaplock 為例 desc就會等於 configmaplock namespace/configmap name desc := le.config.Lock.Describe() // 這裡代表更新了租約成功就跳出去了，等待下一次timmer的呼叫。 if err == nil { klog.V(5).Infof(\u0026#34;successfully renewed lease %v\u0026#34;, desc) return } //租約更新失敗紀錄 le.config.Lock.RecordEvent(\u0026#34;stopped leading\u0026#34;) //租約更新失敗紀錄 le.metrics.leaderOff(le.config.Name) //租約更新失敗紀錄 klog.Infof(\u0026#34;failed to renew lease %v: %v\u0026#34;, desc, err) //再也不透過timmer定期更新租約 cancel() }, le.config.RetryPeriod, ctx.Done()) // if we hold the lease, give it up if le.config.ReleaseOnCancel { le.release() } } 小結 本篇非常簡單的帶過source code 了解了在有 kubernetes 的環境之下如何透過 kubernetes 已經有的資源完成一個分散式鎖，如果有任何疑問或內文有錯的地方歡迎提出跟我起討論！\n","description":"","id":40,"section":"posts","tags":["kubernetes"],"title":"kubernetes 分散式資源鎖","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/resourcelock/"},{"content":" 本篇文章的流程圖取自於KT Connnect 轻量级云原生测试环境治理工具並加以整理與新增實驗過程。\n越來越多的開發團隊基於 Kubernetes 部署公司的產品，一但服務上了 Kubernetes 當然會牽扯到基礎設計( Infra )、持續交付（ CD ）的過程，在如此龐大且複雜的架構與前提之下，我們有沒有什麼現有的專案可以幫助開發團隊快速的在 Kubernetes 上進行除錯。\n比如把線上（Remote）的流量導入本地(Local)，又會是反過說把本地測試的測試請求打到線上環境。\n痛點 在 Kubernetes 原生提供了一種方法（ port-forward ） ， 透過 port-forward 讓 local 端可以透過 : 存取線上的服務。\n我們應該知道 Kubernetes 原生提提供的 port-forward 能力不足，在微服務開發架構下，服務的調用除了調用方（ client ）依賴別人( service-A ) 以外，還有別人( service-B )依賴 調用方( client )。就目前 Kubernetes 原生提供的方法為基礎的話，勢必要將服務部署到一個測試環境中，但部署到測試環境又相當的麻煩，也有可能遇到各式不同的問題，例如：\n單向調用：只能是從 local 向 kubernetes 發起請求，kubernetes 上的其他服務無法將請求轉發到local。\n多個服務之間部署複雜：當微服務架構一大起來，部署與設定就會是一個問題需要 SRE 或是 DevOps 人員設置一個測試環境。\n程式碼修改問題：一但採用 port-forward 去導流 loacl 端進入到 kubernetes 的流量，那我們的程式碼本來透過 存取的方式必定要修改成 : 一但導流的東西多了，也意味著程式碼修改的地方多了更容易發生人為設定上的錯誤。\n解決方案 阿里巴巴（ alibaba ）這時腦洞大開開發了一個專案 kt-connect 我們先看看他們怎麼介紹自己的。\nManage and Integration with your Kubernetes dev environment more efficient.\n好吧看不是很懂xD，簡單來說阿里巴巴為了解決開發人員上述遇到的三個問題。\n開發人員可以將 kubernetes 上的流量轉到 local 進行測試。 開發人員可以將 local 的測試請求發送到 kubernetes 進行測試。 開發人員不需要更動過多的程式碼，可以沿用 的方式對服務發起請求。 大概看完了 kt-connect 帶來的好處接著就來安裝玩玩看！\n需要有 kubernetes 環境（開發環境具有 kubeconfig 部分權限 e.g. create list delete deploy 等） ssh 一顆熱愛的 debug 心 install kt-connect dependency package 使用 kc connection 之前 我們需要安裝一些依賴套件。\ninstall sshuttle #mac brew install sshuttle #linux pip install sshuttle #windows https://rdc-incubators.oss-cn-beijing.aliyuncs.com/stable/ktctl_windows_amd64.tar.gz install kt-connect 緊接著安裝 kt-connect 本體。\n#mac curl -OL https://rdc-incubators.oss-cn-beijing.aliyuncs.com/stable/ktctl_darwin_amd64.tar.gz tar -xzvf ktctl_darwin_amd64.tar.gz mv ktctl_darwin_amd64 /usr/local/bin/ktctl #linux curl -OL https://rdc-incubators.oss-cn-beijing.aliyuncs.com/stable/ktctl_linux_amd64.tar.gz tar -xzvf ktctl_linux_amd64.tar.gz mv ktctl_linux_amd64 /usr/local/bin/ktctl 安裝完後可以下 check 指令確認一下依賴包是否都安裝完成\nktctl check 1:01PM INF system info darwin-amd64 1:01PM INF checking ssh version OpenSSH_8.1p1, LibreSSL 2.7.3 1:01PM INF checking ssh version start at pid: 16888 1:01PM INF checking kubectl version Client Version: v1.18.2 Server Version: v1.17.1+6af3663 1:01PM INF checking kubectl version start at pid: 16890 1:01PM INF checking sshuttle version 1.0.4 1:01PM INF checking sshuttle version start at pid: 16891 1:01PM INF KT Connect is ready, enjoy it! connect kubernetes in localhost 先來測試看看從 localhost 與 remote 建立 tunnel ，記得一定要用 sudo 不然kt connection 無法操作系統的網路。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 sudo ktctl --namespace=default connect Password: 2:21PM INF Connect Start At 28422 2:21PM INF Client address 192.168.51.191 2:21PM INF deploy shadow deployment kt-connect-daemon-ojbky in namespace default 2:21PM INF pod label: kt=kt-connect-daemon-ojbky 2:21PM INF pod: kt-connect-daemon-ojbky-6484749d95-2zqnl is running,but not ready 2:21PM INF pod: kt-connect-daemon-ojbky-6484749d95-2zqnl is running,but not ready 2:21PM INF pod: kt-connect-daemon-ojbky-6484749d95-2zqnl is running,but not ready 2:21PM INF Shadow pod: kt-connect-daemon-ojbky-6484749d95-2zqnl is ready. 2:21PM INF Fail to get pod cidr from node.Spec.PODCIDR, try to get with pod sample Forwarding from 127.0.0.1:2222 -\u0026gt; 22 Forwarding from [::1]:2222 -\u0026gt; 22 2:21PM INF port-forward start at pid: 28424 Handling connection for 2222 Warning: Permanently added \u0026#39;[127.0.0.1]:2222\u0026#39; (ECDSA) to the list of known hosts. client: Connected. 2:21PM INF vpn(sshuttle) start at pid: 28428 2:21PM INF KT proxy start successful client: warning: closed channel 1 got cmd=TCP_STOP_SENDING len=0 server: warning: closed channel 1 got cmd=TCP_EOF len=0 當 connection 建立好後可以在 remote 到建立了一個 tunnel 的 pod ，這個 pod 專門用來跟 local端進行 vpn/socks5 的連接方式(預設vpn)\n1 2 3 4 kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE default kt-connect-daemon-ojbky-6484749d95-2zqnl 1/1 Running 0 60s ... 在 remote 環境準備一個 nginx deployment 進行測試\n1 2 3 4 kubectl create deploy nginx --image nginx deployment.apps/nginx created kubectl expose deploy nginx --port 80 service/nginx exposed 安裝完 nginx 之後可以透過下列指令檢視安裝成果\n1 2 3 4 5 6 7 8 kubectl get svc,deploy NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24m service/nginx ClusterIP 10.99.58.138 \u0026lt;none\u0026gt; 80/TCP 4m25s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/kt-connect-daemon-ojbky 1/1 1 1 2m48s deployment.apps/nginx 1/1 1 1 4m34s curl with nginx pod ip in localhost 我們先在本地 透過 pod ip 存取 nginx 服務 看看服務是否正常\n1 2 3 4 5 6 curl 10.32.0.5:80 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; ... curl with nginx service ip 接著在本地透過 nginx service virtual ip 存取 nginx 服務 看看服務是否正常\n1 2 3 4 5 6 curl 10.99.58.138:80 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; ... curl with nginx service DNS name 最後在本地透過 nginx cluster DNS 存取 nginx 服務 測試 服務是否正常。\n1 2 3 4 5 6 7 8 curl nginx.default.svc.cluster.local \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { ... 上述三個測試表示舞們可以在 localhost 存取遠端 kubernetes 上的 nginx 。\nexchange remote deployment traffic to local 如果我們想要把 remote 流量導到 localhost 來該怎麼做？\n可以透過ktctl exchang \u0026lt;deployment\u0026gt; --expose \u0026lt;remote port :local port\u0026gt;指令，將 remote 的 deployment 流量轉移到 localhost 來。\n以下範例我現在本地啟動一個 apache 服務，把原本進入kubernetes nginx 流量導流到 localhost 的 apache 上。\n在localhost開一個簡單的服務！\ndocker run -dit --name test-apache -p 80:80 httpd Unable to find image \u0026#39;httpd:latest\u0026#39; locally latest: Pulling from library/httpd bb79b6b2107f: Pull complete 26694ef5449a: Pull complete 7b85101950dd: Pull complete da919f2696f2: Pull complete 3ae86ea9f1b9: Pull complete Digest: sha256:b82fb56847fcbcca9f8f162a3232acb4a302af96b1b2af1c4c3ac45ef0c9b968 Status: Downloaded newer image for httpd:latest bbe6575b59c5a11d430da4c158894b7740fae8e520c72ac8a71de227fcb59e3d docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES bbe6575b59c5 httpd \u0026#34;httpd-foreground\u0026#34; 28 seconds ago Up 27 seconds 0.0.0.0:80-\u0026gt;80/tcp test-apache ##在本地測試一下！ curl 127.0.0.1:80 \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;It works!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; 原本架構 user==========\u0026gt;(remote) nginx\n導流後架構 user==========\u0026gt;(remote) nginx==(導流)==\u0026gt;(localhost)apache\n透過以下指令告訴遠端要透過 80 port 進入 nginx 的流量，需要導流到本地的 80 port。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 sudo ktctl exchange nginx --expose 80:80 2:37PM INF \u0026#39;KT Connect\u0026#39; not runing, you can only access local app from cluster 2:37PM INF Client address 192.168.51.191 2:37PM INF deploy shadow deployment nginx-kt-rjulr in namespace default 2:37PM INF pod label: kt=nginx-kt-rjulr 2:37PM INF pod: nginx-kt-rjulr-7b478cd45d-c2xw5 is running,but not ready 2:37PM INF pod: nginx-kt-rjulr-7b478cd45d-c2xw5 is running,but not ready 2:37PM INF pod: nginx-kt-rjulr-7b478cd45d-c2xw5 is running,but not ready 2:37PM INF Shadow pod: nginx-kt-rjulr-7b478cd45d-c2xw5 is ready. 2:37PM INF create exchange shadow nginx-kt-rjulr in namespace default 2:37PM INF scale deployment nginx to 0 2:37PM INF * nginx (0 replicas) success 2:37PM INF remote 10.32.0.4 forward to local 80:80 Forwarding from 127.0.0.1:2204 -\u0026gt; 22 Forwarding from [::1]:2204 -\u0026gt; 22 2:37PM INF exchange port forward to local start at pid: 30915 2:37PM INF redirect request from pod 10.32.0.4 22 to 127.0.0.1:2204 starting Handling connection for 2204 Warning: Permanently added \u0026#39;[127.0.0.1]:2204\u0026#39; (ECDSA) to the list of known hosts. 2:37PM INF ssh remote port-forward start at pid: 30917 curl with nginx service in remote 在 remote 環境測試請求 nginx service 能不能將 traffic 導流到 localhost 上。\n1 2 curl 10.99.58.138 \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;It works!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; curl with nginx DNS in remote 測試能不能在遠端透過 nginx dns name 把請求導流到 localhost 上。\n1 2 curl nginx.default.svc.cluster.local \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;It works!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Dashboard Kt connect 有提供一個dashboard可以使用，但目前不是很清楚這個dashboard可以提供怎麼樣的功能，就加減看一下囉！\n基本上按照官方的教學，先安裝RBAC 讓dashboard可以存取kubernetes上的資源。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 cat \u0026lt;\u0026lt;EOF | k apply -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: ktadmin rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - namespaces - nodes - nodes/proxy - services - endpoints - pods verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: - extensions resources: - ingresses verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - nonResourceURLs: [\u0026#34;/metrics\u0026#34;] verbs: [\u0026#34;get\u0026#34;] --- apiVersion: v1 kind: ServiceAccount metadata: name: ktadmin namespace: default --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: ktadmin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ktadmin subjects: - kind: ServiceAccount name: ktadmin namespace: default EOF clusterrole.rbac.authorization.k8s.io/ktadmin created serviceaccount/ktadmin created clusterrolebinding.rbac.authorization.k8s.io/ktadmin created 接著繼續按照官方文件安裝 dashboard 所需要的 deployment 與 service 即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: kt-dashboard spec: ports: - port: 80 targetPort: 80 selector: app: kt-dashboard type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: kt-dashboard name: kt-dashboard spec: replicas: 1 selector: matchLabels: app: kt-dashboard template: metadata: labels: app: kt-dashboard spec: serviceAccount: ktadmin containers: - image: registry.cn-shanghai.aliyuncs.com/kube-helm/kt-dashboard:stable imagePullPolicy: Always name: dashboard ports: - containerPort: 80 - image: registry.cn-shanghai.aliyuncs.com/kube-helm/kt-controller:stable imagePullPolicy: Always name: controller ports: - containerPort: 8000 EOF service/kt-dashboard created deployment.apps/kt-dashboard created RBAC 、 Service 與 Deployment 安裝好之後可以透過 kubectl 指令確認狀態與 kt connect dashboard 暴露出的 port 是哪個。\n1 2 3 4 5 6 7 8 9 10 kubectl get pod,svc NAME READY STATUS RESTARTS AGE pod/kt-connect-daemon-lucia-7b97887df4-d6lqx 1/1 Running 0 3m21s pod/kt-dashboard-68bbc66bc6-skcsp 2/2 Running 0 12m pod/nginx-f89759699-zjb6b 1/1 Running 0 9m39s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kt-dashboard NodePort 10.100.1.170 \u0026lt;none\u0026gt; 80:30080/TCP 13m service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 90m service/nginx ClusterIP 10.99.58.138 \u0026lt;none\u0026gt; 80/TCP 69m 我們可以透過 service 所告訴我們的 port 去存取這個服務，存取的圖是如下所示。\n","description":"","id":41,"section":"posts","tags":["kubernetes"],"title":"kubernetes ssh tunnel debug pratice","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/debug/kt-connection/"},{"content":" 前言 透過原理來了解事情的因果關係可能會太複雜，但作為一個軟體工程師理解背後如何實踐以及為什麼會有這樣的東西出現（歷史緣由）是非常重要的。\n本篇文章將會記錄 finalizers 的背後原理以及一些 source code ，這是使用者在操作 Kubernetes 常常會看到的一個欄位，好像有聽過但又不太了解的東西xD\n觀察細節 finalizers 定義於 Kubernetes 的 metadata的欄位\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ... // ObjectMeta is metadata that all persisted resources must have, which includes all objects // users must create. type ObjectMeta struct { ... // Populated by the system when a graceful deletion is requested. // Read-only. // More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata // +optional DeletionTimestamp *Time `json:\u0026#34;deletionTimestamp,omitempty\u0026#34; protobuf:\u0026#34;bytes,9,opt,name=deletionTimestamp\u0026#34;` // Number of seconds allowed for this object to gracefully terminate before // it will be removed from the system. Only set when deletionTimestamp is also set. // May only be shortened. // Read-only. // +optional DeletionGracePeriodSeconds *int64 `json:\u0026#34;deletionGracePeriodSeconds,omitempty\u0026#34; protobuf:\u0026#34;varint,10,opt,name=deletionGracePeriodSeconds\u0026#34;` // Must be empty before the object is deleted from the registry. Each entry // is an identifier for the responsible component that will remove the entry // from the list. If the deletionTimestamp of the object is non-nil, entries // in this list can only be removed. // Finalizers may be processed and removed in any order. Order is NOT enforced // because it introduces significant risk of stuck finalizers. // finalizers is a shared field, any actor with permission can reorder it. // If the finalizer list is processed in order, then this can lead to a situation // in which the component responsible for the first finalizer in the list is // waiting for a signal (field value, external system, or other) produced by a // component responsible for a finalizer later in the list, resulting in a deadlock. // Without enforced ordering finalizers are free to order amongst themselves and // are not vulnerable to ordering changes in the list. // +optional // +patchStrategy=merge Finalizers []string `json:\u0026#34;finalizers,omitempty\u0026#34; patchStrategy:\u0026#34;merge\u0026#34; protobuf:\u0026#34;bytes,14,rep,name=finalizers\u0026#34;` ... 從註解中我們大概可以了解這個欄位要表達的意義,我把它整理成比較容易閱讀的方式（可能只有我覺得godoc的 // 註解換行很煩xD\nMust be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any order. Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is a shared field, any actor with permission can reorder it. If the finalizer list is processed in order, then this can lead to a situation in which the component responsible for the first finalizer in the list is waiting for a signal (field value, external system, or other) produced by a component responsible for a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to order amongst themselves and are not vulnerable to ordering changes in the list.\n大膽假設 我認為上述的文字簡單來說有三個重點(以下的順序不重要)\nIf the deletionTimestamp of the object is non-nil, entries in this list can only be removed.\n簡單來說當 deletionTimestamp 不是 nil 的時候需要先刪除 Finalizers 內的條目 Finalizers may be processed and removed in any order\nFinalizers 條目的刪除順序不是固定的 Must be empty before the object is deleted from the registry.\n再刪除這個物件前 Finalizers 條目必須是空的 從上述註解的我們大概可以推測幾件事情，第一 當使用者刪除 Kubernetes 物件時，GC 回收機制需要檢查 Finalizers是否為空，第二其他物件可以任意刪除 Finalizers 欄位，前提是 deletionTimestamp 欄位不為 nil。\n大膽假設這個現象了，那就應該小心求證事實。\n小心求證 我已最近我在玩耍的 argo cd 進行球證對象，大部分處理物件的邏輯都會落在 controller 的 reconcile 階段裡，只要在 reconcile 搜尋 Finalizer 應該可以發現點什麼。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 func (ctrl *ApplicationController) processProjectQueueItem() (processNext bool) { if origProj.DeletionTimestamp != nil \u0026amp;\u0026amp; origProj.HasFinalizer() { if err := ctrl.finalizeProjectDeletion(origProj.DeepCopy()); err != nil { log.Warnf(\u0026#34;Failed to finalize project deletion: %v\u0026#34;, err) } } return ... } func (ctrl *ApplicationController) finalizeProjectDeletion(proj *appv1.AppProject) error { apps, err := ctrl.appLister.Applications(ctrl.namespace).List(labels.Everything()) if err != nil { return err } appsCount := 0 for i := range apps { if apps[i].Spec.GetProject() == proj.Name { appsCount++ break } } if appsCount == 0 { return ctrl.removeProjectFinalizer(proj) } else { log.Infof(\u0026#34;Cannot remove project \u0026#39;%s\u0026#39; finalizer as is referenced by %d applications\u0026#34;, proj.Name, appsCount) } return nil } func (ctrl *ApplicationController) removeProjectFinalizer(proj *appv1.AppProject) error { proj.RemoveFinalizer() var patch []byte patch, _ = json.Marshal(map[string]interface{}{ \u0026#34;metadata\u0026#34;: map[string]interface{}{ \u0026#34;finalizers\u0026#34;: proj.Finalizers, }, }) _, err := ctrl.applicationClientset.ArgoprojV1alpha1().AppProjects(ctrl.namespace).Patch(context.Background(), proj.Name, types.MergePatchType, patch, metav1.PatchOptions{}) return err } func (proj AppProject) HasFinalizer() bool { return getFinalizerIndex(proj.ObjectMeta, common.ResourcesFinalizerName) \u0026gt; -1 } // getFinalizerIndex returns finalizer index in the list of object finalizers or -1 if finalizer does not exist func getFinalizerIndex(meta metav1.ObjectMeta, name string) int { for i, finalizer := range meta.Finalizers { if finalizer == name { return i } } return -1 } func (proj *AppProject) RemoveFinalizer() { setFinalizer(\u0026amp;proj.ObjectMeta, common.ResourcesFinalizerName, false) } // setFinalizer adds or removes finalizer with the specified name func setFinalizer(meta *metav1.ObjectMeta, name string, exist bool) { index := getFinalizerIndex(*meta, name) if exist != (index \u0026gt; -1) { if index \u0026gt; -1 { meta.Finalizers[index] = meta.Finalizers[len(meta.Finalizers)-1] meta.Finalizers = meta.Finalizers[:len(meta.Finalizers)-1] } else { meta.Finalizers = append(meta.Finalizers, name) } } } 我把主要的判斷邏輯抓出來，可以看到大致上的邏輯有幾項\nprocessProjectQueueItem function 裡面會判斷 Application 物件的 DeletionTimestamp 以及 Finalizers 內有沒有我們要關注的 key。 檢查 Applications namespaces 下面的其他的相關物件，並且計算物件的總數。 如果\u0026gt;0 直接回傳，因為相關的資源還沒情理乾淨 如果=0 移除 Finalizers 我們要關注的 key。 這邊可以看到幾個之前提過的重點\n簡單來說當 deletionTimestamp 不是 nil 的時候需要先刪除 Finalizers 內的條目 Finalizers may be processed and removed in any order\nFinalizers 條目的刪除順序不是固定的 結語 類似像 Finalizers 這種的小螺絲都能起到這麼大的作用，我們應該更靜下心來學時事務的本質，不只要會操作它更要理解背後的原理是什麼。\n","description":"","id":42,"section":"posts","tags":["kubernetes"],"title":"Hey ! Kubernetes finalizers 不再煩惱","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/finalizers/"},{"content":" 由於協助自己公司內訓使用 helm 管理 Kubernetes Applicetion ，就非常的簡單的寫了怎麼看 Helm 的架構與 template 。\n目錄結構 可以先看一下這一個目錄結構\n. └── helm-example ├── Chart.yaml ├── charts ├── templates │ ├── NOTES.txt │ ├── _helpers.tpl │ ├── deployment.yaml │ ├── hpa.yaml │ ├── ingress.yaml │ ├── service.yaml │ ├── serviceaccount.yaml │ └── tests │ └── test-connection.yaml └── values.yaml 首先我們會看到 chart,yaml 這個檔案，裡面記錄著這個 chart 是第幾版，是一個library 還是一個 application ，部署的 application 是幾版，另外最重要的是 apiVersion 是在定義這個chart 是 helm v2 還是 v3 ，因為 helm v2 跟 v3 不相容這邊要特別注意！\n可來看一下chart.yaml 裡面寫了什麼。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: v2 name: helm-example description: A Helm chart for Kubernetes # A chart can be either an \u0026#39;application\u0026#39; or a \u0026#39;library\u0026#39; chart. # # Application charts are a collection of templates that can be packaged into versioned archives # to be deployed. # # Library charts provide useful utilities or functions for the chart developer. They\u0026#39;re included as # a dependency of application charts to inject those utilities and functions into the rendering # pipeline. Library charts do not define any templates and therefore cannot be deployed. type: application # This is the chart version. This version number should be incremented each time you make changes # to the chart and its templates, including the app version. # Versions are expected to follow Semantic Versioning (https://semver.org/) version: 0.1.0 # This is the version number of the application being deployed. This version number should be # incremented each time you make changes to the application. Versions are not expected to # follow Semantic Versioning. They should reflect the version the application is using. appVersion: 1.16.0 value value.yaml 這個檔案定義了這個 chart 會用到的變數，基本上是提供給 _helpers.tpl 、Deployment.yaml\u0026hellip;做使用的。\n我們來看一下value.yaml 裡面寫了些什麼\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 # Default values for helm-example. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: nginx pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: \u0026#34;\u0026#34; imagePullSecrets: [] nameOverride: \u0026#34;example-helm-playground\u0026#34; fullnameOverride: \u0026#34;\u0026#34; serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: \u0026#34;\u0026#34; podAnnotations: {} podSecurityContext: {} # fsGroup: 2000 securityContext: {} # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000 service: type: ClusterIP port: 80 ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \u0026#34;true\u0026#34; hosts: - host: chart-example.local paths: [] tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after \u0026#39;resources:\u0026#39;. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi autoscaling: enabled: false minReplicas: 1 maxReplicas: 100 targetCPUUtilizationPercentage: 80 # targetMemoryUtilizationPercentage: 80 nodeSelector: {} tolerations: [] affinity: {} 可以看到裡面定義了很多參數，這些參數基本上 基本上 基本上 基本上 會套用到剛剛說到的_helpers.tpl 、Deployment.yaml\u0026hellip;做使用的，有些沒寫好的在這邊就算在這邊寫說我Deployment要使用什麼參數也沒用xDD\ntemplates templates 底下會放 Kubernetes要使用到的資源，如Deployment、configmap、service等等，這些資源都是一個樣板。\n有一個比較特別的檔案是_helpers.tpl，裡面放了一些變數，可以讓整個template底下的物件共享這些變數。\n我們可以把文件點開來看裡面裡面是什麼一回事。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 {{/* {{/* Expand the name of the chart. */}} {{- define \u0026#34;helm-example.name\u0026#34; -}} {{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- end }} {{/* Create a default fully qualified app name. We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec). If release name contains chart name it will be used as a full name. */}} {{- define \u0026#34;helm-example.fullname\u0026#34; -}} {{- if .Values.fullnameOverride }} {{- .Values.fullnameOverride | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- else }} {{- $name := default .Chart.Name .Values.nameOverride }} {{- if contains $name .Release.Name }} {{- .Release.Name | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- else }} {{- printf \u0026#34;%s-%s\u0026#34; .Release.Name $name | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- end }} {{- end }} {{- end }} {{/* Create chart name and version as used by the chart label. */}} {{- define \u0026#34;helm-example.chart\u0026#34; -}} {{- printf \u0026#34;%s-%s\u0026#34; .Chart.Name .Chart.Version | replace \u0026#34;+\u0026#34; \u0026#34;_\u0026#34; | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- end }} {{/* Common labels */}} {{- define \u0026#34;helm-example.labels\u0026#34; -}} helm.sh/chart: {{ include \u0026#34;helm-example.chart\u0026#34; . }} {{ include \u0026#34;helm-example.selectorLabels\u0026#34; . }} {{- if .Chart.AppVersion }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} {{- end }} app.kubernetes.io/managed-by: {{ .Release.Service }} {{- end }} {{/* Selector labels */}} {{- define \u0026#34;helm-example.selectorLabels\u0026#34; -}} app.kubernetes.io/name: {{ include \u0026#34;helm-example.name\u0026#34; . }} app.kubernetes.io/instance: {{ .Release.Name }} {{- end }} {{/* Create the name of the service account to use */}} {{- define \u0026#34;helm-example.serviceAccountName\u0026#34; -}} {{- if .Values.serviceAccount.create }} {{- default (include \u0026#34;helm-example.fullname\u0026#34; .) .Values.serviceAccount.name }} {{- else }} {{- default \u0026#34;default\u0026#34; .Values.serviceAccount.name }} {{- end }} {{- end }} 簡單來說就是定義了好幾個全域參數，這邊我幫大家整理一下這幾個參數。\nlafite-chatbot.name\n條件是如果 value 的 nameOverride 有定義 就用 nameOverride 取代 .Chart.Name 如果長度超過63個字 後面的字都不要 如果最後一個字元有 把該字元修剪掉 helm-example.fullname\n如果 Values.fullnameOverride 有定義的話 .Values.fullnameOverride 如果長度超過63個字 後面的字都不要 如果最後一個字元有 把該字元修剪掉 $name 如果 Values.fullnameOverride 有定義的話 就用 nameOverride 取代 .Chart.Name 如果 .Release.Name 有包含 $name 的話 如果長度超過63個字 後面的字都不要 如果最後一個字元有 把該字元修剪掉 最後按照上面的規則印出 .Release.Name-$name 如果長度超過63個字 後面的字都不要 如果最後一個字元有 把該字元修剪掉 helm-example.chart\n印出 .Chart.Name-.Chart.Version 如果字元有遇到+ 轉換成_ 如果長度超過63個字 後面的字都不要 如果最後一個字元有 把該字元修剪掉 helm-example.labels\nhelm.sh/chart : 加上之前定義的 helm-example.chart 如果有定義 .Chart.AppVersion 那就加上 app.kubernetes.io/version : .Chart.AppVersion 的雙引號 app.kubernetes.io/managed-by: {{ .Release.Service }} `helm-example.selectorLabels\napp.kubernetes.io/name: 加上之前定義的 helm-example.name app.kubernetes.io/instance: 加上之前定義的 Release.Name helm-example.serviceAccountName\n如果.Values.serviceAccount.create 有定義的話 預設使用之前有定義的 helm-example.fullname 不然就是用 .Values.serviceAccount.name 預設使用default 不然就是 .Values.serviceAccount.name 了解了基礎的目錄結構後可以再深入地看一下元件的樣板\nDeployment 這邊遇到有用go template 的段落才會進行解說\n使用 _helpter.tpl 1 2 3 4 metadata: name: {{ include \u0026#34;helm-example.fullname\u0026#34; . }} labels: {{- include \u0026#34;helm-example.labels\u0026#34; . | nindent 4 }} 先看到 name 的部分 這邊他會直接套用 _helpter.tpl 內的 fullname\n再來看到 labels 的部分 這邊他會直接套用 _helpter.tpl 內的 labels 並且透過 nindent function 將每個開頭空四格\n使用 value 1 2 3 {{- if not .Values.autoscaling.enabled }} replicas: {{ .Values.replicaCount }} {{- end }} 接下來看到 replicas 的部分如果在 Values 沒有定義 autoscaling.enabled 就會採用 Values 內的 replicaCount。\n使用 go template function 1 2 3 selector: matchLabels: {{- include \u0026#34;helm-example.selectorLabels\u0026#34; . | nindent 6 }} 在這個部分 matchLabels 直接套用 _helpter.tpl 內的 selectorLabels 並且透過 nindent function 將每個開頭空四格。\n使用 go template to yaml 1 2 3 4 5 metadata: {{- with .Values.podAnnotations }} annotations: {{- toYaml . | nindent 8 }} {{- end }} 如果有在 value 寫 podAnnotations 這裡就會跑for each 把內容物把內容物，透過 nindent function 將每個開頭空四格。\n例如你在value內定義以下這個範例\n1 2 podAnnotations: prometheus.io/scrape: \u0026#34;true\u0026#34; Helm 會幫你轉換成以下格式\n1 2 3 4 template: metadata: annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; 只要了解上述這幾點並且搭配 go template 就可以做出非常多的變化，唯一缺點大概就是就是可讀性非常差吧。\nNote Note 是記錄這個 chart 是怎麼用的，直接來看範例。\n1 2 3 4 5 6 7 8 9 {{- if .Values.ingress.enabled }} 1. Get the application URL by running these commands: {{- if .Values.ingress.enabled }} {{- range $host := .Values.ingress.hosts }} {{- range .paths }} http{{ if $.Values.ingress.tls }}s{{ end }}://{{ $host.host }}{{ . }} {{- end }} {{- end }} ... 這邊如果我們有在 value 定義 ingress 定義 ingress.enabled 的話。\n就會用 for range 去遞迴取出 value 底下的 ingress 的所有 host\n並且用它組合成該 http:// \u0026hellip;\u0026hellip;的形式\n讓使用者知道這服務該怎麼存取。\n結語 go-template 非常不直觀在幫自己公司內訓的時候不太好 debug 找括弧看變數等，都不是容易維護。\n","description":"","id":43,"section":"posts","tags":["ci/cd"],"title":"Helm template 有講沒講差不多","uri":"https://blog.jjmengze.website/zh-tw/posts/cicd/helm/helmv3-gogo/"},{"content":" 記錄一下，在強迫使用TDD模式開發的情境與心得。\n比如我們需要一個下載器，這一個下載器會幫助我們從網路下載一些資料並且抽取出特定的資料。\n因應這個情境我使用TDD開發模式強迫自己使用測試驅動的思維思考。\n首先我們要需要一個下載器 Downloader ，所以我們會在 package 寫下 NewDownloader 的方法，接著讓 IDE 幫我補齊測試。\n以最小步驟開發 比如我在download package 先定義一個 NewDownloader 的方法，透過 IDE 指引幫我們建立他的測試。\n1 2 3 func NewDownloader() *Downloader { } IDE會幫我們建立 Test Function 如下圖所示。\n這時候我們會受到IDE的提示，告訴我們 Downloader 這個類型沒有定義。\n另外也告訴我們這裡有 TODO List 要我們加入一些 test cases 。\n我們先完成最少測試所需要的物件 Downloader ，透過IDE快速地建立所需要的物件。\n其實 IDE 也只是簡單幫你打幾個字而已，像是這是一個 interface 還是一個 struct 還是一個 function 還是其他物件這邊就需要自己定義。\n我們需要的是一個 struct ，為什麼？\n還記得我們前面定義的 function 是一個 NewDownloader 嗎？\n通常這邊會需要的是 struct，所以 code 應該會長這樣子。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type Downloader struct { } func TestNewDownloader(t *testing.T) { tests := []struct { name string want *Downloader }{ // TODO: Add test cases. } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := NewDownloader(); !reflect.DeepEqual(got, tt.want) { t.Errorf(\u0026#34;NewDownloader() = %v, want %v\u0026#34;, got, tt.want) } }) } } 還缺少什麼呢？\n剛剛有提到的 Test Case ，接著補滿我們的 Test Case 。\n這裡只要因為我們只是 New 一個 NewDownloader 出來而已所以 Test Case 很間單的建立一個物，這時候程式碼可能會長這樣子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func TestNewDownloader(t *testing.T) { tests := []struct { name string want *Downloader }{ { name: \u0026#34;really init\u0026#34;, want: \u0026amp;Downloader{}, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := NewDownloader(); !reflect.DeepEqual(got, tt.want) { t.Errorf(\u0026#34;NewDownloader() = %v, want %v\u0026#34;, got, tt.want) } }) } } 可以看到 IDE 還有紅色的字表示有問題\n於是我們到 download.go 去觀察有什麼問題。\n原來這邊告訴我們沒有定義 Download 的型別\n疑！奇怪，我們不是在 Test 已經定義過了嗎？\n是這樣的，在 test 定義過的 Download struct 是沒有辦法再不是 test 的 package 所使用的。\n所以我們需要把 Test 定義過的 Download struct 過移過來到這裡，看看還有什麼其他問題。\n搬移過來了， Download 型別已經沒有出現紅字了，這時候 IDE 告訴我們 NewDownloader 這個 function 沒有寫 return 回傳值。那我們就寫一個給他吧。\n於是我們的 code 現在變成這樣子\ndownload.go\n1 2 3 4 5 6 7 8 package download type Downloader struct { } func NewDownloader() *Downloader { return \u0026amp;Downloader{} } download_test.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package download import ( \u0026#34;reflect\u0026#34; \u0026#34;testing\u0026#34; ) func TestNewDownloader(t *testing.T) { tests := []struct { name string want *Downloader }{ { name: \u0026#34;really init\u0026#34;, want: \u0026amp;Downloader{}, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := NewDownloader(); !reflect.DeepEqual(got, tt.want) { t.Errorf(\u0026#34;NewDownloader() = %v, want %v\u0026#34;, got, tt.want) } }) } } 這時候跑test 應該會看到一個完美的 Test 成功\ngo test . ok kata-go-example/download 0.239s 持續迭代 雖然你可能會覺得很煩，為什麼要寫這些有的沒的，這些步驟 IDE 可以幫你大量處理，不過當我們習慣之後 IDE 會變成輔助你哪裡忘記寫，記住一個要點 小步驟的迭代。\n好的有 Downloader 物件之後我們沒有一個真的下載的方法，所以我們需要一個 Download 的方法，但是 因為很重要所以要說很多次 但是 但是 但是 但是 ，我們通常會希望這個 Download 的方法 不是寫死的，保留一些抽象空間可以替換。\n這邊我提供幾種方法\n第一種 在結構注入 function type 現定義一個最簡單的download function ，我們預期輸入一串網址，下載器會幫我處理，這串網址對應的IP位置，最簡單function 可能會是長這樣子。\n1 2 3 func (d *Downloader) Download(url string) string { } 定義完成之後透過 IDE 幫忙補齊 Test function\nIDE 建立完成的 Code 大概會長以下這樣子，也會期待你幫他補完test case 。\n到這裡不知道大家有沒有發現還是還沒實作 download 的 function， test code 看起來除了要我們加入一些 test case 之外好像就沒有別的錯誤了。\n這邊需要使用到一些小技巧例如 function type ，我們可以定義一個 Download 的function type 如下所示\n1 type DownloadFunc func(url string) string 定義了 DownloadFunc 我們可能會希望這個 Function 在 Downloader 的struct 裡面讓 Downloader 的結構擁有外部的傳進來的下載方法，如此一來就不會強依賴 Downloader 裡面寫什麼。\n既然我改了 NewDownloader 勢必會導致 NewDownloader 的測試方法不會通過\n這邊IDE就提示我們參數不正確，我們這邊就簡單地補上一個 Download function 就行，要訣修改到之前的 test code 需要先保證舊的 test code 是可以正確執行的！\n好，那就繼續回到 Download function 測試這個部分，我們在 struct 中新增了 DownloadFunc 就可以讓 Downloader 直接使用，code 可能會像是以下範例。\n1 2 3 func (d *Downloader) Download(url string) string { return d.downloadFunc(url) } 那麼 test code 會變成怎樣樣子呢？\n原本是長這樣子的test code\n但我們新增了在 struct 裡新增了 DownloadFunc 讓 Download 的邏輯被抽離了，這邊我們只要簡單的驗證一下 DownloadFunc 是否正確被使用即可，記得最後還需要補上test case 呦！\n最後我們的Download Function 的test code 應該會長這樣子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 func TestDownloader_Download(t *testing.T) { type args struct { url string } fakeWant := \u0026#34;this is fake function\u0026#34; tests := []struct { name string args args want string }{ { name: \u0026#34;Fake Download Func\u0026#34;, args: args{ \u0026#34;google.com\u0026#34;, }, want: fakeWant, }, } fakeDownloadFunc := func(url string) string { return fakeWant } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { d := NewDownloader(fakeDownloadFunc) if got := d.Download(tt.args.url); got != tt.want { t.Errorf(\u0026#34;Download() = %v, want %v\u0026#34;, got, tt.want) } }) } } 而我們 package 裡面的 code 應該會是長這樣子\n1 2 3 4 5 6 7 8 9 10 11 12 13 type DownloadFunc func(url string) string type Downloader struct { downloadFunc DownloadFunc } func NewDownloader(downloadFunc DownloadFunc) *Downloader { return \u0026amp;Downloader{downloadFunc} } func (d *Downloader) Download(url string) string { return d.downloadFunc(url) } 第二種 在方法注入 function type 第二種方法一樣是注入function type 不過跟第一種不一樣的地方在於我們這一次是在functnio 的參數注入。\n一樣定義一個 function type\n1 type DownloadFunc func(url string) string 這一次我們不在 Struct 裡面接收這個 function ，改用在functnion 裡面接收這個function ，例如說我們會定義一個 function 他要求攜帶一個 function 進來。\n1 2 3 func (d *Downloader) DownloadWithFunc(url string, downloadFunc DownloadFunc) string { } 這時候我們一樣用 IDE 幫我們撰寫 test code\n長出來的code 大概是會如下方圖片所示，一樣要求你加入一些test case 。\n這時候我們先來修正 package ，比如說我希望外面function 處理完成後之後我要加入一點東西，範例如下所示。\n1 2 3 func (d *Downloader) DownloadWithFunc(url string, downloadFunc DownloadFunc) string { return downloadFunc(url)+\u0026#34;WithFunc\u0026#34; } 改完了，那 test code 要怎麼寫？\n這時候我們跟 方法一 一樣依外部的function ，所以我們一樣要訂 fakeDownloadFunc。\n大致上的test code會像是下面範例的樣子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 func TestDownloader_Download(t *testing.T) { type args struct { url string } fakeWant := \u0026#34;this is fake function\u0026#34; tests := []struct { name string args args want string }{ { name: \u0026#34;Fake Download Func\u0026#34;, args: args{ \u0026#34;google.com\u0026#34;, }, want: fakeWant, }, } fakeDownloadFunc := func(url string) string { return fakeWant } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { d := NewDownloader(fakeDownloadFunc) if got := d.Download(tt.args.url); got != tt.want { t.Errorf(\u0026#34;Download() = %v, want %v\u0026#34;, got, tt.want) } }) } } func TestDownloader_DownloadWithFunc(t *testing.T) { type args struct { url string downloadFunc DownloadFunc } fakeWant := \u0026#34;this is fake function\u0026#34; fakeDownloadFunc := func(url string) string { return fakeWant } tests := []struct { name string args args want string }{ { name: \u0026#34;Fake Download Func\u0026#34;, args: args{ downloadFunc: fakeDownloadFunc, url: \u0026#34;google.com\u0026#34;, }, want: fakeWant + \u0026#34;WithFunc\u0026#34;, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { d := \u0026amp;Downloader{} if got := d.DownloadWithFunc(tt.args.url, tt.args.downloadFunc); got != tt.want { t.Errorf(\u0026#34;DownloadWithFunc() = %v, want %v\u0026#34;, got, tt.want) } }) } } ","description":"","id":44,"section":"posts","tags":["Go","TDD"],"title":"強迫使用 TDD 開發模式以Go為例-2","uri":"https://blog.jjmengze.website/zh-tw/posts/go/tdd-test2/"},{"content":" connection 特性 當發送一個 http2 請求的時候會建立一個 connection (TCP connection) 發送第二個請求的時候會延續使用第一個請求所建立的Connection 若是有多個 stream 的情況，每個 stream 是可以在同一個 tcp connection 併發傳輸，稱做為多路復用(multiplexing) stream 特性 一條 TCP 連線上可以有多個處於 Open 狀態的 stream（併發傳輸） Client 可以主動建立 stream (ID基數 1,3,5,7,9\u0026hellip;) Server 可以主動建立 stream (ID偶數 2,4,6,8,10\u0026hellip;) 任何一端都可以自主關閉 stream 同一條 stream 裡面的 frame 都是有序地排列 Client 向 Server 請求時 Stream ID 會為基數（1,3,5,7,9\u0026hellip;)\nServer 向 Client 主動推送 Stream ID 會為偶數（2,4,6,8,10\u0026hellip;)\nframe 特性 可以包含一到多個 Header (Header很小只要一個message ，到Header很大會被切成多個frame) 可以包含零到多個 Data! 想像一下 http1.1 時的 body 跟 header 都變成了 frame\n簡單的來說 body 也叫 frame , header 也叫 frame 只是裡面資料不同而已。\n到這裡你能比較 http1.1 與 http2 的不同嗎 ？ 在 http1.1 中 每一次請求都要重新建立一個 tcp connection 在 http1.1 中 每一次傳輸都要帶上重複的 header 在 http1.1 中 資料都是以純文本的方式傳輸 在 http1.1 中 server 無法主動推送資料給 client 在 http1.1 中 header 沒有進行壓縮 本篇文章還會再深入一點點解析 http2 stream 的相關知識\nstream status 可以觀察下圖 stream status 的狀態變化，所有的 stream 一開始會處於 idle 狀態，最終會結束於 closed 狀態。\n了解各個狀態監視如何切換的，需要知道 接收/發送 了什麼 flag，下方我列出了幾種會發送或是接收的 flag。\nHEADERS (H) PUSH_PROMISE (PP) END_STREAM (ES) RST_STREAM (RST) 這邊會簡單的說明一下幾種可能的狀況\nidle 狀態轉換\n當 stream 狀態處於 idle ， server 收到 head (h) flag 時後會把當前 stream 進行狀態的切換，從圖中可以我們從看到 idle 收 h flag 後切換成 open 狀態。 當 stream 狀態處於 idle 狀態，如果這時候 Server 想要推送消息給 Client 端，那Server 會發出 PUSH_PROMISE (PP) Flag 這時 Server 會處於 reserved 狀態，這時候就要準備發送後面的資料（Head+Data）。 Open 狀態轉換\n當 stream 狀態處於 open 狀態，這時我們如果發送（send）或是接收(recv) END_STREAM (ES) flag 的話，都會進入一個所謂的半關閉狀態(half closed)這時stream會等待接受flag進而轉變成close狀態。 reserved 狀態轉換\n當 stream 狀態處於 reserved 狀態，這時候Server 一但推送了第一筆資料的Head出去就會把狀態切換成半關閉狀態 (half closed) 等待接受flag進而轉變成close狀態。 half closed 狀態轉換\n當 stream 狀態處於 half closed 狀態，這時我們如果發送（send）或是接收(recv) END_STREAM (ES) 、RST_STREAM (RST) flag 的話，會把該 Stream 的狀態切換成Closed。 Data source:http2-in-action/\nstream 的priorty順序 由於 http2 大量的運用在前後與端溝通上，就算工程師只關心後端服務之間的溝通也需要知道，關於 http2 stream 的 priorty 的相關知識可以幫助我們在設計服務的時候善用已經存在的欄位。\n+---------------+ |Pad Length? (8)| +-+-------------+-----------------------------------------------+ |E| Stream Dependency? (31) | +-+-------------+-----------------------------------------------+ | Weight? (8) | +-+-------------+-----------------------------------------------+ | Header Block Fragment (*) ... +---------------------------------------------------------------+ | Padding (*) ... +---------------------------------------------------------------+ 從上圖我們可以看到 E 、 Stream Dependency? 、 Weight? 三個欄位，這三個欄位就表示了 http2 stream 的 priorty 的狀態。\n？表示是可以選的欄位\nStream Dependency : 表示依賴其他 Stream 當其他 Stream 完成才能換該 Stream Weight : 表示該 Stream 的權重，預設值為 16 ，那可以設定的範圍在 1~256。 wireshark 動手做看看 直接從 wireshark 抓取 http2 的封包來觀察 priorty，看看 http2 priorty ， Stream ID ， Frame 到底是怎麼一回事。\n下圖為 wireshark 抓取瀏覽器存取 https://www.nba.com/?37 的封包\nMagic SETTINGS WINDOW_UPDATE 可以看到第一個封包是 client (192.168.51.150) 送往 server (23.11.187.239) 封包的簡易描述是 Magic 類型的封包，同時包含了SETTINGS frame 與 WINDOW_UPDATE。\nMagic 點開magic stream 的話可以看到 client 發送了一個 PRI * HTTP/2.0\\r\\n\\r\\nSM\\r\\n\\r\\n SETTINGS 點開SETTINGS stream 的話可以看到 client 發送了好幾個設定值，告訴了Server 我要設定成這樣。 Header table size Max concurrent streams Max hader list size Unknown WINDOW_UPDATE 點開WINDOW_UPDATE stream 的話可以看到 client 發送了一個設定，告訴Server最大我的資料只能傳輸多少。 Headers 可以看到 client 發送了好幾個 HEADS 給SERVER並且Stream ID 都是奇數，這表示Client 發送給Server 的請求Stream ID都會是奇數。接下來 Cleint 又分別要了 Head[3] , Head[5] , Head[7] 那這些 Header有什麼關聯？\nstream ID priority 點開 Head[3] 可以看到這個請求的權重，那跟 Head[5] 有什麼差別\n點開 Head[5] 可以看到 ，觀察後可以看到 Head[3] 的priority 大於 Head[5]，權重計算後 Head[3] 要比 Head[5] 的資料更快被接收。\nstream ID dependency 除了 priority 之外還有 stream ID dependency 可以從上圖觀察到這兩個請求都要依賴 stream ID 0，表示都要先接收完成stream ID 0才可以接收各自的資料。\nData 可以看到 Server 回傳了資料給 Client 端，如圖所示 Client 要了 Head[1]的資料， Server 就回傳了六筆資料給 Client 。\n小結 重新複習了 http2 的特性，以前常提到得多路復用（multiplexing），Stream 以及 Frame 到底是什麼又有什麼特性，以及當 Client/Server 發送或是接收請求後 connection 會處於什麼狀態 做一個簡單的分析與介紹。\n最後從 wireshark 觀察 http2 請求的內容，觀察 Stream ID 與 Stream ID priority ，共用同一個 Connection 有些請求需要先被執行才能在執行其他請求的依賴關係，除此之外 沒有依賴關係的請求需要按照 priority 的大小 優先處理。\n","description":"","id":45,"section":"posts","tags":["http2"],"title":"Http2_stream","uri":"https://blog.jjmengze.website/zh-tw/posts/protocol/http2/http2_stream/"},{"content":" 先寫測試 假設今天我們要寫一下計算長方形週長的函數，那我們可能會定義一個 Perimeter 的 function ，這個 function 會預期攜帶兩個參數一個是長度 length 一個是寬度 width ，以及他們的型態是浮點數的型態 float64 。\n我們這測試函數可能是長這樣子\n測試長什麼樣子 Go 在撰寫測試的時候習慣使用以下這種風格( table-drive)，這種測試模式相當的方便。\n我會先寫出測試的目標所需點三個數值並且包裝成一個結構體。\n情境名稱 情境需要的參數 情境預期的答案 在情境所需要的參數部分，在計算週長的情境之下我們需要長寬，故我們也將包裝成一個結構體。\n所以本次測試的程式碼大概如下所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func TestPerimeter(t *testing.T) { type args struct { width float64 height float64 } tests := []struct { name string args args want float64 }{ { name: \u0026#34;rectangles\u0026#34;, args: args{ width: 4.0, height: 4.0, }, want: 16.00, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := Perimeter(tt.args.width, tt.args.height); got != tt.want { t.Errorf(\u0026#34;Perimeter() = %.2f, want %.2f\u0026#34;, got, tt.want) } }) } } 按照 TDD 的教戰手冊，就是撰寫最少的程式碼達到用測試推導出真正的函數要怎麼定義。\n這時候我們可跑一下測試看看，站在測試的角度我們真正的函數還缺少了什麼東西。\n1 2 3 4 go test # go-tdd-example/perimeter [go-tdd-example/perimeter.test] ./main_test.go:27:14: undefined: Perimeter FAIL go-tdd-example/perimeter [build failed] 可以看到在測試的過程中我們看到 Error ，這個 Error 是告訴我們 undefined: Perimeter ，原因是我們在 main package 裡面還沒有定義 Perimeter function ，那就來定義吧！\n補充main package func Perimeter(width float64, height float64) float64 { return 2*(width + height) } 這時後再執行一次 go test 看看結果。\n1 2 3 go test PASS ok go-tdd-example/perimeter 0.006s 重構！ 測試寫完了也正確執行，我們可以思考一下 Perimeter 這個函數所代表的意思，應該是計算週長吧？\n那會不會有其他開發者，把計算長方行週長的函數拿來計算三角形的週長，我想這是有可能的吧xD\n所以我們需要把計算長方形的周長函數進行封裝，希望這個函數只能計算長方形週長。\n那我們會先定義長方形的結構體，希望計算週長的函數 Perimeter 只接受長方形的結構體，並且計算他的週長。\n1 2 3 4 type Rectangle struct { Width float64 Height float64 } 測試長什麼樣子 按照 TDD 的教戰手冊，就是撰寫最少的程式碼達到用測試推導出真正的函數要怎麼定義，簡單的說就是讓編譯器幫助我們建構正確的程式碼。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func TestPerimeter(t *testing.T) { type args struct { Rectangle Rectangle } tests := []struct { name string args args want float64 }{ { name: \u0026#34;rectangles\u0026#34;, args: args{ \u0026amp;Rectangle{ Width: 4.0, Height: 4.0, }, }, want: 16.00, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := Perimeter(tt.args.Rectangle); got != tt.want { t.Errorf(\u0026#34;Perimeter() = %.2f, want %.2f\u0026#34;, got, tt.want) } }) } } 這時候我們可跑一下測試看看，站在測試的角度我們真正的函數還缺少了什麼東西。\n1 2 3 4 5 6 go test # go-tdd-example/perimeter [go-tdd-example/perimeter.test] ./main_test.go:28:23: not enough arguments in call to Perimeter have (Rectangle) want (float64, float64) FAIL go-tdd-example/perimeter [build failed] 可以看到在測試的過程中我們看到 Error ，這個 Error 是告訴我們\nnot enough arguments in call to Perimeter have (Rectangle) want (float64, float64) FAIL go-tdd-example/perimeter [build failed] 補充main package 原因是我們 Perimeter function ，定義的是 (width, height float64) 現在需要改成接收(rectangle Rectangle)。\n那我們的 Perimeter function 就會修改成以下形式。\n1 2 3 func Perimeter(rectangle Rectangle) float64 { return 2 * (rectangle.Width + rectangle.Height) } 這時後再執行一次 go test 看看結果。\n1 2 3 go test PASS ok go-tdd-example/perimeter 0.006s 不停的重構 做一個合格的工程師一直收到新的需求也是很正常的，今天PM告訴我客戶想要計算圓形的週長，對於工程師來說能盡量少改code就少改code xD，這時候遵循 TDD 的原則先寫 test !\n對於圓形來說，他有的結構體應該只有半徑，那我們先定義出他的結構體吧！\n1 2 3 type Cycle struct { Radius float64 } 測試的 code 可能會長這樣子，應算很直覺的 test code ，因為我們現在 Perimeter 的計算函數，需要計算長方形跟圓形。\n情境需要的參數自然就變成 Rectangle 加 Cycle 在 for 迴圈裡面 Perimeter function 有輸入 Rectangle 也有輸入 Cycle 看起來一切都很合理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 func TestPerimeter(t *testing.T) { type args struct { Rectangle Rectangle Cycle Cycle } tests := []struct { name string args args want float64 }{ { name: \u0026#34;rectangles\u0026#34;, args: args{ \u0026amp;Rectangle{ Width: 4.0, Height: 4.0, }, Cycle{ Radius: 16.0, }, }, want: 7.85, }, { name: \u0026#34;rectangles\u0026#34;, args: args{ \u0026amp;Cycle{ Radius: 1, }, }, want: 1.57, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := Perimeter(tt.args.Rectangle); got != tt.want { t.Errorf(\u0026#34;Perimeter() = %.2f, want %.2f\u0026#34;, got, tt.want) } if got := Perimeter(tt.args.Cycle); got != tt.want { t.Errorf(\u0026#34;Perimeter() = %.2f, want %.2f\u0026#34;, got, tt.want) } }) } } 這時候我們可跑一下測試看看，站在測試的角度我們真正的函數還缺少了什麼東西。\n1 2 3 4 go test # go-tdd-example/perimeter [go-tdd-example/perimeter.test] ./main_test.go:34:31: cannot use tt.args.Cycle (type Cycle) as type Rectangle in argument to Perimeter FAIL go-tdd-example/perimeter [build failed] 可以看到在測試的過程中我們看到 Error ，這個 Error 是告訴我們\ncannot use tt.args.Cycle (type Cycle) as type Rectangle in argument to Perimeter 這個意思大概是 Perimeter function 要求輸入一個 Rectangle 的類別，但是我們輸入 Cycle 類別。\n明明 長方形（Rectangle） 也要計算周長， 圓形（Cycle）也要計算週長啊！我不服氣！xD\n找到共同點 從上面的問題我們找到 長方形 跟 圓形 的共同點，都需要計算週長！那我們需要借助 go 的 interface 來定義共同的方法，在本篇的例子就是 計算週長 Perimeter function。\n1 2 3 type Shape interface { Perimeter() float64 } 現在測試變成怎麼樣了？ 還記得前面提到的測試結構嗎？我會先寫出測試的目標所需點三個數值並且包裝成一個結構體。\n情境名稱 情境需要的參數 情境預期的答案 這時候情境所需要的參數需要做變動，現在情境需要的是有計算週長能力的東西也就是interface Shape。\n簡單的說現在情需要的參數是一個有能力計算周長的物件，在這個情境下我會把物件拿去計算看是否能得到我預期的答案。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func TestPerimeter(t *testing.T) { type args struct { shape Shape } tests := []struct { name string args args want float64 }{ { name: \u0026#34;rectangles\u0026#34;, args: args{ Rectangle{ Width: 4.0, Height: 4.0, }, Cycle{ Radius: 1, }, }, want: 1.57, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := tt.args.shape.Perimeter(); got != tt.want { t.Errorf(\u0026#34;Perimeter() = %.2f, want %.2f\u0026#34;, got, tt.want) } }) } } 這時候我們可跑一下測試看看，站在測試的角度我們真正的函數還缺少了什麼東西。\n1 2 3 4 5 6 7 8 9 go test # go-tdd-example/perimeter [go-tdd-example/perimeter.test] ./main_test.go:17:15: cannot use \u0026amp;Rectangle literal (type *Rectangle) as type Shape in field value: *Rectangle does not implement Shape (missing Perimeter method) ./main_test.go:27:11: cannot use \u0026amp;Cycle literal (type *Cycle) as type Shape in field value: *Cycle does not implement Shape (missing Perimeter method) FAIL go-tdd-example/perimeter [build failed] 可以看到在測試的過程中我們看到 Error ，這個 Error 是告訴我們\n./main_test.go:17:15: cannot use \u0026amp;Rectangle literal (type *Rectangle) as type Shape in field value: *Rectangle does not implement Shape (missing Perimeter method) ./main_test.go:27:11: cannot use \u0026amp;Cycle literal (type *Cycle) as type Shape in field value: *Cycle does not implement Shape (missing Perimeter method) FAIL go-tdd-example/perimeter [build failed] 會看到這兩個錯誤，第一個錯誤是在說 *Rectangle 沒有實作 Shape interface ，因為他卻少了 Perimeter function 。\n第二個錯是在說 *Cycle 沒有實作 Shape interface ，因為他卻少了 Perimeter function 。\n補充main package 因為在測試的時候編譯器告訴我們 Rectangle 沒有實作 Shape interface ，因為他卻少了 Perimeter function ，那我們就識做一個吧，這時候我們需要用到 go 的function reciver 來幫 Rectangle 新增一個 Perimeter function。\n1 2 3 4 5 6 7 8 9 10 11 type Shape interface { Perimeter() float64 } func (r *Rectangle) Perimeter() float64 { return 2 * (r.Width + r.Height) } func (r *Cycle) Perimeter() float64 { return 3.14 * r.Radius / 2 } 這時後再執行一次 go test 看看結果。\n1 2 3 go test PASS ok go-tdd-example/perimeter 0.006s 我們成功了！\n小結 本篇是一個 TDD 基本的範例，不斷嘗試執行這些 test code 並讓編譯器 指引 我們找到正確的方案。\n用 Go 語言實作了一個週長的計算函數，在最初的需求我們只需要一個 長方形的 計算方法，非常簡單就可以實作出來。 透過 go test 提示我們 真正的函數長什麼樣子，藉著我們去補齊內部的邏輯，當補齊內部邏輯後再次執行 test code 直到測試通過為止。\n當有新的需求（圓形周長計算），我們透過 test code 與軟體工程的概念抽離周長計算方法，使用 go 的 interface 與 function reciver 協助我們透過介面的方式進行測試與重構。\nTDD 在現今的需求之後變得非常的重要，我們可以透過 test 完成重構讓程式碼變得可以測試與能夠預期城市的行為。\n","description":"","id":46,"section":"posts","tags":["Go","TDD"],"title":"強迫使用 TDD 開發模式以Go為例","uri":"https://blog.jjmengze.website/zh-tw/posts/go/tdd-test/"},{"content":" 如何確認網頁是否支援 http2 plugins chrome 瀏覽器可以透過安裝 HTTP/2 and SPDY indicator 這個 plugins ，來觀察目前正在瀏覽的網站是否支援 http2 。\n如果當前瀏覽頁面啟用了 HTTP2 則為藍色，如果啟用了 SPDY 則為綠色。\n可以看一下我最常瀏覽的網頁他是支援什麼樣的 protocal\n當我瀏覽 Kubernetes.io 的時候可以看到右上角是藍色的閃電表示支援 http2\n那今天什麼網頁支援 SPDY 呢？\n當我瀏覽 google 首頁的時候發現它支援 SPDY protocal\ndev mod 可以透過 chrome 的開發者模式觀察 請求是過過什麼樣的協定去進行傳輸的。\n當我們開啟 dev mod 的時候存取 github 的頁面可以觀察到有許多的請求是透過http2完成的，如下圖所示。\n從圖中我們可以觀察到 有http/1.1的請求也有 h2的請求，這兩種請求分別代表著前端網頁透過 http1.1 以及 基於 tls 的 http2 請求。\n蝦米 wireshark 抓不到 http2 的封包 抓不到 http2 封包 先來看一張圖，是我設定 wireshark filter 只顯示 http2 的封包，並且同時存取 kubernetes.io 。\n從上圖會發現\u0026hellip;.什麼資料都沒有，那怎麼會這樣子？\n怎麼解決 原因出在瀏覽器存取服務的時候需要透過基於 TLS 的 Http2 protocal ，所有的資料都被加密了我們不能看到 Client server 之間到底溝通了什麼，解決的方法其實很簡單只要在瀏覽器 Chrome 設定 SSLKEYLOGFILE 就可以順利的從 Chrome 抓到 http2 封包了。\n由於我的作業系統是 MACOS 這邊就示範 MACOS 怎麼在 Chrome 上啟動 SSLKEYLOGFILE。\n建立keylogfile.log 文件，更改權限。讓 chrome 啟動時能寫入攜帶sslkeylogfile mkdir ~/sslkeylogfile touch ~/sslkeylogfile/keylogfile.log sudo chmod 777 ~/sslkeylogfile/keylogfile.log 設定環境變數 export SSLKEYLOGFILE=~/sslkeylogfile/keylogfile.log echo \u0026#34;alias chrome=\\\u0026#34;open -a \u0026#39;Google Chrome\u0026#39;\\\u0026#34;\u0026#34; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc 如果要實驗 HTTP2 的話需要從 terminal 開啟 Chrome chrome 透過 terminal 啟動 chrome 的圖如下所示\n可以看一下 sslkeylogfile 的資料，裡面寫什麼不是很重要 wireshark 看得懂就好xDDD\n1 2 3 4 5 6 7 8 9 cat ~/sslkeylogfile/keylogfile.log CLIENT_HANDSHAKE_TRAFFIC_SECRET e379a0d9d454a5c80bf4d7b49dcbc6eb9055f07cda497306768b828b20380056 c0dc0b38f983aa5f1dd82f4b555c6be6195570622e90c956444938027e0dedc5 SERVER_HANDSHAKE_TRAFFIC_SECRET e379a0d9d454a5c80bf4d7b49dcbc6eb9055f07cda497306768b828b20380056 52ff297d7a0b1705bf6a876cbb307836ecc462455b9455d5eac606a1a6b1cac1 CLIENT_TRAFFIC_SECRET_0 e379a0d9d454a5c80bf4d7b49dcbc6eb9055f07cda497306768b828b20380056 ea654ba45d35cae4a0d81fa2eb3e795c930ea8e06caab92f4da992cba14c354b SERVER_TRAFFIC_SECRET_0 e379a0d9d454a5c80bf4d7b49dcbc6eb9055f07cda497306768b828b20380056 ef0d144892cf974cbb9e8d6ddebf5bd80370951cbaf5cc1223e8c63084e1da20 EXPORTER_SECRET e379a0d9d454a5c80bf4d7b49dcbc6eb9055f07cda497306768b828b20380056 d4e350adc0738aea83d6035fe0a21792fccd1a317208b6f9af547ffa36ba8195 CLIENT_HANDSHAKE_TRAFFIC_SECRET f95d9c5cd50060635969f3a88c89c548a063e1fa74c8cdb59d3af8d55a7e4558 c27c5c111347c718a3875a1300fa7a7dc88bbe4229189d7e0f0b887a0c9ac8a0 SERVER_HANDSHAKE_TRAFFIC_SECRET f95d9c5cd50060635969f3a88c89c548a063e1fa74c8cdb59d3af8d55a7e4558 507c5b0b8040cf85364dd272cf7d26eff2072baee3b2ed32206bddba85a88bc0 ... 設定wireshark 開啟 wireshark 後點選wireshark 選擇 Preferences 打開設定頁面，如下圖所示。\n接著選擇 Protocols -\u0026gt; TLS -\u0026gt; (Pre)-Master-Secret log filename 把~/sslkeylogfile/keylogfile.log 放進去。\n驗證 wireshark 抓取瀏覽器存取 http2 的封包 現在開啟 wireshark filter 輸入 http2 接著透過瀏覽器存取 github的網站，觀察 wireshark 是否能抓取到封包，如下圖所示。\n從圖中看起來都能夠順利地抓到封包，後續會針對 http2 的欄位進一步分析與複習相關的概念。\n","description":"","id":47,"section":"posts","tags":["http2"],"title":"怎麼透過 wireshark 抓取瀏覽器存取 Http2 的封包？","uri":"https://blog.jjmengze.website/zh-tw/posts/protocol/http2/http2_wireshark/"},{"content":" ref:kubernetes scheduler\n從上圖可以看到會經過幾個步驟分別是排成（scheduing）綁定(binding)，每個步驟又分成好幾個 Task 每一個 Task 由一到多個plugins組合而成。\n下面將簡單敘述各個 Task 的作用，後續章節講到plugins時會比較了解在做什麼。\n先是排成(scheduing)\nSort 用於對 scheduler queue 中的 Pod 進行排序。一次只能啟用一個 sort plugins 。 PreFilter 用來進行預先檢查 pod 的需要滿足的條件，若是處理失敗則離開調度週期，重新進入 scheduler queue 。 Filter 用來對節點進行過濾，只會留下滿足 pod 執行條件的節點。 PreScore 將 Filter 階段所產出的節點進行預評分工作，若是處理失敗則離開調度週期，重新進入 scheduler queue。 Score 將 Filter 階段所產出的節點進行評分 NormalizeScore 進行分數的正規化處理。 Reserve 這時Pod處於保留狀態，它將在綁定週期結束時觸發 失敗 : Unreserve plugins 成功 : PostBind插件。 Permit Pod的調度週期結束時，做最後的把關用來Permit、deny或是wait這次的調度。 最後進行綁定(binding)\nPreBind 在Pod真正綁定前設定相關的volume並將其安裝在目標節點，若是處理失敗則離開綁定週期，重新進入scheduler queue。 Bind 將Pod綁定到節點。 PostBind 成功綁定Pod後，用來清理相關的資源。 結語 接下來將會簡單介紹一下每個階段的 plugins 在做什麼 ，以便我們了解整個 framwork 的架構。\n","description":"","id":48,"section":"posts","tags":["kubernetes"],"title":"Kubernetes Scheduler context流水線略窺一二","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/scheduler/kubernetes-scheduler-context/"},{"content":" kube-scheduler 是Kubernetes的預設調度程式，每個新建立的Pod，kube-scheduler需要幫他選擇一個最佳的節點，讓節點上的Kubelet可以把Pod執行起來。\n但是，每個Pod對資源的要求都不同，因此，需要根據特定的調度演算法對現有的節點進行過濾。\nkube-scheduler通過兩步操作為Pod選擇節點：\n篩選 篩選步驟中，scheduler根據篩選規則，保留符合規則的節點，例如某些節點被打上taint的標記，Pod無法容忍這些被污染過(taint)的節點，scheduler根據篩選規則將合適的節點留下。 計分 計分步驟中，scheduler根據計分規則為每個對篩選完的節點打分數接著為每一個節點進行得分排名，藉以選擇最合適擺放的Pod節點。 如果存在多個得分相等的節點，則 scheduler 會隨機選擇其中之一。 了解上述的大方向後，後續的文章將source code來了解整個scheduler的架構與運作流程。\n","description":"","id":49,"section":"posts","tags":["kubernetes"],"title":"Kubernetes Scheduler 探討 source code 前的準備","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/source-code/scheduler/kubernetes-scheduler/"},{"content":" Openshift cluster monitoring operator 我才不告訴你勒 slide: https://hackmd.io/p/OonUQ9QKQ7-7JPBd1N9tOA?both\nWe have a collaborative session\nplease prepare laptop or smartphone to join!\nWho am I? Jason Li SRE/Backend developer \u0026#x2764;\u0026#xfe0f; kubernetes Go Rust \u0026#x1f431; lover 不斷的從入門到放棄 Agenda Background Related Work Method Conclusion Background Prometheus Operator, Prometheus, Prometheus Adapter, kube-state-metrics, \u0026hellip; e.t.c.\nIn order to manage such diverse components, a centralized management configuration file is required.\nRelated Work UI Prometheus Metrics Thanos UI Grafana Prometheus Prometheus Operator Prometheus-k8s\n\u0026#x1f44e; - Prometheus-user-workload Alertmanager Prometheus Operator Provide Kubernetes native deployment and management related monitoring components.\nautomate the configuration of a Prometheus based monitoring stack for Kubernetes clusters.\nPrometheus Alertmanager Related components Prometheus Operator(cont’d) Metrics node-exporter kube-state-metrics openshift-state-metrics \u0026#x1f44e; prometheus-adapter\n\u0026#x1f44e; Telemeter Client\n\u0026#x1f44e; configuration sharing\nnode-exporter Node exporter for hardware and OS metrics exposed by *NIX kernels. We can scrape, including a wide variety of system metrics further down in the output (prefixed with node_). 1 2 3 4 5 6 7 8 9 10 # HELP node_network_transmit_queue_length transmit_queue_length value of /sys/class/net/\u0026lt;iface\u0026gt;. # TYPE node_network_transmit_queue_length gauge node_network_transmit_queue_length{device=\u0026#34;br0\u0026#34;} 1000 node_network_transmit_queue_length{device=\u0026#34;eth0\u0026#34;} 1000 node_network_transmit_queue_length{device=\u0026#34;lo\u0026#34;} 1000 node_network_transmit_queue_length{device=\u0026#34;ovs-system\u0026#34;} 1000 node_network_transmit_queue_length{device=\u0026#34;tun0\u0026#34;} 1000 node_network_transmit_queue_length{device=\u0026#34;veth24377b8e\u0026#34;} 0 node_network_transmit_queue_length{device=\u0026#34;veth58bd788d\u0026#34;} 0 ... kube-state-metrics Focused on the health of the individual Kubernetes components, such as deployments, nodes and pods. Exposes raw data unmodified from the Kubernetes API Designed to be consumed either by Prometheus openshift-state-metrics Expands upon kube-state-metrics by adding metrics for OpenShift specific resources. Expose cluster-level metrics for OpenShift specific resources openshift-state-metrics (cont’d) BuildConfig Metrics Build Metrics DeploymentConfig Metrics ClusterResourceQuota Metrics Route Metrics Group Metrics ref: https://github.com/openshift/openshift-state-metrics\nThanos Thanos Thanos Querier Thanos Ruler Method Component Key Prometheus Operator prometheusOperator Prometheus prometheusK8s Alertmanager alertmanagerMain kube-state-metrics kubeStateMetrics openshift-state-metrics openshiftStateMetrics Grafana grafana Telemeter Client telemeterClient Prometheus Adapter k8sPrometheusAdapter Thanos Querier thanosQuerier Method (cont’d) Only Prometheus and Alertmanager have extensive configuration options. Other components usually provide only the nodeSelector field. Method (cont’d) move components to the node\n1 2 3 4 5 6 7 8 data: config.yaml: | prometheusOperator: nodeSelector: foo: bar prometheusK8s: nodeSelector: foo: bar persistent volume claim\n1 2 3 4 5 6 7 8 9 10 11 data: config.yaml: | prometheusK8s: volumeClaimTemplate: metadata: name: localpvc spec: storageClassName: local-storage resources: requests: storage: 40Gi custom Alertmanager configuration At this stage, cluster monitoring does not provide Alertmanager settings Conclusion \u0026#x1f4af; \u0026#x1f4aa; \u0026#x1f389;\nWrap up Self-updating monitoring stack that is based on Prometheus wider eco-system Provides monitoring of cluster components Expect to manage each component through the configuration file\u0026#x1f389; Thank you! \u0026#x1f411;","description":"","id":50,"section":"posts","tags":["openshift","talk"],"title":"Cluster Monitoring Operator","uri":"https://blog.jjmengze.website/zh-tw/posts/ocp/cluster-monitoring-operator/"},{"content":" 圖片來源\nKubernetes Cluster 中 API Server 是管理整個叢集的大腦， API-Server 一般執行在 Controller Plane （Master Node）上，維運人員通常會透過 CLI kubectl 工具去管理 Kubernetes。\n但還有許多方式可以存取 Kubernetes API Server ，今天會建立一個特殊場景。有一個名為 Jean 的使用者，透過 Rest API 的方式存取 Kubernetes 的資源\n向 Kubernetes API Server 發起請求通常需要有一個有權限的 ServiceAccount ，使用這個 ServiceAccount 通過 ClusterRole 、 Role 、 ClusterRoleBinding 、 RoleBinding 賦予操作相關資源的權限， 與 API Server 的請求基本上是基於 TLS ，所以 User 發起請求的時候還需要自帶憑證，當然我們也可以透過非安全方式存取 API Server ， 但是不推薦在環境中使用，這邊就不再多做說明。\ncreate user create private key 為了簡單起見，本篇將 X.509 client certificates 與 OpenSSL 結合使用建立User：\nCreate a private key for jean: 1 openssl genrsa -out jean.key 2048 create CSR 建立一個證書籤名請求（CSR）\nWithout Group\n1 2 3 openssl req -new -key jean.key \\ -out jean.csr \\ -subj \u0026#34;/CN=jean\u0026#34; singup CSR 與 Kubernetes CA 簽署 CSR ， Kubernetes CA 通常在 /etc/kubernetes/pki/ 間單的授權100天。\n1 2 3 4 5 openssl x509 -req -in jean.csr \\ -CA /etc/kubernetes/pki/ca.crt \\ -CAkey /etc/kubernetes/pki/ca.key \\ -CAcreateserial \\ -out jean.crt -days 100 檢查一下當前資料夾的檔案，應該會有三個檔案分別是 jean.crt 、 jean.csr 、 jean.key\n1 2 ls jean.crt jean.csr jean.key 做完以上的步驟就簡單的建立了一個名為 Jean 的 user ，我們後續可以使用 rest api 或是 CLI kubectl 去存取 Kubernetes 資源。\naccess kubernetes api server check API Server IP 我們需要先確認 Master 在哪裡，這邊有兩種方式都可以存取到 Kubernetes Master Node 的IP。\nmaster NIC IP 1 2 3 kubectl get node jason-test -o go-template --template=\u0026#39;{{ (index .status.addresses 0).address }}\u0026#39; 172.18.0.5 API Server Service IP 1 2 3 kubectl get svc kubernetes -o go-template --template=\u0026#39;{{.spec.clusterIP}}\u0026#39; 10.96.0.1 第一種就是直接確認 master Node 的 IP ，第二種是可以透過 Kubernetes Service 上的 VIP 。\naccess pods list 例如我們可以透過 curl 指令直接存取 Master Node IP 上的 API Server\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 curl https://172.18.0.5:6443/api/v1/namespaces/default/pods/ --cacert /etc/kubernetes/pki/ca.crt --cert ./jean.crt --key ./jean.key { \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { }, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;pods is forbidden: User \\\u0026#34;jean\\\u0026#34; cannot list resource \\\u0026#34;pods\\\u0026#34; in API group \\\u0026#34;\\\u0026#34; in the namespace \\\u0026#34;default\\\u0026#34;\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;pods\u0026#34; }, \u0026#34;code\u0026#34;: 403 } 但這裡會發現無法存取 Pod resource in \u0026quot;\u0026quot; group 這邊是因為沒有針對 jean 去設定 RBAC 的存取範圍。\n新增一個讀取 default namespace 的 role 1 2 3 4 5 6 7 8 9 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; indicates the core API group resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] 幫 Jean 綁定有能力讀取 Pod 的角色 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: rbac.authorization.k8s.io/v1 # This role binding allows \u0026#34;jean\u0026#34; to read pods in the \u0026#34;default\u0026#34; namespace. # You need to already have a Role named \u0026#34;pod-reader\u0026#34; in that namespace. kind: RoleBinding metadata: name: read-pods namespace: default subjects: # You can specify more than one \u0026#34;subject\u0026#34; - kind: User name: jane # \u0026#34;name\u0026#34; is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \u0026#34;roleRef\u0026#34; specifies the binding to a Role / ClusterRole kind: Role #this must be Role or ClusterRole name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io 完成以上綁定的動作之後 Jean 就有了讀取 default namespaces 的權限了\nreaccess kubernetes api server 一樣透過 curl 指令存取 Kubernetes api server ，最後透過 jq 幫忙整理輸出的內容。\n1 2 3 4 5 6 7 8 9 10 11 curl -s https://172.18.0.5:6443/api/v1/namespaces/default/pods/ --cacert /etc/kubernetes/pki/ca.crt --cert ./jean.crt --key ./jean.key | jq -r \u0026#39;.items[].metadata.name\u0026#39; greetings-run-1598345882182-pod-c7vbz h2c-client h2c-server-78bbb8f665-qz7kw hello-run-1598334732648-pod-ckvgk hello-run-1598342326047-pod-5hbj7 hello-run-1598342412495-pod-kbcsp hello-run-1598342606769-pod-xj6wd hello-run-1598344900394-pod-nc95n print-date-run-1598433322555-pod-m9lqf 小結 其實大多數的方法 例如透過 client-go 撰寫程式去存取 Kubernetes 資源又或是透過 CLI kubectl 去存取 Kubernetes 狀態，底層都是透過 Reset api ，學習怎麼使用 Reset api 去讀取資料也是一個很有用的方法\n像是 python client 如果沒有更新的話，我們可以使用 reset api 的方式取得當前 Kubernetes 資源近一步去修改。\n","description":"","id":51,"section":"posts","tags":["kubernetes"],"title":"使用reset api 存取 Kubernetes ","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/reset-api-server/"},{"content":" 簡單理解 Operator Lifecycle Manager - 2 (OLM) OpenShift Container Platform 4，使用了大量的 Operator 管理 Kubernetes Resource ，或許這跟 redhat 的推行Kubernetes政策有關吧 xD。\n但是這邊會延伸出蛋生雞雞生蛋的問題，那 Operator 要怎麼被管理呢？\nredhat 給出了一個解答那就是 Operator Lifecycle Manager 又稱 OLM，使用者透過 Declarative 的方式告訴 OLM ，要建立什麼樣子的 Operator 。\n:::info\n本篇文章的重點在於 OLM，是用來管理 Operator。\n朝著這方面去思考一且都會變得比較簡單！\n:::\n接續著上篇提到觀念本篇會示範如何在 cluster 中從 CatalogSources 下載一個 PackageManifest 並且安裝到環境。\nGUI 操作 可以從圖片看到在 administration 模式底下可以從左邊選取 Operators ，裡面有兩個分頁分別是 OperatorHub 以及 install Operators。\n我們可以透過 OCP Portal 直接下載想要的 Operator ，可以在右手邊看到有許多種類提供我們做選擇。\n這個部份留給有興趣的朋友，今天著重於 CLI 的操作。\nCRD - OperatorGroup 從 CatalogSources 下載一個 PackageManifest 之前我們要先建立一個project(namespaces)，讓操作都在這個 project 下。\n1 oc new-project playolm 建立完成 project 後我們還需要建立一個 OperatorGroup 確認之後安裝的 operator 只有操作這個 namespaces 的權限，對於 OperatorGroup 想要多瞭解一下可以參考olm book。\n1 2 3 4 5 6 7 8 9 10 11 cat \u0026lt;\u0026lt;EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: prometheus-operatorgroup namespace: playolm spec: targetNamespaces: - playolm EOF operatorgroup.operators.coreos.com/prometheus-operatorgroup created 透過指令確認一下剛剛部署上去的 operatorgroup\n1 2 3 oc get operatorgroup prometheus-operatorgroup NAME AGE prometheus-operatorgroup 114s CRD - Subscription 剛剛設定好operatorgroup，現在要到 CatalogSources 下載一個 PackageManifest 還記得上一個章節有拿一個 community-operators 的 prometheus-exporter-operator yaml 簡單的看一下裡面的內容嗎？\n這邊在執行一次看一下裡面的 yaml 檔\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 oc get packagemanifests prometheus-exporter-operator -o yaml apiVersion: packages.operators.coreos.com/v1 kind: PackageManifest metadata: creationTimestamp: \u0026#34;2020-06-20T07:23:23Z\u0026#34; labels: catalog: community-operators catalog-namespace: openshift-marketplace olm-visibility: hidden openshift-marketplace: \u0026#34;true\u0026#34; operatorframework.io/arch.amd64: supported operatorframework.io/os.linux: supported opsrc-datastore: \u0026#34;true\u0026#34; opsrc-owner-name: community-operators opsrc-owner-namespace: openshift-marketplace opsrc-provider: community provider: Red Hat provider-url: \u0026#34;\u0026#34; name: prometheus-exporter-operator namespace: default selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/prometheus-exporter-operator spec: {} status: catalogSource: community-operators catalogSourceDisplayName: Community Operators catalogSourceNamespace: openshift-marketplace catalogSourcePublisher: Red Hat channels: - currentCSV: prometheus-exporter-operator.v0.2.0 currentCSVDesc: annotations: alm-examples: |- [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;monitoring.3scale.net/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;PrometheusExporter\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;example-memcached\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;dbHost\u0026#34;: \u0026#34;your-memcached-host\u0026#34;, \u0026#34;dbPort\u0026#34;: 11211, \u0026#34;grafanaDashboard\u0026#34;: { \u0026#34;label\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;autodiscovery\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;enabled\u0026#34; } }, \u0026#34;type\u0026#34;: \u0026#34;memcached\u0026#34; } } ] capabilities: Deep Insights categories: Monitoring certified: \u0026#34;false\u0026#34; containerImage: quay.io/3scale/prometheus-exporter-operator:v0.2.0 createdAt: \u0026#34;2020-06-08 00:00:00\u0026#34; description: Operator to setup 3rd party prometheus exporters, with a collection of grafana dashboards repository: https://github.com/3scale/prometheus-exporter-operator support: Red Hat, Inc. apiservicedefinitions: {} customresourcedefinitions: owned: - description: Configures a prometheus exporter to monitor a memcached instance displayName: PrometheusExporter kind: PrometheusExporter name: prometheusexporters.monitoring.3scale.net version: v1alpha1 description: | A Kubernetes Operator based on the Operator SDK to centralize the setup of 3rd party prometheus exporters on **Kubernetes/OpenShift**, with a collection of grafana dashboards. You can setup different prometheus exporters to monitor the internals from different databases, or even any available cloudwatch metric from any AWS Service, by just providing a few parameters like **dbHost** or **dbPort** (operator manages the container image, port, argument, command, volumes... and also prometheus **ServiceMonitor** and **GrafanaDashboard** k8s objects). Current prometheus exporters types supported, managed by same prometheus-exporter-operator: * memcached * redis * mysql * postgresql * sphinx * es (elasticsearch) * cloudwatch The operator manages the lifecycle of the following objects: * Deployment (one per CR) * Service (one per CR) * ServiceMonitor (optional, one per CR) * GrafanaDashboard (optional, one per Namespace) ### Documentation Documentation can be found on our [website](https://github.com/3scale/prometheus-exporter-operator#documentation). ### Getting help If you encounter any issues while using operator, you can create an issue on our [website](https://github.com/3scale/prometheus-exporter-operator) for bugs, enhancements, or other requests. ### Contributing You can contribute by: * Raising any issues you find using Prometheus Exporter Operator * Fixing issues by opening [Pull Requests](https://github.com/3scale/prometheus-exporter-operator/pulls) * Submitting a patch or opening a PR * Improving [documentation](https://github.com/3scale/prometheus-exporter-operator) * Talking about Prometheus Exporter Operator All bugs, tasks or enhancements are tracked as [GitHub issues](https://github.com/3scale/prometheus-exporter-operator/issues). ### License Prometheus Exporter Operator is licensed under the [Apache 2.0 license](https://github.com/3scale/prometheus-exporter-operator/blob/master/LICENSE) displayName: Prometheus Exporter Operator installModes: - supported: true type: OwnNamespace - supported: true type: SingleNamespace - supported: false type: MultiNamespace - supported: true type: AllNamespaces provider: name: Red Hat version: 0.2.0 name: alpha defaultChannel: alpha packageName: prometheus-exporter-operator provider: name: Red Hat 裡面的內容很多看起來心煩意亂的，不過我們真正要關心的只有以下四點。\nname: prometheus-exporter-operator defaultChannel: alpha catalogSource: community-operators catalogSourceNamespace: openshift-marketplace 有這幾個參數就可以告訴olm說我們要用的是哪個catalogsources以及是哪一個packagemanifests。\n從defaultChannel 我們可以看到我們要用到是 alpha 版，alpha 對應到的是operator prometheus-exporter-operator.v0.2.0 版。\n進入這個 CRD Subscription 的重頭戲，宣告我要用哪一個catalogsources以及是哪一個packagemanifests。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat \u0026lt;\u0026lt; EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: prometheus-exporter-operator namespace: playolm spec: channel: alpha installPlanApproval: Manual name: prometheus-exporter-operator source: community-operators sourceNamespace: openshift-marketplace installPlanApproval: Manual EOF subscription.operators.coreos.com/prometheus-exporter-operator created 這時候可以透過指令看看這個project訂閱了哪些subscription。\n1 2 3 $ oc get subscription NAME PACKAGE SOURCE CHANNEL prometheus-exporter-operator prometheus-exporter-operator community-operators alpha 這邊的 subscription 可以把它當成 如果packagemanifests 有更新我們會收到更新的概念，說白了就是你訂閱youtuber有開啟小鈴鐺他就會通知你xD。\nCRD - Installplan 可以透過另外一個CRD看到目前 project 底下有訂閱（安裝？ 抱歉找不到一個適當的中文）哪些 Operator 他的版本是多少，現在安裝了嗎等資訊。\n1 2 3 oc get installplan NAME CSV APPROVAL APPROVED install-b5tmz prometheus-exporter-operator.v0.2.0 Manual false 從上署指令的結果來看這個 project 訂閱了prometheus-exporter-operator.v0.2.0 現在還沒有被 approved 所以還沒安裝到環境中。\n現在我們要手動上一個 patch 讓這個安裝包被 approved ，再來觀察環境的變化。\n1 2 oc patch installplan install-b5tmz --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/approved\u0026#34;, \u0026#34;value\u0026#34;:true}]\u0026#39; installplan.operators.coreos.com/install-b5tmz patched 再透過指令看現在prometheus-exporter-operator.v0.2.0是否被approved了。\n1 2 3 oc get installplan NAME CSV APPROVAL APPROVED install-b5tmz prometheus-exporter-operator.v0.2.0 Manual true CRD - ClusterServiceVersion (CSV) 可以透過 ClusterServiceVersion 觀察目前在 cluster 執行的 Operator 是哪一個版本，以及有哪些 operator 正在執行。\n:::warning\n這邊要注意，只有被approved了的installplan才會被clusterserviceversion紀錄。\n:::\n1 2 3 oc get clusterserviceversion NAME DISPLAY VERSION REPLACES PHASE prometheus-exporter-operator.v0.2.0 Prometheus Exporter Operator 0.2.0 Succeeded 看起來都有成功在執行，現在可以觀察 Kubernetes Cluster 裡面是否有安裝 prometheus 的相關元件。\n觀察prometheus CRD有沒有成功安裝\n1 2 3 4 oc get crd | grep prometheus prometheuses.monitoring.coreos.com 2020-06-20T07:24:12Z prometheusexporters.monitoring.3scale.net 2020-07-24T03:20:46Z prometheusrules.monitoring.coreos.com 2020-06-20T07:24:13Z 觀察 Prometheus CRD有沒有成功安裝\n1 2 3 4 oc get crd | grep prometheus prometheuses.monitoring.coreos.com 2020-06-20T07:24:12Z prometheusexporters.monitoring.3scale.net 2020-07-24T03:20:46Z prometheusrules.monitoring.coreos.com 2020-06-20T07:24:13Z 觀察 Prometheus ServiceAccount 有沒有成功安裝\n1 2 oc get sa | grep prometheus prometheus-exporter-operator 2 7m45s 觀察 Prometheus role rolebinding 有沒有成功安裝\n1 2 3 oc get role,rolebinding | grep prometheus role.rbac.authorization.k8s.io/prometheus-exporter-operator.v0.2.0-2nb8s 8m40s rolebinding.rbac.authorization.k8s.io/prometheus-exporter-operator.v0.2.0-2nb8s-prometheus-exporfkbqh 8m40s 最後看看 Prometheus Operator 有沒有成功安裝到 Kubernetes 上\noc get deployments NAME READY UP-TO-DATE AVAILABLE AGE prometheus-exporter-operator 1/1 1 1 9m38s 看起來一切都很正常，但魔鬼藏在細節裡是不是 prometheus-exporter-operator Deployment版本如我們預期的一樣\n1 2 oc get deployment prometheus-exporter-operator -o go-template --template \u0026#39;{{range .spec.template.spec.containers}} {{.image}}{{end}}\u0026#39; quay.io/3scale/prometheus-exporter-operator:v0.2.0 從上述輸出的結果來看版本跟 packagemanifests 所定義的是一樣的。\n結語 雖然感覺得出來 RedHat 立意良好，但是操作那麼多 CRD 情況真的有點麻煩，另外我在測試這幾個 CRD 的時候發現當刪除 ClusterServiceVersion 的時候，Operator的相關資源都會被刪除，感覺這個邏輯不怎麼正確。\n我個人覺得是不是 installplan 刪除或是 DISAPPROVED 才會刪除 ClusterServiceVersion ，使用者就算 誤刪 了也應該偵測到這件事情並重新生成一個 ClusterServiceVersion 才對。\n以上純屬個人見解，希望能與大家討論交流。\n","description":"","id":52,"section":"posts","tags":["openshift"],"title":"簡單理解 Operator Lifecycle Manager - 2 (OLM)","uri":"https://blog.jjmengze.website/zh-tw/posts/ocp/operator-lifecycle-manager-2/"},{"content":" 簡單理解 Operator Lifecycle Manager - 1 (OLM) OpenShift Container Platform 4，使用了大量的 Operator 管理 Kubernetes Resource ，或許這跟 redhat 的推行Kubernetes政策有關吧 xD。\n但是這邊會延伸出蛋生雞雞生蛋的問題，那 Operator 要怎麼被管理呢？\nredhat 給出了一個解答那就是 Operator Lifecycle Manager 又稱 OLM，使用者透過 Declarative 的方式告訴 OLM ，要建立什麼樣子的 Operator 。\n:::info\n本篇文章的重點在於 OLM，是用來管理 Operator。\n朝著這方面去思考一且都會變得比較簡單！\n:::\n我們先來看看 OLM 定義了哪些物件給我們使用\nOLM CRD Operator Lifecycle Manager 定義了六個自定義的資源(Custom Resource Definitions,CRD)分別是\nCatalogSource Subscription ClusterServiceVersion (CSV) PackageManifest InstallPlan OperatorGroup 如果手邊有 OpenShift 或是 OKD 環境的朋友可以試試看以下這個指令，可以看到環境中有這六個 CRD 存在。\n1 2 3 4 5 6 7 oc get crd | grep -E \u0026#39;catalogsource|subscription|clusterserviceversion|packagemanifest|installplan|operatorgroup\u0026#39; catalogsourceconfigs.operators.coreos.com 2020-06-20T06:57:36Z catalogsources.operators.coreos.com 2020-06-20T06:58:04Z clusterserviceversions.operators.coreos.com 2020-06-20T06:57:48Z installplans.operators.coreos.com 2020-06-20T06:57:53Z operatorgroups.operators.coreos.com 2020-06-20T06:58:12Z subscriptions.operators.coreos.com 2020-06-20T06:57:57Z 建立了 CRD 需要有相對應的 Controller 來觀察資源的變化，這六個 OLM CRD 被三個 Controller 所管理分別是 catalog-operator , olm-operator 以及packageserver，可以透過下面這個指令觀察到是否存在你的環境中。\n1 2 3 4 5 oc -n openshift-operator-lifecycle-manager get deploy NAME READY UP-TO-DATE AVAILABLE AGE catalog-operator 1/1 1 1 32d olm-operator 1/1 1 1 32d packageserver 2/2 2 2 32d CRD - CatalogSources 簡單的來說 CatalogSources 就是 Operator 的 Repo Source，收集了四種類型的 Operator。\n分別是\nCertified Operators\n在這個 repo 內的 Operator 都有經過 Redhat 的認證，相關的說明可以從Red Hat OpenShift Operator Certification搜尋到。 Community Operators\n在這個 repo 內的都是社群提供的，各式各樣的 Operator 可以從 Github找到。 Redhat-marketplace\n由 Refhat 與 IBM 共同合作組織給 Enterprise 用的 Repo，相關資料可以查詢 marketplace。 Redhat-operators\n這些 Operator 由 Red Hat 發行 我們可以透過指令去看環境上有沒有這幾項 catalogsources 。\n1 2 3 4 5 NAME DISPLAY TYPE PUBLISHER AGE certified-operators Certified Operators grpc Red Hat 32d community-operators Community Operators grpc Red Hat 32d redhat-marketplace Red Hat Marketplace grpc Red Hat 32d redhat-operators Red Hat Operators grpc Red Hat 32d 有了這些 Repo 後那怎麼查看 Repo 上的 Operator 那就要用到另外一個 CRD packagemanifests 。\nCRD - Packagemanifests 簡單的來說 packagemanifests 就記載了 Operator 部署的相關訊息\n我們可以透過指令觀察上面提到的 catalogsources 裡面有多少個 operator 。\nCommunity Operators 1 2 3 4 5 6 7 8 9 10 11 oc get packagemanifests -l catalog=certified-operators NAME CATALOG AGE cyberarmor-operator-certified Certified Operators 32d kong-offline-operator Certified Operators 32d ibm-spectrum-symphony-operator Certified Operators 32d portshift-operator Certified Operators 32d storageos-1tb Certified Operators 32d joget-openshift-operator Certified Operators 32d aqua-operator-certified Certified Operators 32d node-red-operator-certified Certified Operators 32d ... Community Operators oc get packagemanifests -l catalog=community-operators NAME CATALOG AGE kiali Community Operators 32d strimzi-kafka-operator Community Operators 32d planetscale Community Operators 32d nsm-operator-registry Community Operators 32d apicurito Community Operators 32d hazelcast-jet-operator Community Operators 32d ... Red Hat Marketplace 1 2 3 4 5 6 7 8 oc get packagemanifests -l catalog=redhat-marketplace NAME CATALOG AGE nxrm-operator-certified-rhmp Red Hat Marketplace 32d enterprise-operator-rhmp Red Hat Marketplace 32d storageos-rhmp Red Hat Marketplace 32d cortex-fabric-operator-rhmp Red Hat Marketplace 32d portshift-operator-rhmp Red Hat Marketplace 32d here-service-operator-certified-rhmp Red Hat Marketplace 32d Red Hat Operators 1 2 3 4 5 6 7 8 9 oc get packagemanifests -l catalog=redhat-operators NAME CATALOG AGE rhsso-operator Red Hat Operators 32d amq-broker-rhel8 Red Hat Operators 32d advanced-cluster-management Red Hat Operators 32d jaeger-product Red Hat Operators 32d fuse-online Red Hat Operators 32d eap Red Hat Operators 32d kubevirt-hyperconverged Red Hat Operators 32d 這邊拿一個 community-operators 的 prometheus-exporter-operator yaml 簡單的看一下裡面的內容。\n大致上描述了以下幾點\ncatalog channels\n主要描述了要管理的Operator的相關訊息，詳細內容會在下一篇做介紹 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 oc get packagemanifests prometheus-exporter-operator -o yaml apiVersion: packages.operators.coreos.com/v1 kind: PackageManifest metadata: creationTimestamp: \u0026#34;2020-06-20T07:23:23Z\u0026#34; labels: catalog: community-operators catalog-namespace: openshift-marketplace olm-visibility: hidden openshift-marketplace: \u0026#34;true\u0026#34; operatorframework.io/arch.amd64: supported operatorframework.io/os.linux: supported opsrc-datastore: \u0026#34;true\u0026#34; opsrc-owner-name: community-operators opsrc-owner-namespace: openshift-marketplace opsrc-provider: community provider: Red Hat provider-url: \u0026#34;\u0026#34; name: prometheus-exporter-operator namespace: default selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/prometheus-exporter-operator spec: {} status: catalogSource: community-operators catalogSourceDisplayName: Community Operators catalogSourceNamespace: openshift-marketplace catalogSourcePublisher: Red Hat channels: - currentCSV: prometheus-exporter-operator.v0.2.0 currentCSVDesc: annotations: alm-examples: |- [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;monitoring.3scale.net/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;PrometheusExporter\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;example-memcached\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;dbHost\u0026#34;: \u0026#34;your-memcached-host\u0026#34;, \u0026#34;dbPort\u0026#34;: 11211, \u0026#34;grafanaDashboard\u0026#34;: { \u0026#34;label\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;autodiscovery\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;enabled\u0026#34; } }, \u0026#34;type\u0026#34;: \u0026#34;memcached\u0026#34; } } ] capabilities: Deep Insights categories: Monitoring certified: \u0026#34;false\u0026#34; containerImage: quay.io/3scale/prometheus-exporter-operator:v0.2.0 createdAt: \u0026#34;2020-06-08 00:00:00\u0026#34; description: Operator to setup 3rd party prometheus exporters, with a collection of grafana dashboards repository: https://github.com/3scale/prometheus-exporter-operator support: Red Hat, Inc. apiservicedefinitions: {} customresourcedefinitions: owned: - description: Configures a prometheus exporter to monitor a memcached instance displayName: PrometheusExporter kind: PrometheusExporter name: prometheusexporters.monitoring.3scale.net version: v1alpha1 description: | A Kubernetes Operator based on the Operator SDK to centralize the setup of 3rd party prometheus exporters on **Kubernetes/OpenShift**, with a collection of grafana dashboards. You can setup different prometheus exporters to monitor the internals from different databases, or even any available cloudwatch metric from any AWS Service, by just providing a few parameters like **dbHost** or **dbPort** (operator manages the container image, port, argument, command, volumes... and also prometheus **ServiceMonitor** and **GrafanaDashboard** k8s objects). Current prometheus exporters types supported, managed by same prometheus-exporter-operator: * memcached * redis * mysql * postgresql * sphinx * es (elasticsearch) * cloudwatch The operator manages the lifecycle of the following objects: * Deployment (one per CR) * Service (one per CR) * ServiceMonitor (optional, one per CR) * GrafanaDashboard (optional, one per Namespace) ### Documentation Documentation can be found on our [website](https://github.com/3scale/prometheus-exporter-operator#documentation). ### Getting help If you encounter any issues while using operator, you can create an issue on our [website](https://github.com/3scale/prometheus-exporter-operator) for bugs, enhancements, or other requests. ### Contributing You can contribute by: * Raising any issues you find using Prometheus Exporter Operator * Fixing issues by opening [Pull Requests](https://github.com/3scale/prometheus-exporter-operator/pulls) * Submitting a patch or opening a PR * Improving [documentation](https://github.com/3scale/prometheus-exporter-operator) * Talking about Prometheus Exporter Operator All bugs, tasks or enhancements are tracked as [GitHub issues](https://github.com/3scale/prometheus-exporter-operator/issues). ### License Prometheus Exporter Operator is licensed under the [Apache 2.0 license](https://github.com/3scale/prometheus-exporter-operator/blob/master/LICENSE) displayName: Prometheus Exporter Operator installModes: - supported: true type: OwnNamespace - supported: true type: SingleNamespace - supported: false type: MultiNamespace - supported: true type: AllNamespaces provider: name: Red Hat version: 0.2.0 name: alpha defaultChannel: alpha packageName: prometheus-exporter-operator provider: name: Red Hat 結語 大致上能夠了解 OLM 是用來管理 Operator 的生命週期，加上透過六個自定義的資源( Custom Resource Definitions , CRD )以及三個 Controller去管理整個 Operator 的生態。\n使用者可以透過 CatalogSources 所提供的 Repo 下載各式各樣的 PackageManifest (Operator)。\n下一篇會針對 OLM 剩下的 CRD 進行間單的介紹，以及從 CatalogSources 下載一個 PackageManifest (Operator) 並且部署到環境中。\n","description":"","id":53,"section":"posts","tags":["openshift"],"title":"簡單理解 Operator Lifecycle Manager - 1 (OLM)","uri":"https://blog.jjmengze.website/zh-tw/posts/ocp/operator-lifecycle-manager-1/"},{"content":" 就想看看你對我做什麼壞壞的事 在操作 Kubernetes Cluster 的時候系統管理員會發給各個開發者、使用者又或是 Robot 帳號，這時好多人在操作 Cluster 有沒有什麼方法可以看看誰對 Cluster 做了什麼，到時候炸鍋的時候比較好找兇手（Ｘ）。\n好拉除了找兇手之外還可以除錯計費做很多我目前還想不到的功能，如果有大大看到這篇文章有想到 audit 還可以做什麼可以跟我分享與討論呦！\n我們先看看 Kubernetes 支援哪些 audit 處理的方法。\nLog backend, which writes events to a disk Webhook backend, which sends events to an external API Dynamic backend, which configures webhook backends through an AuditSink API object. 目前 Kubernetes 支援這三種方法處理 audit 資料，下文會三種方法都會使用一次作為範例，在看看實際的範例之前我們可以先來看看 audit 資料可以記錄哪些東西。\n發生了什麼事情（What 什麼時候發生的（When 誰讓事情發生了（Who 雖然還有紀錄其他可以的東西，不過我認為這三個最為重要，有興趣的朋友可以看看官方的文件。\n我們了解到 Kubernetes audit 會記錄什麼後，接著就是 audit 紀錄什麼時候會被觸發，這邊分為四個階段。\nRequestReceived\nevents generated as soon as the audit handler receives the request, and before it is delegated down the handler chain.（簡單來說就是 apiserver 收到請求的階段） ResponseStarted\nOnce the response headers are sent, but before the response body is sent. This stage is only generated for long-running requests (e.g. watch).(這個階段我不是非常了解，看起來像是 watch 之類的才會觸發這個階段) ResponseComplete\nThe response body has been completed and no more bytes will be sent.（這個階段代表 apiserver 的回應） Panic\nEvents generated when a panic occurred.(發生 panic 才會觸發) 目前我們知道 Kubernetes audit 會記錄事情發生的內容，事情發生的階段。那有可不可以分門別類，例如我只想要紀錄 pods 的事件、 configmaps 的事件 或是某個 user 的行為呢？\n答案是可以的！每當我看完 Kubernetes 的設計真的覺得社群考慮的相當的彈性，繼續項開源專案學習，只有站在巨人的肩膀上才能看得更遠。好廢話不多說，剛剛談到的分門別類在 audit 裡稱為 policy ， policy 分為以下四個級別。\nNone (不要記錄與此規則匹配的事件) Metadata (記錄請求的 metadata 例如 user timestamp resource verb e.t.c. 但不記錄請求的內容，有點類似只把 header 紀錄一下來 body 隨它去的感覺。) Request (記錄整個事件請求的內容， body 跟 header 都要存的意思) RequestResponse (記錄整個事件回應的內容， body 跟 header 都要存的意思) 官方有給出一個範例讓我們可以依照自己的需求進行修改，範例如下所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 apiVersion: audit.k8s.io/v1 # This is required. kind: Policy # Don\u0026#39;t generate audit events for all requests in RequestReceived stage. omitStages: - \u0026#34;RequestReceived\u0026#34; rules: # Log pod changes at RequestResponse level - level: RequestResponse resources: - group: \u0026#34;\u0026#34; # Resource \u0026#34;pods\u0026#34; doesn\u0026#39;t match requests to any subresource of pods, # which is consistent with the RBAC policy. resources: [\u0026#34;pods\u0026#34;] # Log \u0026#34;pods/log\u0026#34;, \u0026#34;pods/status\u0026#34; at Metadata level - level: Metadata resources: - group: \u0026#34;\u0026#34; resources: [\u0026#34;pods/log\u0026#34;, \u0026#34;pods/status\u0026#34;] # Don\u0026#39;t log requests to a configmap called \u0026#34;controller-leader\u0026#34; - level: None resources: - group: \u0026#34;\u0026#34; resources: [\u0026#34;configmaps\u0026#34;] resourceNames: [\u0026#34;controller-leader\u0026#34;] # Don\u0026#39;t log watch requests by the \u0026#34;system:kube-proxy\u0026#34; on endpoints or services - level: None users: [\u0026#34;system:kube-proxy\u0026#34;] verbs: [\u0026#34;watch\u0026#34;] resources: - group: \u0026#34;\u0026#34; # core API group resources: [\u0026#34;endpoints\u0026#34;, \u0026#34;services\u0026#34;] # Don\u0026#39;t log authenticated requests to certain non-resource URL paths. - level: None userGroups: [\u0026#34;system:authenticated\u0026#34;] nonResourceURLs: - \u0026#34;/api*\u0026#34; # Wildcard matching. - \u0026#34;/version\u0026#34; # Log the request body of configmap changes in kube-system. - level: Request resources: - group: \u0026#34;\u0026#34; # core API group resources: [\u0026#34;configmaps\u0026#34;] # This rule only applies to resources in the \u0026#34;kube-system\u0026#34; namespace. # The empty string \u0026#34;\u0026#34; can be used to select non-namespaced resources. namespaces: [\u0026#34;kube-system\u0026#34;] # Log configmap and secret changes in all other namespaces at the Metadata level. - level: Metadata resources: - group: \u0026#34;\u0026#34; # core API group resources: [\u0026#34;secrets\u0026#34;, \u0026#34;configmaps\u0026#34;] # Log all other resources in core and extensions at the Request level. - level: Request resources: - group: \u0026#34;\u0026#34; # core API group - group: \u0026#34;extensions\u0026#34; # Version of group should NOT be included. # A catch-all rule to log all other requests at the Metadata level. - level: Metadata # Long-running requests like watches that fall under this rule will not # generate an audit event in RequestReceived. omitStages: - \u0026#34;RequestReceived\u0026#34; 上面大致上瞭解了 Kuberetes audit 的大方向如，當 event 發生的時候他會記錄什麼，有哪幾個 stage 會觸發 event 以及 event 可以看照 policy 分門別類。\n了解這幾項東西之後就可以來進行實驗，我會透過 kubeadm 建立一個 all in one 的 kubernetes 測試 audit 紀錄在 disk 上以及 透過 webhook 的方式把 event 傳出來進行額外的處理。\nKubeadm 安裝測試環境 kubeadm 安裝方法這邊不多談 Google 有很多安裝的方法，在本次實驗需要設定 audit 的相關參數，kubeadm 目前還沒有完全支援 audit 的 feature gate 除了設定 kubeadm config 之外還需要手動設定部分參數。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration #featureGates: # not support DynamicAuditing # DynamicAuditing: true apiServer: extraArgs: audit-log-path: /home/ubuntu/audit.log audit-policy-file: /etc/kubernetes/addon/audit-policy.yaml # not support DynamicAuditing # runtime-config=auditregistration.k8s.io/v1alpha1: \u0026#34;true\u0026#34; # audit-dynamic-configuration: extraVolumes: - name: audit hostPath: /etc/kubernetes/addon/audit-policy.yaml mountPath: /etc/kubernetes/addon/audit-policy.yaml readOnly: true pathType: File - name: audit-log hostPath: /home/ubuntu mountPath: /home/ubuntu pathType: DirectoryOrCreate 1 2 3 4 5 6 cat \u0026lt;\u0026lt;EOF | \u0026gt;/etc/kubernetes/addon/audit-policy.yaml # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1 kind: Policy rules: - level: Metadata 可以透過上述這個 kubeadm config 去設定部分的 audit 參數，再由 kubeadm init 啟動的時候將它自動地帶入 kube-apiserver中。\n1 2 3 4 5 6 kubeadm init --config admin-apiserver.yml W0629 07:46:16.046453 86387 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [init] Using Kubernetes version: v1.18.5 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster ... 由於本次測試環境只有一個節點所以需要進行untaint的動作，讓 pod 可以在 master node 上開啟。\nkubectl taint node jason-test node-role.kubernetes.io/master- 當 kubeadm 執行完也設定完 CNI 後需要修改/etc/kubernetes/manifests/kube-apiserver 相關參數，讓 apiserver 支援 audit-dynamic-configuration 需要加入以下參數。\n1 2 3 - --audit-dynamic-configuration - --feature-gates=DynamicAuditing=true - --runtime-config=auditregistration.k8s.io/v1alpha1=true 修改完成後，可以透過 kubectl 指令檢查是否開啟 auditregistration.k8s.io/v1alpha1 的資源。\n1 2 kubectl api-resources | grep AuditSink auditsinks auditregistration.k8s.io false AuditSin 完成以上步驟後就完成了 kubernetes audit 的測試環境搭建。\nLog Backend 事實上剛剛的流程已經把 Backend 設定好了，位置就在 audit-log-path: /home/ubuntu/audit.log ，只要觸發 audit 就會把紀錄儲存在這個位置上。那我們來看看設定完 Kubernetes 之後會儲存哪寫資料。\n1 2 3 4 tail -f audit.log {\u0026#34;kind\u0026#34;:\u0026#34;Event\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;audit.k8s.io/v1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;Metadata\u0026#34;,\u0026#34;auditID\u0026#34;:\u0026#34;caa00b7a-e564-486c-837f-219eade633dd\u0026#34;,\u0026#34;stage\u0026#34;:\u0026#34;RequestReceived\u0026#34;,\u0026#34;requestURI\u0026#34;:\u0026#34;/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=10s\u0026#34;,\u0026#34;verb\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;user\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;system:kube-controller-manager\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;system:authenticated\u0026#34;]},\u0026#34;sourceIPs\u0026#34;:[\u0026#34;10.0.2.4\u0026#34;],\u0026#34;userAgent\u0026#34;:\u0026#34;kube-controller-manager/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election\u0026#34;,\u0026#34;objectRef\u0026#34;:{\u0026#34;resource\u0026#34;:\u0026#34;leases\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;kube-controller-manager\u0026#34;,\u0026#34;apiGroup\u0026#34;:\u0026#34;coordination.k8s.io\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;},\u0026#34;requestReceivedTimestamp\u0026#34;:\u0026#34;2020-07-04T15:19:21.083898Z\u0026#34;,\u0026#34;stageTimestamp\u0026#34;:\u0026#34;2020-07-04T15:19:21.083898Z\u0026#34;} {\u0026#34;kind\u0026#34;:\u0026#34;Event\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;audit.k8s.io/v1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;Metadata\u0026#34;,\u0026#34;auditID\u0026#34;:\u0026#34;caa00b7a-e564-486c-837f-219eade633dd\u0026#34;,\u0026#34;stage\u0026#34;:\u0026#34;ResponseComplete\u0026#34;,\u0026#34;requestURI\u0026#34;:\u0026#34;/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=10s\u0026#34;,\u0026#34;verb\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;user\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;system:kube-controller-manager\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;system:authenticated\u0026#34;]},\u0026#34;sourceIPs\u0026#34;:[\u0026#34;10.0.2.4\u0026#34;],\u0026#34;userAgent\u0026#34;:\u0026#34;kube-controller-manager/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election\u0026#34;,\u0026#34;objectRef\u0026#34;:{\u0026#34;resource\u0026#34;:\u0026#34;leases\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;kube-controller-manager\u0026#34;,\u0026#34;apiGroup\u0026#34;:\u0026#34;coordination.k8s.io\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;},\u0026#34;responseStatus\u0026#34;:{\u0026#34;metadata\u0026#34;:{},\u0026#34;code\u0026#34;:200},\u0026#34;requestReceivedTimestamp\u0026#34;:\u0026#34;2020-07-04T15:19:21.083898Z\u0026#34;,\u0026#34;stageTimestamp\u0026#34;:\u0026#34;2020-07-04T15:19:21.085030Z\u0026#34;,\u0026#34;annotations\u0026#34;:{\u0026#34;authorization.k8s.io/decision\u0026#34;:\u0026#34;allow\u0026#34;,\u0026#34;authorization.k8s.io/reason\u0026#34;:\u0026#34;RBAC: allowed by ClusterRoleBinding \\\u0026#34;system:kube-controller-manager\\\u0026#34; of ClusterRole \\\u0026#34;system:kube-controller-manager\\\u0026#34; to User \\\u0026#34;system:kube-controller-manager\\\u0026#34;\u0026#34;}} ... 這邊會看到許多觸發 audit 後記錄的訊息，這邊需要注意的地方有 requestURI 、 verb 、 user 、 sourceIPs 、 userAgent。\n我先執行一個簡單的 kubectl 指令來觀察 audit 有沒有攔截到這個請求。\n1 kubectl get pod 再去看 audit 所記錄的 log 有沒有儲存這個操作\ncat /home/ubuntu/audit/log | grep {\u0026#34;kind\u0026#34;:\u0026#34;Event\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;audit.k8s.io/v1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;Metadata\u0026#34;,\u0026#34;auditID\u0026#34;:\u0026#34;0595bb26-d8a2-4b59-b3d1-7d538c2e131f\u0026#34;,\u0026#34;stage\u0026#34;:\u0026#34;RequestReceived\u0026#34;,\u0026#34;requestURI\u0026#34;:\u0026#34;/api/v1/namespaces/default/pods?limit=500\u0026#34;,\u0026#34;verb\u0026#34;:\u0026#34;list\u0026#34;,\u0026#34;user\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;kubernetes-admin\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;system:masters\u0026#34;,\u0026#34;system:authenticated\u0026#34;]},\u0026#34;sourceIPs\u0026#34;:[\u0026#34;10.0.2.4\u0026#34;],\u0026#34;userAgent\u0026#34;:\u0026#34;kubectl/v1.18.5 (linux/amd64) kubernetes/e6503f8\u0026#34;,\u0026#34;objectRef\u0026#34;:{\u0026#34;resource\u0026#34;:\u0026#34;pods\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;},\u0026#34;requestReceivedTimestamp\u0026#34;:\u0026#34;2020-07-04T15:26:20.895927Z\u0026#34;,\u0026#34;stageTimestamp\u0026#34;:\u0026#34;2020-07-04T15:26:20.895927Z\u0026#34;} {\u0026#34;kind\u0026#34;:\u0026#34;Event\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;audit.k8s.io/v1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;Metadata\u0026#34;,\u0026#34;auditID\u0026#34;:\u0026#34;0595bb26-d8a2-4b59-b3d1-7d538c2e131f\u0026#34;,\u0026#34;stage\u0026#34;:\u0026#34;ResponseComplete\u0026#34;,\u0026#34;requestURI\u0026#34;:\u0026#34;/api/v1/namespaces/default/pods?limit=500\u0026#34;,\u0026#34;verb\u0026#34;:\u0026#34;list\u0026#34;,\u0026#34;user\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;kubernetes-admin\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;system:masters\u0026#34;,\u0026#34;system:authenticated\u0026#34;]},\u0026#34;sourceIPs\u0026#34;:[\u0026#34;10.0.2.4\u0026#34;],\u0026#34;userAgent\u0026#34;:\u0026#34;kubectl/v1.18.5 (linux/amd64) kubernetes/e6503f8\u0026#34;,\u0026#34;objectRef\u0026#34;:{\u0026#34;resource\u0026#34;:\u0026#34;pods\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;},\u0026#34;responseStatus\u0026#34;:{\u0026#34;metadata\u0026#34;:{},\u0026#34;code\u0026#34;:200},\u0026#34;requestReceivedTimestamp\u0026#34;:\u0026#34;2020-07-04T15:26:20.895927Z\u0026#34;,\u0026#34;stageTimestamp\u0026#34;:\u0026#34;2020-07-04T15:26:20.897187Z\u0026#34;,\u0026#34;annotations\u0026#34;:{\u0026#34;authorization.k8s.io/decision\u0026#34;:\u0026#34;allow\u0026#34;,\u0026#34;authorization.k8s.io/reason\u0026#34;:\u0026#34;\u0026#34;}} 整理 audit log 的資訊得到以下重點\n\u0026ldquo;requestURI\u0026rdquo;:\u0026quot;/api/v1/namespaces/default/pods?limit=500\u0026quot; \u0026ldquo;verb\u0026rdquo;:\u0026ldquo;list\u0026rdquo; \u0026ldquo;user\u0026rdquo;:{\u0026ldquo;username\u0026rdquo;:\u0026ldquo;kubernetes-admin\u0026rdquo;,\u0026ldquo;groups\u0026rdquo;:[\u0026ldquo;system:masters\u0026rdquo;,\u0026ldquo;system:authenticated\u0026rdquo;]} \u0026ldquo;sourceIPs\u0026rdquo;:[\u0026ldquo;10.0.2.4\u0026rdquo;] \u0026ldquo;userAgent\u0026rdquo;:\u0026ldquo;kubectl/v1.18.5 (linux/amd64) kubernetes/e6503f8\u0026rdquo; 可以清楚看到 kubernetes-admin 這個 user 執行了這個操作 ，操作是 /api/v1/namespaces/default/pods?limit=500 執行的動作是 list ，這個操作是來自 10.0.2.4 的 kubectl。\n看到以上這些重點之後，大致上可以推測初使用者在 10.0.2.4 透過 kubectl執行get pod --namespace default的動作。\nDynamic Backend 接著讓我們來看看 Dynamic Backend 是怎麼一回事，可以通過 AuditSink API 設定相對應的 Webhook 對資料進行預處理再送往其他地方。\n預設的 kubernetes 不會幫你設定 AuditSink API ，需要手動去開啟。在前面的章節已經有先開啟了，這邊我們可以回頭去看一下設定了哪些東西。\n\u0026ndash;audit-dynamic-configuration \u0026ndash;feature-gates=DynamicAuditing=true \u0026ndash;runtime-config=auditregistration.k8s.io/v1alpha1=true 這三個設定在 /etc/kubernetes/manifests/kube-apiserver.yaml 裡可以看到，還沒有修改的小夥伴可以現在加上去。\n確定完設定檔後我們可以透過 kubectl 去檢查這個資源是不是被開啟。\n1 2 kubectl api-resources | grep AuditSink auditsinks auditregistration.k8s.io false AuditSin 接著我們要去部署一個 webhook server 讓 kubernetes audit 可以把資料送上來，由於要建立 webhook 需要撰寫 code 以及需要設定 CA 這邊我提供實驗的 repo 給大家試試看。\n使用上非常簡單，只要透過 kubectl apply -f pod.yml -f service.yml這樣就把 webhook 建立好了。\n1 2 3 4 5 6 7 8 9 10 11 12 k apply -f pod.yml -f service.yml pod/webhook created service/admissionwebhook created ... k get pod,svc NAME READY STATUS RESTARTS AGE pod/webhook 1/1 Running 0 7s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/admissionwebhook ClusterIP 10.97.173.35 \u0026lt;none\u0026gt; 443/TCP 7s service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 2d14h 建立好 webhook pod 後我們需要建立 AuditSink 物件讓 audit 事件把資料往這個 webhook 送，\n1 2 kubectl apply -f auditSink.yml auditsink.auditregistration.k8s.io/mysink created 這一個 auditSink.yml 描述了，限定 audit 觸發的事件以及要往哪個地方送資料。\napiVersion: auditregistration.k8s.io/v1alpha1 kind: AuditSink metadata: name: mysink spec: policy: level: Metadata stages: - RequestReceived webhook: clientConfig: service: namespace: default name: admissionwebhook path: /sink port: 443 caBundle: \u0026#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2RENDQWFRQ0NRRHVXMXNYUVhqUjdEQU5CZ2txaGtpRzl3MEJBUXNGQURBZk1SMHdHd1lEVlFRRERCUkIKWkcxcGMzTnBiMjRnVjJWaWFHOXZheUJEUVRBZ0Z3MHlNREEzTURjd05UQTNOVEJhR0E4ek1ERTVNVEV3T0RBMQpNRGMxTUZvd0h6RWRNQnNHQTFVRUF3d1VRV1J0YVhOemFXOXVJRmRsWW1odmIyc2dRMEV3Z2dFaU1BMEdDU3FHClNJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUUMvWjZ1U1UrNlgrR3htN0NuYXR0RXBvUTN0VjJIb01TNGcKeCtGdEV4Q1Nqb0FjTk5wdHI1cjdTVjVIck1zS09wNDhrZFQ5NVI0YytWZUFnOGlqbVdpSEdQSXFyb1dIRys5cQpibzhoM0FZWk9MeWxQVjBUVWVSb0hQSkRmRmZsV054WlFnRHRXa2p0c2VRUVdsVisyVE5KVXF1cGtBMUMvZTJWCnVEckVkRzZTRFZaWDZMYTdTd3ViOXA4UnVaY21TMURrWlB3bFBmMEt2UVp5UHlMUXl4TVRjeVdJM3ZnNzlENWsKcTJPNFB0N080bm9MM0YzRGRyMHU3aGZwQjlJUUFUelZnTWRvQjNPRmV1NjRTZVRydmdmeG1XK1FZNFA0Z05aRQova21TTnNmaklnT202VnF1eEZHTEpsdUE3WlJoQkdadG1ON1N6dktTb21OZjlhTHJkWERSQWdNQkFBRXdEUVlKCktvWklodmNOQVFFTEJRQURnZ0VCQUtpMDBFRUZBdTFKL1M0ejBpVTVUYlJvOXI1WjRTZk5Ub3ZhS1U4bHA3YTAKaUUwWXRSVUdSMjhkRjRrRE12OXp4dTRCYy82N0ptb3g0SGtZMTFIU0RtOXVUUUs0T1dHMGo5MnIvOGY2RlRZMQpNSk1VNUJ0dy90Ym5hV2ZNWE5Xa0R5TnVhQXA1U3hMV1luODE4OWNqM0Zyd2NIL1VoODl4WFhDQnpzQUNOZGNiClRiN3ZKdnBGdWgyOXpNWUJGNTBPNHNnTi9UMWhNbTk3Y2xBbU9OZ1JoSTQ3cVdDamtlNlJKb2I0MnF6Ri8yK0gKK3k3eWlqQktkU2pQOVJOS3VreXM5VlY4eEN5TlpTSUFGYzM1dldMUDFLUEY1aFVTMWo1c0o2V3JncjdGVDU2ZQpZMC9rdGZkcWpSNXJVbWhzUW9GeHhJMzFHY1hjYktWTFNoRldtS2pMMERvPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\u0026#34; 這時候有觸發 audit 的事件就會往 webhook service送，可以透過 kubectl 指令去確認相關 log 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 k log -f webhook ... I0707 05:47:44.205612 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:4b6cb6dd-456b-4b77-9e63-90634a6d93eb Stage:RequestReceived RequestURI:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=10s Verb:update User:{Username:system:kube-controller-manager UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-controller-manager/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election ObjectRef:0xc000335980 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:42.212444 +0000 UTC StageTimestamp:2020-07-07 05:47:42.212444 +0000 UTC Annotations:map[]} I0707 05:47:44.206093 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:3d52b965-d65f-4060-b839-8e5ffe874b7f Stage:RequestReceived RequestURI:/api/v1/persistentvolumeclaims?allowWatchBookmarks=true\u0026amp;resourceVersion=492314\u0026amp;timeout=8m47s\u0026amp;timeoutSeconds=527\u0026amp;watch=true Verb:watch User:{Username:system:kube-scheduler UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-scheduler/v1.18.5 (linux/amd64) kubernetes/e6503f8/scheduler ObjectRef:0xc000335a80 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:43.363791 +0000 UTC StageTimestamp:2020-07-07 05:47:43.363791 +0000 UTC Annotations:map[]} I0707 05:47:44.206530 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:d1a20f9d-0204-43ae-8ab1-82b2c09858ed Stage:RequestReceived RequestURI:/apis/apps/v1/replicasets?allowWatchBookmarks=true\u0026amp;resourceVersion=492314\u0026amp;timeout=5m20s\u0026amp;timeoutSeconds=320\u0026amp;watch=true Verb:watch User:{Username:system:kube-controller-manager UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-controller-manager/v1.18.5 (linux/amd64) kubernetes/e6503f8/shared-informers ObjectRef:0xc000335b80 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:43.610833 +0000 UTC StageTimestamp:2020-07-07 05:47:43.610833 +0000 UTC Annotations:map[]} I0707 05:47:44.206956 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:5d48dc35-f089-4e76-a2f4-098e5628b055 Stage:RequestReceived RequestURI:/api/v1/namespaces/kube-system/endpoints/kube-scheduler?timeout=10s Verb:get User:{Username:system:kube-scheduler UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-scheduler/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election ObjectRef:0xc000335c80 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:44.024424 +0000 UTC StageTimestamp:2020-07-07 05:47:44.024424 +0000 UTC Annotations:map[]} I0707 05:47:44.207483 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:e2b5f015-d0e0-48ab-85a8-c9a42121cdfd Stage:RequestReceived RequestURI:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler?timeout=10s Verb:get User:{Username:system:kube-scheduler UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-scheduler/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election ObjectRef:0xc000335d80 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:44.026864 +0000 UTC StageTimestamp:2020-07-07 05:47:44.026864 +0000 UTC Annotations:map[]} I0707 05:47:44.207995 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:0070ecce-3119-4e44-850d-5914e45dcde6 Stage:RequestReceived RequestURI:/api/v1/namespaces/kube-system/endpoints/kube-scheduler?timeout=10s Verb:update User:{Username:system:kube-scheduler UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-scheduler/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election ObjectRef:0xc000335e80 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:44.028928 +0000 UTC StageTimestamp:2020-07-07 05:47:44.028928 +0000 UTC Annotations:map[]} I0707 05:47:44.208582 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:ed450b92-27cc-449d-95cf-c2822fd0e380 Stage:RequestReceived RequestURI:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler?timeout=10s Verb:get User:{Username:system:kube-scheduler UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-scheduler/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election ObjectRef:0xc000335f80 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:44.032618 +0000 UTC StageTimestamp:2020-07-07 05:47:44.032618 +0000 UTC Annotations:map[]} I0707 05:47:44.209296 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:9deb9b72-4a27-4a36-b6f8-74e976aee3cf Stage:RequestReceived RequestURI:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler?timeout=10s Verb:update User:{Username:system:kube-scheduler UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-scheduler/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election ObjectRef:0xc000126080 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:44.034645 +0000 UTC StageTimestamp:2020-07-07 05:47:44.034645 +0000 UTC Annotations:map[]} [GIN] 2020/07/07 - 05:47:44 | 200 | 105.108µs | 10.32.0.1 | POST \u0026#34;/sink?timeout=30s\u0026#34; [GIN] 2020/07/07 - 05:47:44 | 200 | 97.207µs | 10.32.0.1 | POST \u0026#34;/sink?timeout=30s\u0026#34; [GIN] 2020/07/07 - 05:47:44 | 200 | 150.111µs | 10.32.0.1 | POST \u0026#34;/sink?timeout=30s\u0026#34; ... 可以看到 webhook 的 log 已經收到許多 audit 事件的資料，我們可以去修改 webhook 的程式碼讓 webhook 收到資料後進行預處理後再往其他地方發送例如：做Alert 。\n後記 目前大多數看到的解決方案是直接採用 Log Backend ，把資料直接記載在 master node 上再透過 logstash or fluentd 直接把資料過濾並且丟到 Elasticsearch ，可以利用 audit 紀錄使用者行為看到 cluster 內發生什麼事情。\n","description":"","id":54,"section":"posts","tags":["kubernetes"],"title":"kubernetes Audit 查起來","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/audit/"},{"content":" 前面說了一些基礎概念，本篇文章就實際來動手做做看用 OpenSSL 、 Nginx 加上 GO 開發一個簡單的 TLS 範例。\n建立 CA key 由於我們要自己簽 SSL 所以需要透過 OpenSSL 先建立一個 CA 的私鑰。\n1 2 3 4 5 6 7 8 openssl genrsa -out ca.key 2048 Generating RSA private key, 2048 bit long modulus (2 primes) ........+++++ ......+++++ e is 65537 (0x010001) root@internalTest:/home/ca# ls ca.key 建立 CA CRT 建立完 CA 的私鑰後，我們還需要建立 CA 的身分證 CA CRT ，這個身分證就代表 CA 公開出去的所有人都看得到。\n1 2 3 4 5 openssl req -x509 -new -nodes -key ca.key -days 10000 -out ca.crt -subj \u0026#34;/CN=playwithca\u0026#34; root@internalTest:/home/ca# ls ca.key ca.crt 建立 Server Key 建立 server 的私鑰 ， 用來給 nginx server 的 TLS private key。\n1 2 3 4 5 6 openssl genrsa -out server.key 2048 Generating RSA private key, 2048 bit long modulus (2 primes) ...................+++++ .............................................................................................+++++ e is 65537 (0x010001) Server CSR 建立一個 CSR 用來向 CA 說請幫我簽名，簽的 Domain 是 example.com 。\n可以想像你去戶政事務所填寫申請身分證的表單\n1 openssl req -new -key server.key -out server.csr -subj \u0026#34;/CN=example.com\u0026#34; Server CRT 透過剛剛產出的 CSR 給 CA 簽名，這邊一般來說是用買的，你給 CA 商 CSR 他會幫你簽名出一個 CRT ， 這個 CRT 就是這個 server 的身分證。\n從你填寫表單的內容戶政事務所發給你對應的身分證\n1 2 3 4 5 6 7 8 9 openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365 -out server.crt -days 365 Signature ok subject=CN = example.com Getting CA Private Key root@internalTest:/home/ca# ls ca.crt ca.key ca.srl server.crt server.csr server.key 這邊簡單設定一下 nginx 的 SSL ，讓 nginx 可以使用指定的證書，並且開啟 443 port 提供其他人連線。\nnginx config ... server { listen 443 ssl default_server; listen [::]:443 ssl default_server; ssl_certificate /home/ca/server.crt; ssl_certificate_key /home/ca/server.key; } ... 設定好 nginx 別忘了重新載入 nginx 設定檔（題外話我覺得 nginx graceful reload做得滿好的，有興趣的朋友可以去研究一下）\n1 nginx -s reload 由於是測試環境沒有實際的 domain ，就先以修改 hostname 作為替代方案。\n1 2 3 4 5 6 7 8 9 cat /etc/hosts ## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 52.163.243.191 example.com ... 修改完成後測試一下能不能透過 https 連線到 nginx 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 curl -k https://example.com \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully i ... 可以順利連線到，那接著可以撰寫一個簡單的 Client 去打打看 Server 囉！\nSample Client 這邊會以一個單向 TLS 認證作為示範，單向 TLS 認證大致上的動作是， Client 向 Server 發起請求。這時候 Client 會拿到 Server 的 證書( crt ) ，Client 會拿著這組 Server crt 跟 CA 的 crt 進行驗證 ，看 Server 是不是被 CA 授權的還是假冒的。\n如果是假冒的那麼 Client 會拒絕連線，反之 Server 若是被 CA 認證的那 Client 就會繼續往 Server 發請求。\n可以想得簡單一點，今天所有人來向你買東西，看到你身分證都馬上拿去問戶政事務所問這個身分證是不是合法的公民，如果不是合法的身分證就不進行交易。\n以下是一個用 go 開發的單向 TLS 請求的 Client 。\n1 2 3 4 5 6 7 tree /go/src/dev/ . ├── 2.0 │ ├── client.crt │ ├── client.key │ └── root-ca.crt └── main.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;io\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; ) func main() { tr := \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ RootCAs: loadCA(\u0026#34;/Users/jason/go/src/dev/example-tls/2.0/root-ca.crt\u0026#34;), }, } c := \u0026amp;http.Client{Transport: tr} if resp, e := c.Get(\u0026#34;https://example.com\u0026#34;); e != nil { log.Fatal(\u0026#34;http.Client.Get: \u0026#34;, e) } else { defer resp.Body.Close() io.Copy(os.Stdout, resp.Body) } } func loadCA(caFile string) *x509.CertPool { pool := x509.NewCertPool() if ca, e := ioutil.ReadFile(caFile); e != nil { log.Fatal(\u0026#34;ReadFile: \u0026#34;, e) } else { pool.AppendCertsFromPEM(ca) } return pool } 以下是單向 TLS 請求的 Client 向 Server 請求的結果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 go run main.go \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; ... 結束語 這篇文章分享了如何用 nginx 、 OpenSSL 以及 Go 做一個自簽的 TLS 服務，雖然是一個簡單的範例但我們可以從中瞭解單向 TLS ， Client 只要向 CA 驗證 Server 的身份即可，那這樣安全嗎？這個討論我會保留到下一章再來跟大家分享。\n","description":"","id":55,"section":"posts","tags":["ci/cd"],"title":"CA 動手玩","uri":"https://blog.jjmengze.website/zh-tw/posts/ca/ca-one-way-ssl/"},{"content":" 前面有一篇文章，在描述 Github Action 做 CI/CD 的好處，對於 Github Action 還不太了解的朋友可以參考 Github Action 起步走第一式了解一下最基礎的東西怎麼用吧～\n在 CI/CD 總有一個階段會需要啊我們應應用服務部署到 Server 上提供對應的服務，對於初期快速發展的公司來說 Heroku 是一個滿好用的平台，可以直接把 source code push 上去， Heroku 會提供免費帳戶每個月 450 個小時的，開通信用卡，還會額外增加 550 個小時的免費時數，此外 Heroku 會幫你設定相關的 Url 讓外部可以讀取你的服務，這讓開發人員不用擔心 infrastructure 的問題，可以更加專注在 application 的開發。\n這篇文章主要是一個小範例，我們可以在 Github Action 中嵌入一個 Job 讓服務部署到 Heroku上～\n申請 Heroku 帳號 首先我們要去申請 Heroku 帳號 ，這邊非常簡單把他打米字號的欄位填一填寫一寫就沒問題了～\n完成這個完成這個申請帳號的步驟之後，接著開一個 Heroku 的應用程式，透過介面上的 New 的 Create new app 建立一個應用程式，需要注意的是App name 只能用小寫的英文，而且是沒有被使用過的，基本上這樣就完成了跟在 Github 上建立一個 Repository 一樣簡單。\n安裝 Heroku CLI 安裝 Heroku CLI 這個部分也相當的簡單安裝過程可以參考官方的教學，我這邊以 MACOS 作為示範有其他作業系統需求的請麻煩請參考官網囉～\nbrew tap heroku/brew \u0026amp;\u0026amp; brew install heroku Updating Homebrew... ==\u0026gt; Auto-updated Homebrew! Updated 3 taps (homebrew/core, homebrew/cask and homebrew/cask-fonts). ... ... ==\u0026gt; Installing dependencies for heroku/brew/heroku: heroku/brew/heroku-node ==\u0026gt; Installing heroku/brew/heroku dependency: heroku/brew/heroku-node 🍺 /usr/local/Cellar/heroku-node/12.16.2: 3 files, 42.2MB, built in 3 seconds ==\u0026gt; Installing heroku/brew/heroku ... 測試一下 Heroku CLI 是不是已經裝好了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 heroku CLI to interact with Heroku VERSION heroku/7.41.1 darwin-x64 node-v12.16.2 USAGE $ heroku [COMMAND] COMMANDS access manage user access to apps addons tools and services for developing, extending, and operating your app apps manage apps on Heroku auth check 2fa status authorizations OAuth authorizations autocomplete display autocomplete installation instructions buildpacks scripts used to compile apps certs a topic for the ssl plugin ci run an application test suite on Heroku clients OAuth clients on the platform config environment variables of apps container Use containers to build and deploy Heroku apps domains custom domains for apps drains forward logs to syslog or HTTPS features add/remove app features git manage local git repository for app help display help for heroku keys add/remove account ssh keys labs add/remove experimental features local run Heroku app locally logs display recent log output maintenance enable/disable access to app members manage organization members notifications display notifications orgs manage organizations pg manage postgresql databases pipelines manage pipelines plugins list installed plugins ps Client tools for Heroku Exec psql open a psql shell to the database redis manage heroku redis instances regions list available regions for deployment releases display the releases for an app reviewapps manage reviewapps in pipelines run run a one-off process inside a Heroku dyno sessions OAuth sessions spaces manage heroku private spaces status status of the Heroku platform teams manage teams update update the Heroku CLI webhooks list webhooks on an app 測試 Heroku 這邊以一個簡單的 go server 作為範例，程式碼如下所示。當我們對這個 server 發起請求他會回應你 現在 server 是使用哪一個 port 跟你做溝通。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main import ( \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; ) func main() { http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/html; charset=utf-8\u0026#34;) p:=os.Getenv(\u0026#34;PORT\u0026#34;) w.Write([]byte(p)) }) http.ListenAndServe(\u0026#34;0.0.0.0:\u0026#34;+os.Getenv(\u0026#34;PORT\u0026#34;), nil) } 透過 Heroku CLI 登入 Heroku ，驗證完身份後，就能夠透過 Git 把source code 推到 Heroku 上囉～\n1 2 3 4 5 6 heroku login heroku: Press any key to open up the browser to login or q to exit: Opening browser to https://cli-auth.heroku.com/auth/cli/browser/5e83cd9d-51cc-4676-b0be-d240a1f5c480 Logging in... done ... 接著設定 heroku git 把git remote url 設定好\nheroku git:remote -a example-githubaction set git remote heroku to https://git.heroku.com/example-githubaction.git 先看看 Heroku 有沒有加到 git remote url\n1 2 3 4 5 git remote -v heroku\thttps://git.heroku.com/example-githubaction.git (fetch) heroku\thttps://git.heroku.com/example-githubaction.git (push) origin\thttps://github.com/jjmengze/heroku-go.git (fetch) origin\thttps://github.com/jjmengze/heroku-go.git (push) 最後透過git push 指令將source code上傳到 Heroku\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 git push heroku master Enumerating objects: 10, done. Counting objects: 100% (10/10), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (10/10), 1.25 KiB | 640.00 KiB/s, done. Total 10 (delta 0), reused 0 (delta 0) remote: Compressing source files... done. remote: Building source: remote: remote: -----\u0026gt; Go app detected ... ... remote: -----\u0026gt; Launching... remote: Released v3 remote: https://example-githubaction.herokuapp.com/ deployed to Heroku remote: remote: Verifying deploy... done. To https://git.heroku.com/example-githubaction.git * [new branch] master -\u0026gt; master 完成後回到之前在 Heroku 上建立的應用程式，並且在上方的選項點選 Settings\n最下方 Heroku 會告訴你這個應用程式的 Domain ，一般來說會是 https://\u0026lt;appname\u0026gt;.herokuapp.com/，所以以本篇的例子會是 https://example-githubaction.herokuapp.com/ 。到頁面下方檢查一下 Domain是不是這個～\n可以請求一下這個 domain 看看得到的結果是什麼\n1 2 curl https://example-githubaction.herokuapp.com/ 59048 表示目前 server 是用 59048 的 port 跟你進行溝通，這邊我就想到壞壞的事情惹，那我能不能用 Heroku 提供的服務 在裡面啟用一個 sshd ，另類 VM 的概念xD (不過那又是另外一個故事了)\n改到 Github Action 既然純手動 ( manually setup ) 可以完成，那我相信透過 CI/CD 自動化機制也可以完成！\n這邊可以到 Github Marketplace 去找找有沒有人做相關的 Action ，好的工程師第一步要學會怎麼找資料 xD\n這邊可以看到有不少 Action 都有在做 HeroKu 相關的部署，可以參考人家怎麼實作在試試看能不能用更簡單的方法做一個出來。\n這邊我直接給出一個方案所有程式碼與內容可以參考我在 Github 的範例專案，有興趣的可以試試看，基本上的思路是透過 Heroku 提供的 container 服務直接把 source code 包成 container 並且上傳到 heroku image registry 上，相關的部署以及詳細動作可以參考官方的範例\n這邊我直接上github action 整合Heroku container image 後的 ci yaml給大家參考，大家也可以依照個做變化做成自己想要的樣子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 push: name: Push runs-on: ubuntu-latest steps: - name: Check out code into the Go module directory uses: actions/checkout@v2 - name: Login to Heroku Container registry env: HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }} run: heroku container:login - name: Build and push env: HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }} run: heroku container:push -a ${{ env.APP_NAME }} web - name: Release env: HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }} run: heroku container:release -a ${{ env.APP_NAME }} web 其中可以看到 push 這個 stage 執行了四個 steps ，比較要關注的是後面那三個。分別是執行\nheroku container:login heroku container:push -a ${{ env.APP_NAME }} web heroku container:release -a ${{ env.APP_NAME }} web 這三部分別對應的是\n登入 heroku 構建 Container image 並將其推送到 Heroku Container Registry 將 Heroku Container Registry 上的 Container image 進行部署與發佈 HEROKU_API_KEY 以及 APP_NAME 都可以在 Github 的 CI/CD 服務直接做環境變數的編輯，透過這一種方式我們可以很簡單的把服務部署到Heroku上面。開發人員可以快速的部署與測試自己的服務～ CI/CD 帶來的好處多多，希望大家可以多多嘗試。\n結束語 今天非常簡單的介紹了 Heroku 與 Github Action 的整合的功能，以及怎麼快速得使用 Heroku 給外部做存取，下次有機會再來介紹他跟docker怎麼整合以及怎麼撰寫自己的 action 貢獻給 Github Action Marketplace 讓大家來使用。\n","description":"","id":56,"section":"posts","tags":null,"title":"Github Action 妳各位，注意 Heroku ！","uri":"https://blog.jjmengze.website/zh-tw/posts/cicd/github/github-action-2/"},{"content":" 記錄一下之前在工作上遇到的 Gitlab domain 沒有簽憑證，但是外部服務(Azure Kubernetes Service)又需要存取 Gitlab Container Registry 會遇到的問題。\n發生了什麼？ Gitlab 的 domain 沒有買 SSL （由於政策問題也不能用 letsencrypt)\nKubernetes 拉取 Gitlab Container Registry 需要帳密\nKubernetes 拉取 Gitlab Container Registry 憑證不被信任\n檢視各項問題 Gitlab domain 沒有ssl 如果我們在 host 上透過 docker 拉取該 Container Registry 的 image 可能會出現的狀況。\n先看一下我使用的 docker 測試環境\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 $docker version Client: Version: 19.03.6 API version: 1.40 Go version: go1.12.17 Git commit: 369ce74a3c Built: Fri Feb 28 23:45:43 2020 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 19.03.6 API version: 1.40 (minimum version 1.12) Go version: go1.12.17 Git commit: 369ce74a3c Built: Wed Feb 19 01:06:16 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.3.3-0ubuntu1~18.04.2 GitCommit: runc: Version: spec: 1.0.1-dev GitCommit: docker-init: Version: 0.18.0 GitCommit: 接著我透過 Docker CLI 去拉 Gitlab Container Registry 上的 private image，由於\u0026hellip;一些公司政策問題我把公司的位置替換掉了請見諒。\n1 2 $docker pull registry-gitlab.com.tw/repo/image-cli:v4.4.0 Error response from daemon: Get registry-gitlab.com.tw/v2/: x509: certificate is not authorized to sign other certificates 直接垃取會出現 x509: certificate is not authorized to sign other certificates，這個部分非常容易解決 Google 一下就可以很快的拿到解，只要docker login 一下就沒問題了。\n在那之前我們先來看一下相關的 docker 設定檔\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 $cat /lib/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com BindsTo=containerd.service After=network-online.target firewalld.service containerd.service Wants=network-online.target Requires=docker.socket [Service] Type=notify # the default is not to use systemd for cgroups because the delegate issues still # exists and systemd currently does not support the cgroup feature set required # for containers run by docker ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always # Note that StartLimit* options were moved from \u0026#34;Service\u0026#34; to \u0026#34;Unit\u0026#34; in systemd 229. # Both the old, and new location are accepted by systemd 229 and up, so using the old location # to make them work for either version of systemd. StartLimitBurst=3 # Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230. # Both the old, and new name are accepted by systemd 230 and up, so using the old name to make # this option work for either version of systemd. StartLimitInterval=60s # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity # Comment TasksMax if your systemd version does not support it. # Only systemd 226 and above support this option. TasksMax=infinity # set delegate yes so that systemd does not reset the cgroups of docker containers Delegate=yes # kill only the docker process, not all processes in the cgroup KillMode=process [Install] WantedBy=multi-user.target 上面可以看到 Docker systemD 的相關設定 ，看起來都相當的簡單而且沒有任何繞過 insecure 的選項。\n也同時在 /etc 底下看看 Docker 的設定\n1 2 $ls /etc/docker/ key.json 這邊我們可以看到 Docker 完全沒有設定任何繞過 insecure 的選項後，使用docker login 後再看看有沒有任何的改變。\n1 2 3 4 docker login registry-gitlab.com.tw Username (test): Password: Error response from daemon: Get https://registry-gitlab.com.tw/v2/: x509: certificate is not authorized to sign other certificates 這時候我們又觀察到新的問題了，在 login 的時候發現我們的Gitlab Registry 的憑證有問題，在繼續 Google 會看到原來要把憑證有問題的 domain 設定到 /etc/docker/daemon.json 下面，這邊我很快的設定起來，並且重啟dockerd。\n1 2 3 4 5 cat \u0026lt;\u0026lt;EOF | \u0026gt;\u0026gt; /etc/docker/daemon.json { \u0026#34;insecure-registries\u0026#34;:[\u0026#34;registry-gitlab.com.tw\u0026#34;] } systemctl restart docker 接著再嘗試透過docker login 登入並且拉取 Gitlab container registry 的 image。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $docker login registry-gitlab.com.tw Username (test): Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded $docker pull registry-gitlab.com.tw/repo/image-cli:v4.4.0 v4.4.0: Pulling from registry-gitlab.com.tw/repo/image-cli:v4.4.0 23302e52b49d: Pulling fs layer cf5693de4d3c: Download complete 0bdf97977791: Downloading [=\u0026gt; ] 110.1kB/3.493MB 8b8c7ad8f3fb: Waiting 40eb930bd6b2: Waiting 到這邊我們終於解決在 docker 拉取沒有 SSL Container registry 上的 private image 了，接著我們要來看在公有雲(Azure Kubernetes Service)上出了什麼問題。\nKubernetes pull image og Gitlab Container Registry 接著來看看在公有雲環境中( Azure Kubernetes Service , AKS )會出現的問題，我在 AKS 部署一個三個節點的環境，Kubernetes 版本為 1.15.11。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 bash-5.0# kubectl get node NAME STATUS ROLES AGE VERSION aks-agentpool-20139558-vmss000000 Ready agent 11m v1.15.11 aks-agentpool-20139558-vmss000001 Ready agent 11m v1.15.11 aks-agentpool-20139558-vmss000002 Ready agent 11m v1.15.11 bash-5.0# kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-698c77c5d7-69rch 1/1 Running 0 11m kube-system coredns-698c77c5d7-vsrzb 1/1 Running 0 14m kube-system coredns-autoscaler-5bd7c6759b-jx9dt 1/1 Running 0 14m kube-system kube-proxy-46phd 1/1 Running 0 11m kube-system kube-proxy-fgb6f 1/1 Running 0 11m kube-system kube-proxy-gxck8 1/1 Running 0 11m kube-system kubernetes-dashboard-74d8c675bc-j25fz 1/1 Running 0 14m kube-system metrics-server-7d654ddc8b-bljdd 1/1 Running 0 14m kube-system omsagent-rs-c45c944df-5crtb 1/1 Running 0 14m kube-system omsagent-w5b8l 1/1 Running 1 11m kube-system omsagent-xfndn 1/1 Running 1 11m kube-system omsagent-xhnbv 1/1 Running 0 11m kube-system tunnelfront-98c8b5dc6-b56d5 1/1 Running 0 14m 在AKS的環境上我直接透過kubectl CLI 建立一個非常簡單的Deployment Resource，測試拉取 Container registry 上的 private image 會有什麼問題。\n1 bash-5.0# kubectl run -ti --rm test --image registry-gitlab.com.tw/repo/image-cli:v4.4.0 bash 接著觀察 pod 是否有成功被建立起起來。\n1 2 3 bash-5.0# kubectl get pod NAME READY STATUS RESTARTS AGE test 0/1 ErrImagePull 0 12s 可以看到 pod 出現 ErrImagePull 的 status 此時，需要更進步一分析出現ErrImagePull的原因。\n1 2 3 4 5 6 7 8 bash-5.0# kubectl describe pod test ... Normal BackOff 16s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Back-off pulling image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34; Warning Failed 16s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Error: ImagePullBackOff Normal Pulling 4s (x3 over 46s) kubelet, aks-agentpool-20139558-vmss000000 Pulling image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34; Warning Failed 4s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Failed to pull image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: Get https://registry-gitlab.com.tw/v2/: x509: certificate is not authorized to sign other certificates Warning Failed 4s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Error: ErrImagePull 這邊可以觀察到問題\nPulling image \u0026ldquo;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026rdquo; 時發生\nx509: certificate is not authorized to sign other certificates\nGoogle 可以搜尋到很多 Kubernetes pull private 的解決方法，這邊直接使用看看還會遇到什麼問題。大致上就是設定 kubernetes 的某個 namespace 如果要拉某一個位置上的 private image 可以透過這一個使用者帳密去執行。\nkubectl create secret docker-registry gitlab-registry \\ --docker-username=jason \\ --docker-password=RmgmeG4K-1oD4j3XW5A- \\ --docker-email=jason@hello.com.tw \\ --docker-server=registry-gitlab.com.tw secret/gitlab-registry created 這一步做完我們可以再來看看是不是可以成功pull到image，我們先把原本的 pod delete 再重新 執行一個。\n1 2 3 4 5 bash-5.0# kubectl delete pod test pod \u0026#34;test\u0026#34; deleted kubectl run -ti --rm test --image registry-gitlab.com.tw/repo/image-cli:v4.4.0 bash ... 這時候持續觀察 pod 的狀態是不是running\n1 2 3 bash-5.0# kubectl get pod NAME READY STATUS RESTARTS AGE test 0/1 ErrImagePull 0 12s 什麼竟然還不是 running 那我們繼續順藤摸瓜看看 pod 到底錯了什麼。\n1 2 3 4 5 6 7 8 bash-5.0# kubectl describe pod test ... Normal BackOff 16s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Back-off pulling image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34; Warning Failed 16s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Error: ImagePullBackOff Normal Pulling 4s (x3 over 46s) kubelet, aks-agentpool-20139558-vmss000000 Pulling image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34; Warning Failed 4s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Failed to pull image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: Get https://registry-gitlab.com.tw/v2/: x509: certificate is not authorized to sign other certificates Warning Failed 4s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Error: ErrImagePull 怎麼還是樣的錯，這時候不得不去看看 pod 的 spec 到底定義了什麼有沒有使用我們指定的 image pull secret 。\n1 2 3 4 5 6 7 8 9 10 11 bash-5.0# kubectl get pod test -o yaml ... name: default-token-wvgsq readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: aks-agentpool-20139558-vmss000000 nodeSelector: node-role.kubernetes.io/agent: \u0026#34;\u0026#34; priority: 0 ... 怎麼沒有 image pull secret 呢\u0026hellip;，找了一下文件 pod 再啟動時都會去用 default service account 的一些設定。\nref:https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account\n這邊就偷懶讓 default namesapce 都用同一個 image pull secret 吧 xDD\nkubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;gitlab-registry\u0026#34;}]}\u0026#39; 這邊再把 Pod 刪掉再重新測試一次看看 image 能不能正確的拉取到。\n1 2 3 4 5 bash-5.0# kubectl delete pod test pod \u0026#34;test\u0026#34; deleted kubectl run -ti --rm test --image registry-gitlab.com.tw/repo/image-cli:v4.4.0 bash ... 這時候持續觀察 pod 的狀態是不是running\n1 2 3 bash-5.0# kubectl get pod NAME READY STATUS RESTARTS AGE test 0/1 ErrImagePull 0 12s 還是不行啊，繼續透過kubectl CLI 找尋錯誤的原因。\nbash-5.0# kubectl describe pod test ... Normal BackOff 21s kubelet, aks-agentpool-20139558-vmss000002 Back-off pulling image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34; Warning Failed 21s kubelet, aks-agentpool-20139558-vmss000002 Error: ImagePullBackOff Normal Pulling 10s (x2 over 23s) kubelet, aks-agentpool-20139558-vmss000002 Pulling image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34; Warning Failed 10s (x2 over 22s) kubelet, aks-agentpool-20139558-vmss000002 Failed to pull image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: Get https://registry-gitlab.com.tw/v2/: x509: certificate is not authorized to sign other certificates Warning Failed 10s (x2 over 22s) kubelet, aks-agentpool-20139558-vmss000002 Error: ErrImagePull 這次看到一樣的錯誤資訊，這時候想到 unssl 的 image registry 不是要去設定 docker/daemon.json ，這樣 docker 去拉 image 的時候才不會認證他的憑證。\n因為在公有雲(Azure Kubernetes Server,AKS)上碰不到這幾台worker 主機，只好用奇門遁甲的方式 mount 他的 filesystem 了。\n這邊我解決的方法透過 daemonset 讓 pod 部署到所有的 node 上，該 pod 把 host 的 /etc mount 出來。讓我可以觀察AKS host docker 的 config。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion: apps/v1 kind: DaemonSet metadata: name: registry-ca namespace: kube-system labels: k8s-app: config-docker spec: selector: matchLabels: name: config-docker template: metadata: labels: name: config-docker spec: containers: - name: config-docker image: nginx:1.18 command: [ \u0026#39;sh\u0026#39; ] args: [ \u0026#39;-c\u0026#39; , \u0026#39;tail -f /dev/null\u0026#39;] volumeMounts: - name: etc-docker mountPath: /etc/docker securityContext: privileged: true terminationGracePeriodSeconds: 30 hostPID: true volumes: - name: etc-docker hostPath: path: /etc/docker 這邊眼尖得觀眾可能會發現為什麼我用了，securityContext以及hostPID這邊後續會講為什麼我要這樣做。\n好的廢話不多說，部署完這個 daemset 後直接進入 pod 裡面看他的設定。\nroot@registry-ca-rdzbv:/# cat /etc/docker/daemon.json { \u0026#34;live-restore\u0026#34;: true, \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;50m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;5\u0026#34; } } 發現！他竟然沒有設定 insecure-registries 這邊我只好手動幫他做設定，設定完後直接送給 host pid reload dockerd。這邊你會好奇說為什麼可動到 host 上的 pid ，因為上面的 yaml 有設定 securityContext 以及 hostPID ，所以 container 可以直皆碰到 host 的 process 並且可以下一些需要 security 的指令。\n1 2 3 root@registry-ca-rdzbv:/#pidof dockerd 3322 root@registry-ca-rdzbv:/#kill -1 3322 這邊完成後，直接delete test pod 看看重新 run 一個能不能成功pull 到 private image 。\n1 2 3 4 5 bash-5.0# kubectl delete pod test pod \u0026#34;test\u0026#34; deleted kubectl run -ti --rm test --image registry-gitlab.com.tw/repo/image-cli:v4.4.0 bash ... 這時候持續觀察 pod 的狀態是不是running\n1 2 3 bash-5.0# kubectl get pod NAME READY STATUS RESTARTS AGE test 1/1 Running 0 30s 此時還有一個問題，剛剛只有修改一個節點如果 Pod 今天部署到其他節點上，還是會出現x509: certificate is not authorized to sign other certificates的問題。有沒有方式可以一勞永逸呢？我這邊直接沿用剛剛的daemset 並且修改一下 yaml file讓這個 pod 可以處理這個問題。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 apiVersion: apps/v1 kind: DaemonSet metadata: name: registry-ca namespace: kube-system labels: k8s-app: config-docker spec: selector: matchLabels: name: config-docker template: metadata: labels: name: config-docker spec: containers: - name: config-docker image: nginx:1.18 command: [ \u0026#39;sh\u0026#39; ] args: [ \u0026#39;-c\u0026#39;, \u0026#39;cp --remove-destination /home/core/daemon.json /etc/docker/daemon.json \u0026amp;\u0026amp; kill -1 $(pidof dockerd) \u0026amp;\u0026amp; tail -f /dev/null\u0026#39;] volumeMounts: - name: etc-docker mountPath: /etc/docker - name: docker-config mountPath: /home/core securityContext: privileged: true terminationGracePeriodSeconds: 30 hostPID: true volumes: - name: etc-docker hostPath: path: /etc/docker - name: docker-config configMap: name: docker-insecure 這邊可以看到直接套了一個 configmap 以及透過 runtime 的 args 把configmap 的資料複製到 /etc/docker/daemon.json 最後重啟了 dockerd 的 process。\n以上是解決在Azure Kubernetes Service 遇到的 unssl image registry 的思路與解決方法。\n","description":"","id":57,"section":"posts","tags":["kubernetes"],"title":"Azure Kubernetes Service pull unssl  image registry","uri":"https://blog.jjmengze.website/zh-tw/posts/azure/private-image/"},{"content":" ref: https://docs.openshift.com/container-platform/4.4/authentication/identity_providers/configuring-gitlab-identity-provider.html\nGitlab 管控使用者 為什麼要使用 Gitlab 進行平台的管控\ndentity providers use OpenShift Container Platform Secrets in the openshift-config namespace to contain the client secret, client certificates, and keys.\nYou can define an OpenShift Container Platform Secret containing a string by using the following command.\nCreating the Application OAuth 第一步，先去Gitlab上建置相關的application，並且設定該application所需要的權限。\n如圖所示先選擇Gitlab網頁上右上角自己的頭像，並選擇Setting。 選擇完Setting後，到左方選擇Applications，進入到設定Application的頁面。 進到設定Application的頁面後，會看到以下畫面。這邊有幾個部分需要留意。 Redirect URI\n需要特定格式進行填寫，https://oauth-openshift.apps.\u0026lt;cluster-name\u0026gt;.\u0026lt;cluster-domain\u0026gt;/oauth2callback/\u0026lt;idp-provider-name\u0026gt;\ncluster name 與 cluster-domain 可以在 openshift console 的網址上看到，例如:console-openshift-console.apps.ocp4demo.jjmengze.website\n其中 ocp4demo 就是我們的 cluster name ，而 jjmengze.website 就是 cluster-domain。\nopenid\n因為 Openshift 需要透過 OpenID Connect (OIDC)驗證 Gitlab 使用者的登入資訊，所以我們要把 openid 給勾選起來。\n填寫完成後按下 Save application 可以看到以下結果。 Create secret 要在 OCP 上面建立一個 secret 後續讓驗證的時候可以攜帶這個 secret 資訊。\n這裡的 clientSecret 的數值就是剛剛在 Gitlab 上建立 Application 的 Secret 的數值呦！\n$kubectl create secret generic gitlab-app-oath --from-literal=clientSecret=\u0026#34;1368992d4100943938f4896d47419b4bba3c5598dce0fa15f8c4ca50fc16758f\u0026#34; -n openshift-config Create Comfigmap 接著建立 configmap ，這裡的 configmap 是 Gitlab 的憑證，只要修改一下憑證的路徑在按照下面的命令執行就可以把憑證放到 OCP 上了。\n$kubectl create configmap gitlab-ca --from-file=ca.crt=/mnt/gitlab-ca.crt -n openshift-config Create OAuth 上面的步驟做完之後，我們還需要建立一個 yaml 案來告訴 OCP 說現在要多提供一種登入及認證方式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: \u0026lt;登入選項會出現的名稱\u0026gt; mappingMethod: claim type: GitLab gitlab: clientID: \u0026lt;Gitlab上取得的application ID\u0026gt; clientSecret: name: gitlab-app-oath url: \u0026lt;Gitlab的位置\u0026gt; ca: name: gitlab-ca de re mi so ，我們可以 apply 這個設定檔案到 OCP 上囉！\n$kubectl apply -f oath-gitlab.yml 就著我們就可以到openshift console 看剛剛的設定是不是大成功了！\n快樂地點進去之後，可以看看。可以看到 OCP 向你的 Gitlab 要權限登入囉。\n按下 Authorize 後就可進入到 Openshift 的頁面，接著開始操作你的環境囉！\n","description":"","id":58,"section":"posts","tags":["openshift"],"title":"OpenShift 燈紅酒綠與 Gitlab identity","uri":"https://blog.jjmengze.website/zh-tw/posts/ocp/gitlab-ocp/"},{"content":" 為什麼要使用？ 這邊我直接點出我個人的觀點，歡迎大家來探討到底需不需要導入 tracing 的概念在專案中。\n就我個人的理解導入 tracing 有幾個好處，能提供系統運作時有更全面的理解與追縱，包括延遲，錯誤以及效能等問題，我們可以透過 tracing 查看整個系統或是單個應用程式是如何處理某一項業務操作 並且追蹤此項操作的流程與相關訊息。\n這樣帶來了什麼好處？\n如果可以透過圖表等方式顯示我們跟蹤的結果，可以加速後續維運人員排除故障的速度並且可以幫助開發者找到效能問題的癥結點，此外我們還可以看到一條業務邏輯實際的流向是如何被處理的。\n那什麼是Opentracing 再談談 Opentracing 之前我們可以將時間回推倒90年代，當時 Google 發表了一篇關於分散是追蹤的論文\u0026ldquo;Dapper, a Large-Scale Distributed Systems Tracing Infrastructure\u0026rdquo;，除了該篇論文外 Google 還發表了另外一篇分散式追蹤的的問題所在\u0026ldquo;Uncertainty in Aggregate Estimates from Sampled Distributed Traces\u0026rdquo;，兩篇論文都非常的精采說明了為什麼需要做分散式追蹤以及追蹤帶來的效益已提以及追蹤帶來的效益與其困難點。\n隨後各家廠商跟著這幾篇論文的引導開發出一系列相關的工具例如， Dapper 、 Zipkin 、 Appdash 等等，越來越多廠商到了戰場上爭奪這一塊 tracing 的大餅，不過就使用者而言就相當的頭痛了，對於各廠商提供的 API 與系統支援度都不相同的情況之下，使用者幾乎沒有辦法無痛得轉移所使用的 tracing System。\n下一個時代來臨必須要有人統一群雄打造一個輕量級且標準化的中介層統一上下層之間的隔閡， CNCF 底下的 OpenTracing 願景為提供一個標準的 tracing API 所有使用者只要在乎 OpenTracing tracing 所提供的 API ，不需要再了解各家廠商如 Zipkin 、 Appdash 的流程，這點我認為他跟 Kubernetes 的 CNI 、 CRI 、 CSI有異曲同工之妙。\n名词解释 在了解 Opentracing 的由來之後，現在來介紹他的架構與相關的名詞，這有助於我們後續開發上的 API 使用。\nTrace 一個 trace 在 Opentracing 代表一個事件在系統的執行過程，我們可以從圖中看到藍色的代表一個事件在 Opentracing 稱為一個 trace ，底下經過紅色綠色以及灰色的處理。\n比較學術的說法唯有向無環圖Directed Acyclic Graph （ DAG )，有興趣的可以去查查這是什麼。\nSpan span 代表在 Opentracing 中各個工作單元。一個 span 可以包含其他 span 的 reference ，如此一來可以形成一個父子關係圖，進而形成一個完整的 trace 。\n其中 span 會攜帶一些資訊方便使用者閱讀例如：\nreference span 名稱 span 開始與結束時間 tag log span Context baggage Items\nChildOf 先來看看流程結構時序圖，Childof 我會歸類成父節點依賴子節點的結果。例如 Server Span 透過 RPC等方式 去叫了 register Span ， register Span 處理了一些資料又去呼叫 SQL Span 做 insert 的動作，SQL Span 對 SQL 存入資料後會返回給 register Span 回報 insert 動作成功，接著 register Span 又會返回給 Server Span 註冊成功的消息。\n這種父節點依賴子節點的行為稱為ChildOf。\n[-Parent Server Span--------------] [-Child register Span A----] [-Child Span B----] FollowsFrom 先來看看流程結構時序圖，父節點不依賴任何子節點的結果我把他歸類為 FollowsFrom ，例如我發送一條訊息，訊息是否成功被處理與發訊息的事件沒有關聯，這時就稱為 FollowsFrom 。\n[-Parent Span-] [-Child Span-] Span-tag 在 OpenTracing 的規範之下，每一個 span 都可以有 tag 屬性以 key-value 的形式出現。 tag 我們可以理解成某一種屬性例如圖中的error tag ，我們可以利用這格屬性過濾掉error =false 的span，方便我們查詢我追蹤。\nSpan-log 除了 tag 之外 Opentracing 還規範了 log ，開發者可以在 span 中加入 log ，他以以 key-value 的形式出現，方便開發者或是維運人員快速地檢視該 span 執行的相關訊息。如圖所示可以看到在 external service api 中的 log event 顯示time out 我們可以很快瞭解在執行這個操作的時候他的撞快為何。\nSpan Context 我認為這個比較難解釋，可以參考著官方的說明對照著看，簡單的說就是span會傳遞 context 資訊給下一個 span ，例如 spanID 、traceID等。\nThe SpanContext carries data across process boundaries. Specifically, it has two major components:\nAn implementation-dependent state to refer to the distinct span within a trace\ni.e., the implementing Tracer’s definition of spanID and traceID\nAny Baggage Items\nThese are key:value pairs that cross process-boundaries.\nThese may be useful to have some data available for access throughout the trace.\nBaggage Items 從字面上來看 baggage 就是行李的意思，一個 trace 開始到結束，可能會從某一個 span 攜帶行李到下一個 span 進行處理，不過這裡有一個非常重要的點就是行李在整個 trace 的過程都不會被丟棄，直到 trace 結束為止。\n從範例來看App A 是一個 span ，攜帶了一些資料到如ot-baggage-environment:production 到 APP B 如果後面還有 APP C 、APP D，這一個資料會傳到整個trace結束為止。\n小結 上面我們討論了 Opentracing 的前世今生，為什麼要有 tracing 以及相關的論文。同時針對 Opentracing 會出現的名詞做一個初步的解說，可以以幫助我們後續再使用 Opentracing 提供的 API 時更能了解他所代表的意義以及正確的使用姿勢。下一篇我會續繼分享Opentracing with jaeger 的用法。\n","description":"","id":59,"section":"posts","tags":["devops"],"title":"前方高能之Opentracing","uri":"https://blog.jjmengze.website/zh-tw/posts/go/opentracing/"},{"content":" Github Action 起步走 由於最近在使用 Github 時不論是 Create 一個新的 Repository 或是 Fork 別人現有的 Project 都會跳出以下提示，為了滿足好奇心就花了一點時間研究了 Github Action 這個新功能。\nGitHub Actions 是 GitHub 在美國11/13號全面開放的 CI/CD workflow系統，我們先看看官方的描述。\n官方的描述:\nGitHub Actions makes it easy to automate all your software workflows, now with world-class CI/CD. Build, test, and deploy your code right from GitHub.\n可以從官方的介紹看出來 GitHub Actions 是一個自動化 CI/CD 的功能，可以直接從GitHub deploy 以及 test 你的 source code。\n為什麼要用？ 在沒有用 Github Action 之前維運人員可能會透過 WebHook 之類的方式去得知當前 Repository 的狀態，例如 Kubernetes 社群就開發了一個機器人kubernetes/test-infra專門去處理相關的狀態改變並且觸發後續的流程，如 unit test 、intregration test 或是 e2e test(end to end test) \u0026hellip;.等等，或是透過其他 CI/CD 工具 如 Jenkins 、 Argo 等等其他工具去建構一個 pipline，Github 開發了Githbu Action 可以幫這我們在 Github Repository 上直接建立自動化部屬以及測試的流程，除此之外 Github 提供了一些吸引開發者去使用的特點。\nDiscovering actions Notifications for workflow runs Disabling or limiting GitHub Actions for your repository or organization \u0026#34;Managing a workflow run\u0026#34; \u0026#34;Events that trigger workflows\u0026#34; \u0026#34;Virtual environments for GitHub-hosted runners\u0026#34; \u0026#34;Managing billing for GitHub Actions\u0026#34; 常見的用法說明 GitHub Actions有許多常見的用詞，這邊稍微做一些簡單的解說。(這邊我個人認為我解釋得不好，不過還是聽聽吧xD)\nContinuous integration (CI): 我們可以透過 CI 去 build 以及 test Repository 上的 Source code。可以從 test 得過程快速地檢測錯誤的邏輯（這邊可以包含整個unit test 、intregration test 或是 e2e test(end to end test) \u0026hellip;.等等）。\n想要了解更多可以上wiki之類的去了解它的定義\nContinuous deployment (CD)\nCD通常是建築於CI的結果之上，當CI執行成功後會觸發CD，這個步驟基本上是將CI打包好的\nbinary檔案部署到各種不同的環境如Test 環境 Staging 等等。客戶的使用者或是測試人員可以快速地得到部署完的環境，並且進行業務邏輯的測試。\nWorkflow file:\n是定義這個 Repository CI/CD 的工作流程(workflow/pipline)的一個 YAML 檔，這個檔案會放在 RepoRepository 跟目錄底下的 .github/workflows 資料夾中。\nAction:\nAction 組合了多個 Step ，我可以將 Action 打包好並且分享到分享到 GitHub Action 社群。\nStep:\n是一個執行一個獨立的指令或是執行別人打包好的Action，一個 Job 裡面會包含一到多個Step，Job 裡面的Step 是共用同一個environment與filesystem。\nJob:\n由一個或是多個Step組成，當有多個Job的時候我們可以定義Job的相依關係與執行流程。Job 可以並行(parallel)執行或是按順序(sequentially)執行。例如Repository 有建立Build Job 以及Test Job ，Test Job 依賴Build Job 的執行結果，如果今天Build Job 執行失敗Test Job 則不會執行。\n通常Job執行在不同的Runner(虛擬環境virtual environment).\nRunner:\nRunner是在執行Job的虛擬環境，我們可以自己建立Runner或是使用Github 代管的Runner ， Runner 會去去監聽 Github Job 的資料，當有 Job 資料產生的時候 Github 會將 Job 下放給某一個 Runner 去執行，一但 Runner 執行完 Job 會將執行結果回報給Github。 Runner 一次只能執行一個 Job 。\nArtifact:\nArtifacts 就是把某一個 Job 執行完的結果（可能是 binary 檔、 log 檔等等），可以把這一個 Job 執行完的結果送到下一個 Job 。\nEvent:\n當某一個事件產生的時候如 create PR 、create issue 或是 create branch等等，Gitlab 會收到這個消息並且且進行處理。\n要怎麼上手使用 Github 有提供一些預設的 Workflow templates給開發者做使用，開發者可以針對不同語言套用不同的樣板。\nGithub還會分析你Repository 的source code並且推薦你適合的Workflow templates，例如我的Repository 包含Golang的source code ，我們會收到Github 的建議Golang Workflow templates 我們可以以這個模型堆疊我們需要的Workflow。\n可以參考actions/starter-workflows查詢Github 官方維護的 Workflow templates。\n起手式 先創一個專案再說，我這邊先用 Golang 建立一個非常簡單的WebServer Repo。\n如圖所示，我們可以在 Github 上面可以看到這一個提示，問我們要不要建立一個 GitHub Actions 。\n這邊點選Set Up Action會跳轉到這個頁面，選擇想要建立的Workflow templates。\n這邊可以直接點擊Set up workerflow，點進去後會跳轉到Workerflow 編輯頁面，頁面上可以看見預設的job、steps等資訊。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 name: Go on: push: branches: [ master ] pull_request: branches: [ master ] jobs: build: name: Build runs-on: ubuntu-latest steps: - name: Set up Go 1.13 uses: actions/setup-go@v1 with: go-version: 1.13 id: go - name: Check out code into the Go module directory uses: actions/checkout@v2 - name: Get dependencies run: | go get -v -t -d ./... if [ -f Gopkg.toml ]; then curl https://raw.githubusercontent.com/golang/dep/master/install.sh | sh dep ensure fi - name: Build run: go build -v . 稍微幾解釋一下這個yaml所代表的意義\nname\n表示這個Workflow的名字 on\n表示這個什麼時候會觸發這個Workerflow push\n表示推到這個某一個branch的時候會觸發，這裡預設是master pull_request\n表示像這個某一個branch發出PR的時候會觸發，這裡預設是master job\n裡面會有要執行的步驟與環境設定 build name\n表示這個job用來建構程式碼 runs-on\n表示這個這個job要執行在什麼虛擬環境上，這裡是ubuntu，可以改成更小的image steps\n表示這個job有多少步驟要執行 name\n描述這個這一步要做什麼，這邊預設是Set up Go 1.13 uses\n這邊的意思是使用別人寫的action，預設是actions/setup-go@v1 with\n可以指定action的參數，這邊指定go用1.13版go-version: 1.13 id\n可以在setup上加上一些標記，這邊預設是go run\n如果不要用別人的action想要執行自己的腳本可以用這個參數，如go build -v . [color=#FF0000]這邊非常粗淺的帶過這些參數所代表的意義，如果想進行更複雜的設定可以參考GitHub Help\n這邊設定好自己想要的Workerflow流程後可以按Start commit 提交這一個commit。\nCommit 之後可以看到Github Action 被觸發，我們可以點進去看一下CI/CD有沒有過。\n這邊就可以看到我在Workflow Build的階段就失敗了xD，可以點進去看為什麼失敗，判斷是不是code寫錯還是哪裡有問題。\n點進去後我們發現，原來是go build -v .出錯拉～那我們這邊做簡單的修改再重新提交一次。\n將 Build 那一段 run 改成正確的語法，再重新commit。\n1 2 - name: Build run: go build cmd/main.go 再次檢查Github Action 的Workflow是不是都亮綠燈通過拉～\n結束語 今天非常簡單的介紹了Github Action的功能，以及怎麼快速的上手使用這個新功能，下次有機會再來介紹他跟docker怎麼整合以及怎麼撰寫自己的action貢獻給Github Action Marketplace 讓大家來使用。\n","description":"","id":60,"section":"posts","tags":["ci/cd"],"title":"Github Action 起步走第一式","uri":"https://blog.jjmengze.website/zh-tw/posts/cicd/github/github-action-1/"},{"content":" 在 Kubernetes 資源是怎麼被回收的？ 是由 kubernetes master node 上的 kube-controller-manger 中的 garbage-collection controller 進行回收管理的。\n我們先不管 kubernetes 底層是怎麼回收這些垃圾物件的，先把焦點放在我們要怎麼設定yaml檔，畢竟我們是yaml工程師嘛(笑)，本篇文章會用進行幾個實驗來觀察各種 kubernetes 回收策略是如何進行的。\n當我們刪除物件時，可以指定該物件底下關聯的子物件是否也跟著自動刪除。\n自動刪除附屬的行為也稱為 級聯刪除（Cascading Deletion） Kubernetes 中有兩種 Cascading Deletion (聯集) 刪除分別是：\n後台（Background） 模式 前台（Foreground） 模式。 Foreground 條件 物件的 metadata.finalizers 被設定為 foregroundDeletion 物件處於 deletion in progress 狀態（deletionTimestamp被建立） 行為 需要等到物件所有的關聯的子物件被删除完之後，才可以删除該物件 如何確定關聯的子物件的父親是誰？ 透過子物件的 ownerReferences 來確定 Background kubernetes 立刻 馬上 删除物件， garbage-collection controller 會在後台(背景)删除該物件的子物件。\n除了在背景刪除子物件的行為外還有一種是不刪除子物件，讓子物件變成孤兒(Orphan)。\n實驗 propagation Policy (Foreground) deploy 部署測試的nginx deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 EOF deployment.apps/nginx-deployment created 狀態 取的deployment ReplicaSet 以及pod的狀態\n1 2 3 4 5 6 7 8 9 kubectl get deploy,pod NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 4m44s NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-nbtkw 1/1 Running 0 4m44s pod/nginx-deployment-6b474476c4-nkbrb 1/1 Running 0 4m44s pod/nginx-deployment-6b474476c4-zh5g7 1/1 Running 0 4m44s 取得ReplicaSet與pod的ownerReferences 用來確定物件之間的關係\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl get rs nginx-deployment-6b474476c4 -o go-template --template={{.metadata.ownerReferences}} [map[ apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:Deployment name:nginx-deployment uid:597d36f5-968a-4025-8621-b24f17f7f3a6]] kubectl get pod nginx-deployment-6b474476c4-nbtkw -o go-template --template={{.metadata.ownerReferences}} [map[ apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:ReplicaSet name:nginx-deployment-6b474476c4 uid:97fb6974-6882-4459-b6b0-b39357a7650b]] destroy Foreground 透過指定的刪除模式來刪除物件，這裡是透過Foreground 的方式刪除物件。\ncurl -X DELETE curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \\ -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Foreground\u0026#34;}\u0026#39; \\ -H \u0026#34;Content-Type: application/json\u0026#34; 狀態 取的deployment 以及pod的狀態觀察刪除狀態。\n從這個狀態可以看到所有的 pod 都在 Terminating 的狀態， ReplicaSet 以及 deployment 都沒有先移除。\n1 2 3 4 5 6 7 8 9 10 11 kubectl get pod,deploy,rs NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-nbtkw 0/1 Terminating 0 133m pod/nginx-deployment-6b474476c4-nkbrb 0/1 Terminating 0 133m pod/nginx-deployment-6b474476c4-zh5g7 0/1 Terminating 0 133m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 0/3 0 0 133m NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-6b474476c4 3 0 0 133m 比較差異 比較刪除前後的差異\nDeployment Deployment狀態的差異\n從 diff 的狀態可以看出來，在 metadata 的部分修改了 finalizers 並且指定了 foregroundDeletion 的模式表示使用 foreground 刪除模式，另外新增了 deletionTimestamp 的時間確定了物件的刪除時間。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 --- a/dpeloy.yaml +++ b/dpeloy.yaml - generation: 1 + deletionGracePeriodSeconds: 0 + deletionTimestamp: \u0026#34;2020-08-10T08:22:55Z\u0026#34; + finalizers: + - foregroundDeletion + generation: 2 namespace: default - resourceVersion: \u0026#34;1480766\u0026#34; + resourceVersion: \u0026#34;1499040\u0026#34; ... status: - availableReplicas: 3 conditions: - - lastTransitionTime: \u0026#34;2020-08-10T06:10:18Z\u0026#34; - lastUpdateTime: \u0026#34;2020-08-10T06:10:18Z\u0026#34; - message: Deployment has minimum availability. - reason: MinimumReplicasAvailable - status: \u0026#34;True\u0026#34; - type: Available ... - observedGeneration: 1 - readyReplicas: 3 - replicas: 3 - updatedReplicas: 3 + - lastTransitionTime: \u0026#34;2020-08-10T08:22:55Z\u0026#34; + lastUpdateTime: \u0026#34;2020-08-10T08:22:55Z\u0026#34; + message: Deployment does not have minimum availability. + reason: MinimumReplicasUnavailable + status: \u0026#34;False\u0026#34; + type: Available + observedGeneration: 2 + unavailableReplicas: 3 ReplicaSet 觀察ReplicaSet的變化也是確定了刪除的時間 deletionTimestamp 以及修改時間 time ，以及是透過 foregroundDeletion 的策略進行刪除。\n在狀態欄的地方也能看出現在狀態replicas的數量被縮減為0個。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 --- a/rs.yaml +++ b/rs.yaml creationTimestamp: \u0026#34;2020-08-10T06:09:12Z\u0026#34; - generation: 1 + deletionGracePeriodSeconds: 0 + deletionTimestamp: \u0026#34;2020-08-10T08:22:55Z\u0026#34; + finalizers: + - foregroundDeletion + generation: 2 ... - time: \u0026#34;2020-08-10T06:10:18Z\u0026#34; + time: \u0026#34;2020-08-10T08:22:55Z\u0026#34; ... ownerReferences: @@ -86,7 +87,7 @@ metadata: kind: Deployment name: nginx-deployment uid: 597d36f5-968a-4025-8621-b24f17f7f3a6 - resourceVersion: \u0026#34;1480765\u0026#34; + resourceVersion: \u0026#34;1499039\u0026#34; ... terminationGracePeriodSeconds: 30 status: - availableReplicas: 3 - fullyLabeledReplicas: 3 - observedGeneration: 1 - readyReplicas: 3 - replicas: 3 + observedGeneration: 2 + replicas: 0 Pod pod 的部分也想當的簡單，因為沒有子物件所以只要確定刪除時間 deletionTimestamp 以及修改時間 time 。\npod的狀態會被修改成 pending 以及相關的資源都會被移除例如: pod ip。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 --- a/pod.yaml +++ b/pod.yaml creationTimestamp: \u0026#34;2020-08-10T06:09:12Z\u0026#34; + deletionGracePeriodSeconds: 30 + deletionTimestamp: \u0026#34;2020-08-10T08:23:25Z\u0026#34; ... - time: \u0026#34;2020-08-10T06:10:16Z\u0026#34; + time: \u0026#34;2020-08-10T08:22:57Z\u0026#34; uid: 97fb6974-6882-4459-b6b0-b39357a7650b - resourceVersion: \u0026#34;1480754\u0026#34; + resourceVersion: \u0026#34;1499048\u0026#34; status: \u0026#34;True\u0026#34; type: Initialized - lastProbeTime: null - lastTransitionTime: \u0026#34;2020-08-10T06:10:16Z\u0026#34; - status: \u0026#34;True\u0026#34; + lastTransitionTime: \u0026#34;2020-08-10T08:22:57Z\u0026#34; + message: \u0026#39;containers with unready status: [nginx]\u0026#39; + reason: ContainersNotReady + status: \u0026#34;False\u0026#34; type: Ready - lastProbeTime: null - lastTransitionTime: \u0026#34;2020-08-10T06:10:16Z\u0026#34; - status: \u0026#34;True\u0026#34; + lastTransitionTime: \u0026#34;2020-08-10T08:22:57Z\u0026#34; + message: \u0026#39;containers with unready status: [nginx]\u0026#39; + reason: ContainersNotReady + status: \u0026#34;False\u0026#34; ... containerStatuses: - - containerID: docker://f20bbce2b58ac42426b61fc21e3f1a61938a51a4dc30277f50e9ac7aea88aa3d - image: nginx:1.14.2 - imageID: docker-pullable://nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d + - image: nginx:1.14.2 + imageID: \u0026#34;\u0026#34; lastState: {} name: nginx - ready: true + ready: false restartCount: 0 - started: true + started: false state: - running: - startedAt: \u0026#34;2020-08-10T06:10:16Z\u0026#34; + waiting: + reason: ContainerCreating hostIP: 172.18.0.5 - phase: Running - podIP: 10.32.0.5 - podIPs: - - ip: 10.32.0.5 + phase: Pending 實驗propagationPolicy (Background) deploy 部署測試的nginx deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 EOF deployment.apps/nginx-deployment created 狀態 取的Deployment ReplicaSet 以及Pod的狀態\n1 2 3 4 5 6 7 8 9 10 11 kubectl get deploy,rs,pod NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 55s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-6b474476c4 3 3 3 55s NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-4hjx8 1/1 Running 0 55s pod/nginx-deployment-6b474476c4-6qvvg 1/1 Running 0 55s pod/nginx-deployment-6b474476c4-plsn7 1/1 Running 0 55s 取得ReplicaSet與pod的ownerReferences用來確定物件之間的關係\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl get replicaset.apps/nginx-deployment-6b474476c4 -o go-template --template={{.metadata.ownerReferences}} [map[ apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:Deployment name:nginx-deployment uid:8a2be904-c306-426c-881e-c6914415c5fe]] kubectl get pod nginx-deployment-6b474476c4-6qvvg -o go-template --template={{.metadata.ownerReferences}} [map[ apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:ReplicaSet name:nginx-deployment-6b474476c4 uid:565540ad-fbf1-4d74-8841-f0475e12a200]] destroy background 1 2 3 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \\ -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Background\u0026#34;}\u0026#39; \\ -H \u0026#34;Content-Type: application/json\u0026#34; 狀態 取的deployment 以及pod的狀態\n從這個狀態可以看到所有的 pod 都在Terminating 的狀態，但是replicaset 以及 deployment 都先被移除。\n1 2 3 4 5 kubectl get pod,deploy,rs NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-4hjx8 0/1 Terminating 0 31m pod/nginx-deployment-6b474476c4-6qvvg 0/1 Terminating 0 31m pod/nginx-deployment-6b474476c4-plsn7 0/1 Terminating 0 31m 比較差異 deployment deployment狀態的差異\n可以看到deployment 直接被殺掉了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 --- a/dpeloy.yaml +++ b/dpeloy.yaml -apiVersion: apps/v1 -kind: Deployment -metadata: - annotations: - deployment.kubernetes.io/revision: \u0026#34;1\u0026#34; - kubectl.kubernetes.io/last-applied-configuration: | - {\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;nginx-deployment\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;replicas\u0026#34;:3,\u0026#34;selector\u0026#34;:{\u0026#34;matchLabels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}},\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}},\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;image\u0026#34;:\u0026#34;nginx:1.14.2\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;ports\u0026#34;:[{\u0026#34;containerPort\u0026#34;:80}]}]}}}} - creationTimestamp: \u0026#34;2020-08-10T10:05:25Z\u0026#34; - generation: 1 - labels: - app: nginx - managedFields: - - apiVersion: apps/v1 ... ... replicaset 可以看到 replicaset 也是直接被殺掉了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 --- a/rs.yaml +++ b/rs.yaml -apiVersion: apps/v1 -kind: ReplicaSet -metadata: - annotations: - deployment.kubernetes.io/desired-replicas: \u0026#34;3\u0026#34; - deployment.kubernetes.io/max-replicas: \u0026#34;4\u0026#34; - deployment.kubernetes.io/revision: \u0026#34;1\u0026#34; - creationTimestamp: \u0026#34;2020-08-10T10:05:25Z\u0026#34; - generation: 1 - labels: - app: nginx - pod-template-hash: 6b474476c4 - managedFields: - - apiVersion: apps/v1 - fieldsType: FieldsV1 - fieldsV1: - f:metadata: - f:annotations: - .: {} - f:deployment.kubernetes.io/desired-replicas: {} - f:deployment.kubernetes.io/max-replicas: {} - f:deployment.kubernetes.io/revision: {} ... ... pod 可以看到 pod 是緩慢的回收 ， 可以看到被設定了移除的時間以及相關狀態。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 --- a/pod.yaml +++ b/pod.yaml - time: \u0026#34;2020-08-10T10:05:25Z\u0026#34; + time: \u0026#34;2020-08-10T10:08:20Z\u0026#34; + deletionGracePeriodSeconds: 30 + deletionTimestamp: \u0026#34;2020-08-10T10:08:20Z\u0026#34; generateName: nginx-deployment-6b474476c4- - resourceVersion: \u0026#34;1513382\u0026#34; + resourceVersion: \u0026#34;1513717\u0026#34; ... - lastTransitionTime: \u0026#34;2020-08-10T10:08:20Z\u0026#34; - status: \u0026#34;True\u0026#34; + lastTransitionTime: \u0026#34;2020-08-10T10:08:20Z\u0026#34; + message: \u0026#39;containers with unready status: [nginx]\u0026#39; + reason: ContainersNotReady + status: \u0026#34;False\u0026#34; type: Ready - lastProbeTime: null - lastTransitionTime: \u0026#34;2020-08-10T10:08:20Z\u0026#34; - status: \u0026#34;True\u0026#34; + lastTransitionTime: \u0026#34;2020-08-10T10:08:20Z\u0026#34; + message: \u0026#39;containers with unready status: [nginx]\u0026#39; + reason: ContainersNotReady + status: \u0026#34;False\u0026#34; - - containerID: docker://bb34d6af8dbe1c72c423fece3d9d797ec8a5a0b62fd82f1f46bfcf5d67157be1 - image: nginx:1.14.2 - imageID: docker-pullable://nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d + - image: nginx:1.14.2 + imageID: \u0026#34;\u0026#34; lastState: {} name: nginx - ready: true + ready: false restartCount: 0 - started: true + started: false state: - running: - startedAt: \u0026#34;2020-08-10T10:08:20Z\u0026#34; + waiting: + reason: ContainerCreating hostIP: 172.18.0.5 - phase: Running - podIP: 10.32.0.6 - podIPs: - - ip: 10.32.0.6 + phase: Pending ... 實驗 propagation Policy (Orphan) deploy 部署測試的nginx deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 EOF deployment.apps/nginx-deployment created 狀態 取的deployment replicaset 以及pod的狀態\n1 2 3 4 5 6 7 8 9 kubectl get deploy,pod NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 4m44s NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-nbtkw 1/1 Running 0 4m44s pod/nginx-deployment-6b474476c4-nkbrb 1/1 Running 0 4m44s pod/nginx-deployment-6b474476c4-zh5g7 1/1 Running 0 4m44s 取得replicaset與pod的ownerReferences確定物件之間的關係\n1 2 3 4 5 6 7 kubectl get rs nginx-deployment-6b474476c4 -o go-template --template={{.metadata.ownerReferences}} [map[apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:Deployment name:nginx-deployment uid:b1d1a61d-8b51-4511-8c91-de44aaa2cdd0]] kubectl get pod nginx-deployment-6b474476c4-nbtkw -o go-template --template={{.metadata.ownerReferences}} [map[apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:ReplicaSet name:nginx-deployment-6b474476c4 uid:b052f80f-d72a-4e32-a4c4-f598274f2b07]] destroy Orphan 1 2 3 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \\ -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Orphan\u0026#34;}\u0026#39; \\ -H \u0026#34;Content-Type: application/json\u0026#34; 狀態 取的deployment 以及pod的狀態\n從這個狀態可以看到所有的 pod 都在 Running 的狀態，ReplicaSet 以及 Pod 都沒有被移除，只有 Deployment 被殺掉而已。\n1 2 3 4 5 6 7 8 9 kubectl get pod,deploy,rs NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-d6wns 1/1 Running 0 12m pod/nginx-deployment-6b474476c4-d7dtf 1/1 Running 0 12m pod/nginx-deployment-6b474476c4-m7rbz 1/1 Running 0 12m NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-6b474476c4 3 3 3 12m 小結 從上面三個實驗可以看到不同的移除方式會有不同的結果\nFront Ground\n需要等到關聯的子物件被刪除後才會進行清理的動作(打上deletionTimestamp)\nBackGround\n先刪除物件(打上deletionTimestamp)，再慢慢回收子物件(打上deletionTimestamp)\n3.Orphan\n直接把物件刪除（打上deletionTimestamp），所有子物件不做任何動作。\n","description":"","id":61,"section":"posts","tags":["kubernetes"],"title":"學習Kubernetes Garbage Collection機制","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/kubernetes-garbage-collection/"},{"content":" Istio 簡介 Istio提供了一個完整的微服務應用解決方案，透過為整個服務網路提供行為監控和細粒度的控制來滿足微服務應用程序的多樣化需求。\nIstio提供了非常簡單的方式建立具有監控(monitor)、負載平衡(load balance)、服務間的認證（service-to-service authentication）\u0026hellip;等功能的網路功能，而不需要對服務的程式碼進行任何修改。\n環境配置 本次安裝是使用三台Bare Metal去作部署，作業系統採用的是Ubuntu 16.04 LTS版。\nKubernetes Role RAM CPUs IP Address Master 16G 8Cores 10.20.0.154 Node1 16G 8Cores 10.20.0.164 Node2 16G 8Cores 10.20.0.174 這邊利用的是kubeadm進行kubernetes的安裝『版本是Kubernetes1.11』，可以參考官方網站的部屬方式。\n安裝 Kubernetes latest version and docker and other package dependencies:\n1 2 3 4 5 6 7 8 $apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl $curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo -E apt-key add - $cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF $sudo apt-get update $sudo apt-get install -y docker.io kubelet kubeadm kubectl Kubernetes v1.8+ 要求關閉系統 Swap，如果不想關閉系統的Swap需要修改 kubelet 設定參數，我們利用以下指令關閉系統Swap：\n1 $swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0 透過以下指令啟動Docker Daemon。\n1 $systemctl enable docker \u0026amp;\u0026amp; systemctl start docker 將橋接的IPv4流量傳遞給iptables\n1 2 3 4 5 6 $cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $sysctl -p /etc/sysctl.d/k8s.conf 在master節點上使用kubeadm進行kubernetes叢集的初始化\n1 $sudo kubeadm init --pod-network-cidr=192.168.0.0/16 會得到下列輸出，我們利用下列輸出的資訊將其他節點加入叢集。\n1 2 3 4 You can now join any number of machines by running the following on each node as root: kubeadm join 172.24.0.3:6443 --token i67sjb.0nvjxbldwuh342of --discovery-token-ca-cert-hash sha256:aa23e1e7a4d55d06fbdf34fa2a1c703dd7e7cfff735b0b0fe800b4335aff68b5 在其他節點上我們可以利用以下指令加入叢集。\n1 $kubeadm join 172.24.0.3:6443 --token i67sjb.0nvjxbldwuh342of --discovery-token-ca-cert-hash sha256:aa23e1e7a4d55d06fbdf34fa2a1c703dd7e7cfff735b0b0fe800b4335aff68b5 在master節點設定kube config。\n1 2 3 $mkdir -p $HOME/.kube $sudo -H cp /etc/kubernetes/admin.conf $HOME/.kube/config $sudo -H chown $(id -u):$(id -g) $HOME/.kube/config 在master 安裝Kubernetes CNI，這邊採用的是Calico。\n1 2 $kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml $kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml 等所有的pod都完成，在master上操作kubectl指令即可看到所有node呈現ready的狀態\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE calico-node-cgtxh 2/2 Running 0 1m calico-node-qjrbm 2/2 Running 0 1m calico-node-v59b2 2/2 Running 0 2m coredns-78fcdf6894-dz9fs 1/1 Running 0 4m coredns-78fcdf6894-mn6k8 1/1 Running 0 4m etcd-master 1/1 Running 0 3m kube-apiserver-master 1/1 Running 0 3m kube-controller-manager-master 1/1 Running 0 3m kube-proxy-5xj2l 1/1 Running 0 4m kube-proxy-bh7wb 1/1 Running 0 1m kube-proxy-jqpqg 1/1 Running 0 1m kube-scheduler-master 1/1 Running 0 3m $kubectl get node NAME STATUS ROLES AGE VERSION master Ready master 4m v1.11.1 node-1 Ready \u0026lt;none\u0026gt; 2m v1.11.1 node-2 Ready \u0026lt;none\u0026gt; 2m v1.11.1 在master節點上，下載並且安裝helm\n1 2 3 $wget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz $tar zxvf helm-v2.9.1-linux-amd64.tar.gz $mv linux-amd64/helm /usr/bin 為kubernetes Helm 建立Tiller Service Account以及綁定Cluster-Admin Role，最後在初始化helm 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $kubectl create serviceaccount tiller --namespace kube-system $cat \u0026lt;\u0026lt;EOF | kubectl create -f - kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects: - kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026#34;\u0026#34; EOF $helm init --service-account tiller Creating /root/.helm Creating /root/.helm/repository Creating /root/.helm/repository/cache Creating /root/.helm/repository/local Creating /root/.helm/plugins Creating /root/.helm/starters Creating /root/.helm/cache/archive Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com Adding local repo with URL: http://127.0.0.1:8879/charts HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure \u0026#39;allow unauthenticated users\u0026#39; policy. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! 完後成後可透過kubectl指令確認\n1 2 3 4 5 6 $kubectl get pod,svc -l app=helm -n kube-system NAME READY STATUS RESTARTS AGE pod/tiller-deploy-759cb9df9-b7n7j 1/1 Running 0 4m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/tiller-deploy ClusterIP 10.107.71.110 \u0026lt;none\u0026gt; 44134/TCP 4m 安裝Istio 透過官方提供的腳本，下載Istio並安裝istioctl binary\n1 2 3 $curl -L https://git.io/getLatestIstio | sh - $cd istio-1.0.0/ $cp bin/istioctl /usr/bin/ 在helm version 2.10.0以前的版本Istio還是需要手安裝Istio CRD\n1 2 $kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml $kubectl apply -f install/kubernetes/helm/istio/charts/certmanager/templates/crds.yaml 透過kubectl指令檢查Istio是否安裝成功\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 $kubectl get pod -n istio-system NAME READY STATUS RESTARTS AGE istio-citadel-7d8f9748c5-zd4vb 1/1 Running 0 4m istio-egressgateway-676c8546c5-fq55b 1/1 Running 0 4m istio-galley-5669f7c9b-q98ld 1/1 Running 0 4m istio-ingressgateway-5475685bbb-5jfwv 1/1 Running 0 4m istio-pilot-5795d6d695-2vfq9 2/2 Running 0 4m istio-policy-7f945bf487-brtxn 2/2 Running 0 4m istio-sidecar-injector-d96cd9459-ws647 1/1 Running 0 4m istio-statsd-prom-bridge-549d687fd9-mb99f 1/1 Running 0 4m istio-telemetry-6c587bdbc4-tzblq 2/2 Running 0 4m prometheus-6ffc56584f-xcpsj 1/1 Running 0 4m root@sdn-k8s-b4:/home/ubuntu|⇒ kubectl get pod,svc NAME READY STATUS RESTARTS AGE pod/app-debug-6b4f85c9cc-gq7nx 1/1 Running 1 28d pod/config 1/1 Running 0 1d pod/hello-55f998cd56-d2q8n 1/1 Running 0 1d pod/hellogo-bb9bd67f7-ckmkc 1/1 Running 0 1d pod/myapp-pod 0/1 Completed 0 1d pod/myapp-pod2 0/1 Completed 0 1d pod/network-controller-server-tcp-7x6vm 1/1 Running 0 1d pod/network-controller-server-tcp-kjtm9 1/1 Running 0 1d pod/network-controller-server-unix-2rnfv 1/1 Running 0 1d pod/network-controller-server-unix-lh8qh 1/1 Running 0 1d pod/nginx-6f858d4d45-vrlgb 1/1 Running 0 1d pod/onos-65df85486f-tvk5t 1/1 Running 0 1d pod/skydive-agent-9x2n4 1/1 Running 0 6h pod/skydive-agent-rww57 1/1 Running 0 6h pod/skydive-analyzer-5f9556687f-gvdbj 2/2 Running 0 6h $kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-citadel ClusterIP 10.106.166.113 \u0026lt;none\u0026gt; 8060/TCP,9093/TCP 5m istio-egressgateway NodePort 10.100.183.7 \u0026lt;none\u0026gt; 80:31607/TCP,443:31053/TCP 5m istio-galley ClusterIP 10.109.104.146 \u0026lt;none\u0026gt; 443/TCP,9093/TCP 5m istio-ingressgateway NodePort 10.101.66.117 \u0026lt;none\u0026gt; 80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:32388/TCP,8060:32100/TCP, 15030:30847/TCP,15031:32749/TCP 5m istio-pilot ClusterIP 10.102.202.205 \u0026lt;none\u0026gt; 15010/TCP,15011/TCP,8080/TCP,9093/TCP 5m istio-policy ClusterIP 10.97.181.32 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,9093/TCP 5m istio-sidecar-injector ClusterIP 10.96.165.139 \u0026lt;none\u0026gt; 443/TCP 5m istio-statsd-prom-bridge ClusterIP 10.101.82.72 \u0026lt;none\u0026gt; 9102/TCP,9125/UDP 5m istio-telemetry ClusterIP 10.108.94.224 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,9093/TCP,42422/TCP 5m prometheus ClusterIP 10.104.3.226 \u0026lt;none\u0026gt; 9090/TCP 5m 範例：Bookinfo Application 這邊示範Istio官方提供的範例：Bookinfo Application\n1 2 3 4 5 6 7 8 9 10 11 $kubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml) service/details created deployment.extensions/details-v1 created service/ratings created deployment.extensions/ratings-v1 created service/reviews created deployment.extensions/reviews-v1 created deployment.extensions/reviews-v2 created deployment.extensions/reviews-v3 created service/productpage created deployment.extensions/productpage-v1 created 透過kubectl指令確認安裝的pod及service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-fc9649d9c-tqbcn 2/2 Running 0 1m productpage-v1-58845c779c-2lqxg 2/2 Running 0 27m ratings-v1-6cc485c997-zqvqt 2/2 Running 0 1m reviews-v1-76987687b7-hfrn2 2/2 Running 0 1m reviews-v2-86749dcd5-ffmvs 2/2 Running 0 1m reviews-v3-7f4746b959-zr4ml 2/2 Running 0 1m $kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.101.208.129 \u0026lt;none\u0026gt; 9080/TCP 1m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 27m productpage ClusterIP 10.101.11.13 \u0026lt;none\u0026gt; 9080/TCP 1m ratings ClusterIP 10.105.132.197 \u0026lt;none\u0026gt; 9080/TCP 1m reviews ClusterIP 10.103.199.76 \u0026lt;none\u0026gt; 9080/TCP 1m 建立一個Gateway讓叢及外部可以存取\n1 $kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml 接著我們透過瀏覽器去存取我們服務，在網址的地方輸入http://\u0026lt;node or master ip\u0026gt;:31380/productpage，網頁會顯下圖的網站內容。\nBookinfo Application 網頁服務\n不斷重新整理該服務的網頁，會發現網頁上的星星的部分有改變。從沒星星==\u0026gt;黑星星==\u0026gt;紅星星切換，分別對應到pod中的三個版本的review,預設的載平衡功能是輪詢（Round Robin）\n設定destination rule，此時還是輪詢法\n1 $kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml Intelligent Routing Istio 提供了智能路由（Intelligent Routing），這邊示範如何使用Istio管理各種服務的流量\n依照版本進行路由管理 1 2 3 4 5 $kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml virtualservice.networking.istio.io/productpage created virtualservice.networking.istio.io/reviews created virtualservice.networking.istio.io/ratings created virtualservice.networking.istio.io/details created 這時候我們再回到瀏覽器查看Bookinfo Application 網頁服務，不管重新整理多少次網頁，都看不到網頁上有星星的標示，因為此時所有的請求都被轉送到review v1版本上\n依照用戶進行路由管理 1 2 $kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml virtualservice.networking.istio.io/reviews create 我們回到瀏覽器上，按下右上角的Sign in 的按鈕。帳號密碼都為jason，登入後會發現不管怎麼更新頁面，網頁上都是呈現黑星星的標示。\n當我們登出後，也進行更新頁面的動作，網頁上不會出現任何星星的標示。\n因為當我們登入jason後，所有的路由都請求都會被轉發到review v2版本上。\nFault injection 有時候我們的程式碼裡面會有bug，可以透過注入故障的方式發現這些潛伏在裡面的bug。\nBookinfo特別示範一個http延遲的例子，為了測BookInfo的微服務，在reviews:v2和ratings以及用戶jason之間注入七秒延遲。此測試將發Bookinfo故意塞進去的一個錯誤。\n剛剛的路由規則，如果已經被您刪除掉。請再利用以下指令加回他的路由規則。\n1 2 $kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml $kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml 使用以下指令注入一個http delay ，該指令在reviews:v2和ratings以及用戶jason之間注入七秒http delay\n1 $kubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml 這時我們去存取網頁會拋出一個異常的錯誤，因為我們把服務與服務之間的存取的時間拉長，我們可以透過注入錯誤的方式發現一個存取延遲的bug。\n注入延遲後的Bookinfo Application網頁服務\n結論 玩過 istio 之後發現功能十分強大，但架構過於複雜當有問題出現時，維運與開發人員難以排查狀況與問題的發生點，但目前 istio 還在 1.0 版本 未來發展起來應該是一頭猛獸 ，會持續關注 service mesh 的相關議題。\nistio 背後撐腰的公司非常可怕 ，可以觀察這個專案後續的走向，作為學習的方向！！\n","description":"","id":67,"section":"posts","tags":["servicemesh","kubernetes"],"title":"service mesh 之 Istio 1.0 安裝及極度簡易操作","uri":"https://blog.jjmengze.website/zh-tw/posts/istio/istio1.0-install/"},{"content":" Serverless 在進入Kubeless之前先科普一下什麼是Serverless。\n就字面上的意思來說就是沒有Server，那沒有Server又是什麼意思？\n先從設計開發到部署開始！\n我們想要做出一套系統軟體，勢必會經歷設計開發部署\u0026hellip;等流程。\n最後總會部署到單台或是多台的Server上面，當把服務架設到Server上時有許多方案可以提供我們參考。\n例如: Google 所提供的 GKE，AWS 所提供的 EC2 亦或是 Microsoft 所提供的 Azure。\n選擇完使用哪個雲服務商所提供的Cloud後，還要考慮這套系統究竟需要多少台Server、多大空間Stroage\u0026hellip;等問題。\n在微服務的架構之下，我們的每個服務會運行在各個Server上（先不考慮Container XD）\n在你的業務擴展後，我們就需要更多更多的資源，也就是更多更多的Server。\n很多微服務事實上被call到的機會很少，但他還是佔了一台Server的資源及空間。同時這些很少被使用到的微服務也一點一滴的花掉你的錢（開Server可是要錢的啊！！！）\n有了Serverless又或是稱為FaaS的出現後，我們再也不需要去關心那些Server，只要把微服務source code提交給雲服務商，雲服務商能提供一個運行的環境給你，總而言之就是開發者不需要再去關心底下Infrastructure Layer。\nKubeless 簡介 Kubeless是一個Kubernetes-native 無伺服器(Serverless)框架，只要撰寫程式（Function），而不需要了解底層基礎建設（Infrastructure Layer）。Kubeless利用Kubernetes資源提供自動擴展，API路由，監控，故障排除\u0026hellip;等功能。\n環境配置 本次安裝是使用OpenStack上的三台虛擬機器去作部署，作業系統採用的是Ubuntu 16.04 LTS版。\nKubernetes Role RAM CPUs IP Address Master 8G 4Cores 172.24.0.2 Node1 8G 4Cores 172.24.0.2 Node2 8G 4Cores 172.24.0.4 部署kubernetes 這邊利用的是kubeadm進行kubernetes的安裝『版本是Kubernetes1.10』，可以參考官方網站的部屬方式。\n安裝套件 安裝 Kubernetes latest version and other dependencies:\n1 2 3 4 5 6 7 $curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - $cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF $sudo apt-get update $sudo apt-get install -y docker.io kubelet kubeadm kubectl 關閉Swap Kubernetes v1.8+ 要求關閉系統 Swap，如果不想關閉系統的Swap需要修改 kubelet 設定參數，我們利用以下指令關閉系統Swap：\n1 $swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0 重啟Docker 透過以下指令啟動Docker Daemon。\n1 $systemctl enable docker \u0026amp;\u0026amp; systemctl start docker IPv4 forward設定 將橋接的IPv4流量傳遞給iptables\n1 2 3 4 5 6 $cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $sysctl -p /etc/sysctl.d/k8s.conf Cluster 初始化 在master節點上使用kubeadm進行kubernetes叢集的初始化\n1 $sudo kubeadm init --pod-network-cidr=192.168.0.0/16 1 $sudo kubeadm init --pod-network-cidr=192.168.0.0/16 會得到下列輸出，我們利用下列輸出的資訊將其他節點加入叢集。\n1 2 3 4 5 ... You can now join any number of machines by running the following on each node as root: kubeadm join 172.24.0.2:6443 --token i67sjb.0nvjxbldwuh342of --discovery-token-ca-cert-hash sha256:aa23e1e7a4d55d06fbdf34fa2a1c703dd7e7cfff735b0b0fe800b4335aff68b5 在其他節點上我們可以利用以下指令加入叢集。\n1 $kubeadm join 172.24.0.2:6443 --token i67sjb.0nvjxbldwuh342of --discovery-token-ca-cert-hash sha256:aa23e1e7a4d55d06fbdf34fa2a1c703dd7e7cfff735b0b0fe800b4335aff68b5 在master節點設定kube config。\n1 2 3 $mkdir -p $HOME/.kube $sudo -H cp /etc/kubernetes/admin.conf $HOME/.kube/config $sudo -H chown $(id -u):$(id -g) $HOME/.kube/config 安裝Calico CNI 1 2 $kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml $kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml 檢查環境 在master上操作kubectl指令即可看到所有node呈現ready的狀態\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE calico-node-cgtxh 2/2 Running 0 1m calico-node-qjrbm 2/2 Running 0 1m calico-node-v59b2 2/2 Running 0 2m coredns-78fcdf6894-dz9fs 1/1 Running 0 4m coredns-78fcdf6894-mn6k8 1/1 Running 0 4m etcd-master 1/1 Running 0 3m kube-apiserver-master 1/1 Running 0 3m kube-controller-manager-master 1/1 Running 0 3m kube-proxy-5xj2l 1/1 Running 0 4m kube-proxy-bh7wb 1/1 Running 0 1m kube-proxy-jqpqg 1/1 Running 0 1m kube-scheduler-master 1/1 Running 0 3m $kubectl get node NAME STATUS ROLES AGE VERSION master Ready master 4m v1.10.0 node-1 Ready \u0026lt;none\u0026gt; 2m v1.10.0 node-2 Ready \u0026lt;none\u0026gt; 2m v1.10.0 安裝Kubeless 安裝CLI 安裝操作 Kubeless 的 CLI\n1 2 3 4 $wget https://github.com/kubeless/kubeless/releases/download/v1.0.0-alpha.7/kubeless_linux-amd64.zip apt install -y unzip unzip kubeless_linux-amd64.zip $sudo mv ~/bundles/kubeless_linux-amd64 /usr/local/bin/ 檢查CLI是否安裝完成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Serverless framework for Kubernetes Usage: kubeless [command] Available Commands: autoscale manage autoscale to function on Kubeless completion Output shell completion code for the specified shell. function function specific operations get-server-config Print the current configuration of the controller help Help about any command topic manage message topics in Kubeless trigger trigger specific operations version Print the version of Kubeless Flags: -h, --help help for kubeless Use \u0026#34;kubeless [command] --help\u0026#34; for more information about a command. 安裝Kubeless Kubeless官方針對不同Kubernetes環境提供了多種腳本（非RBAC，RBAC和openshift），這邊我採用的是有RBAC的部署。\n1 2 3 4 5 6 7 8 9 10 11 12 13 $kubectl create ns kubeless namespace/kubeless created $kubectl create -f https://github.com/kubeless/kubeless/releases/download/v1.0.0-alpha.8/kubeless-v1.0.0-alpha.8.yaml configmap/kubeless-config created deployment.apps/kubeless-controller-manager created serviceaccount/controller-acct created clusterrole.rbac.authorization.k8s.io/kubeless-controller-deployer created clusterrolebinding.rbac.authorization.k8s.io/kubeless-controller-deployer created customresourcedefinition.apiextensions.k8s.io/functions.kubeless.io created customresourcedefinition.apiextensions.k8s.io/httptriggers.kubeless.io created customresourcedefinition.apiextensions.k8s.io/cronjobtriggers.kubeless.io created 可以透過kubetl指令觀察kubeless是否有被部署起來\n1 2 3 4 5 6 7 8 9 10 11 12 kubectl get pods -n kubeless NAME READY STATUS RESTARTS AGE kubeless-controller-manager-66868fb689-77fkp 3/3 Running 0 1m $kubectl get deployment -n kubeless NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kubeless-controller-manager 1 1 1 1 2m $kubectl get crd NAME CREATED AT cronjobtriggers.kubeless.io 2018-08-24T12:44:57Z functions.kubeless.io 2018-08-24T12:44:57Z httptriggers.kubeless.io 2018-08-24T12:44:57Z ### 安裝Kubeless UI 官方提供了操作Kubeless的Dashboard，可以透過該介面操作。 ```bash $kubectl create -f https://raw.githubusercontent.com/kubeless/kubeless-ui/master/k8s.yaml 可以透過kubetl指令觀察Kubeless的Dashboard是否有被部署起來\n1 2 3 4 5 6 7 kubectl -n kubeless get pod,svc NAME READY STATUS RESTARTS AGE pod/kubeless-controller-manager-66868fb689-77fkp 3/3 Running 0 39m pod/ui-6d868664c5-kr99m 2/2 Running 0 1m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ui NodePort 10.100.15.46 \u0026lt;none\u0026gt; 3000:30722/TCP 1m 可以透Browser瀏覽 Kubeless UI(172.24.0.2:30722)\nKubeless Dashboard\n建立及測試Function 這邊示範一個簡單的質數判斷的Function，以Go為例。\n這邊會示範從UI上操作以及使用CLI操作\nUI操作Kubeless 透過Browser連進Kubeless Dashboard 後點選 Create Function\n建立一個Function\n按下Create後，就可以撰寫我們判斷質數的程式碼。\n程式的撰寫畫面會如下圖所示\n在function內撰寫我們想要的功能\n撰寫完，質數判斷的function大致上是這個樣子\n質數判斷的function\n測試Function 接著可以輸入右側的Request，填入你想要判斷的數值。如：177,9487\u0026hellip;等數值。\n填完數值後按下Run fuction，發現底下會沒有輸出！！！\n因為kubeless UI有RBAC的問題，這邊偷懶直接給cluster-admin。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: kubeless-ui-default roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: User name: system:serviceaccount:kubeless:ui-acct apiGroup: rbac.authorization.k8s.io EOF 這邊還會產生另外一個問題，在Kubeless UI 所產生的Request只能是String型態的。\n在這個範例之中會看到以下輸出。\n只能得到must be a positive integer的回應\n要解決這個問題我們可以藉由Kubeless CLI發送請求給特定的function。\n1 2 3 4 5 6 $kubeless function call checkprime --data 9487 非質數 $kubeless function call checkprime --data 177 非質數 $kubeless function call checkprime --data 7 質數 結束實驗後我們可以透過Kubeless UI刪除掉剛剛建立的checkprime function\nCLI操作Kubeless 建立一個checkprime.go，並且撰寫質數判斷的function。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package kubeless import ( \u0026#34;strconv\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/kubeless/kubeless/pkg/functions\u0026#34; ) func IsPrime(event functions.Event, context functions.Context) (string, error) { num, err := strconv.Atoi(event.Data) if err != nil { fmt.Println(err) return \u0026#34;must be a positive integer\u0026#34;, nil } if num \u0026lt;= 1 { return \u0026#34;must be a positive integer\u0026#34;, nil } for i := 2; i \u0026lt; num; i++ { if num%i == 0 { return \u0026#34;非質數\u0026#34;, nil break } } return \u0026#34;質數\u0026#34;, nil } 透過kubeless CLI幫我們建立Function到環境上\n1 2 3 4 $kubeless function deploy checkprime --runtime go1.10 --from-file checkprime.go --handler checkprime.IsPrime INFO[0000] Deploying function... INFO[0000] Function checkprime submitted for deployment INFO[0000] Check the deployment status executing \u0026#39;kubeless function ls checkprime\u0026#39; 透過Kubeless CLI 、 Kubectl 去確認Function 有沒有成功的被建立到環境上：\n1 2 3 4 5 6 7 8 9 10 11 $kubectl get functions NAME AGE checkprime 1m $kubeless function ls NAME NAMESPACE\tHANDLER RUNTIME\tDEPENDENCIES\tSTATUS checkprime\tdefault checkprime.IsPrime\tgo1.10 1/1 READY $kubectl get po NAME READY STATUS RESTARTS AGE checkprime-7d4f$b7b64c-k47w9 1/1 Running 0 37s 測試Function 透過Kubeless CLI去測試剛剛撰寫的質數判斷是否正確\n1 2 3 4 5 6 $kubeless function call checkprime --data 9487 非質數 $kubeless function call checkprime --data 177 非質數 $kubeless function call checkprime --data 7 質數 另外也能透過kubernetes把該function proxy 出來進行測試\n1 2 3 4 5 6 7 8 9 $kubectl proxy -p 34567 \u0026amp; [1] 3533 Starting to serve on 127.0.0.1:34567 $curl --data 9487\\ --header \u0026#34;Content-Type:application/json\u0026#34; \\ localhost:34567/api/v1/namespaces/default/services/checkprime:8080/proxy/ 非質數 結束實驗後我們可以透過Kubeless CLI刪除掉剛剛建立的checkprime function\n1 2 3 4 $kubeless function delete checkprime $kubeless function ls NAME\tNAMESPACE\tHANDLER\tRUNTIME\tDEPENDENCIES\tSTATUS 小結 還有很多開源的 FAAS 框架可以直接套用在 kubernetes 上，本篇文章只針對 kubeless 做一個非常簡單的 Demo ，有機會的話還會玩玩看 Knative , fission \u0026hellip;等框架。\n","description":"","id":68,"section":"posts","tags":["kubernetes","serverless"],"title":"FAAS之kubeless的新滋味","uri":"https://blog.jjmengze.website/zh-tw/posts/kubeless/kubeless/"},{"content":" kolla是為了提供production-ready的 OpenStack Cloud 之container和deployment tools。\n環境配置 本次安裝是使用三台Bare Metal去作部署，作業系統採用的是Ubuntu 16.04 LTS版。\nKubernetes Role OpeStack Role RAM CPUs IP Address Master Controller 16G 8Cores 10.0.0.190 Node1 Compute1 16G 8Cores 10.0.0.191 Node2 Compute2 16G 8Cores 10.0.0.192 部署kubernetes 這邊利用的是kubeadm進行kubernetes的安裝『版本是Kubernetes1.10』，可以參考官方網站的部屬方式。\n安裝 Kubernetes latest version and other dependencies:\n1 2 3 4 5 6 7 8 9 $curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo -E apt-key add - $cat \u0026lt;\u0026lt;EOF \u0026gt; kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF $sudo cp -aR kubernetes.list /etc/apt/sources.list.d/kubernetes.list $sudo apt-get update $sudo apt-get install -y docker.io kubelet kubeadm kubectl Kubernetes v1.8+ 要求關閉系統 Swap，如果不想關閉系統的Swap需要修改 kubelet 設定參數，我們利用以下指令關閉系統Swap：\n1 $swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0 透過以下指令啟動Docker Daemon。\n1 $systemctl enable docker \u0026amp;\u0026amp; systemctl start docker 確認Docker是否支援Cgroup Driver或是Systemd Driver，進一步修改 kubelet 設定參數。\n1 2 $CGROUP_DRIVER=$(sudo docker info | grep \u0026#34;Cgroup Driver\u0026#34; | awk \u0026#39;{print $3}\u0026#39;) $sudo sed -i \u0026#34;s|KUBELET_KUBECONFIG_ARGS=|KUBELET_KUBECONFIG_ARGS=--cgroup-driver=$CGROUP_DRIVER |g\u0026#34; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 將橋接的IPv4流量傳遞給iptables\n1 2 3 4 5 6 $cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $sysctl -p /etc/sysctl.d/k8s.conf 設置Kubernetes Server CIDR的DNS位置。\n1 $sudo sed -i \u0026#39;s/10.96.0.10/10.3.3.10/g\u0026#39; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 利用以下重新載入kubelet相關的啟動參數\n1 2 3 4 $sudo systemctl daemon-reload $sudo systemctl stop kubelet $sudo systemctl enable kubelet $sudo systemctl start kubelet 在master節點上使用kubeadm進行kubernetes 叢集的初始化\n1 $sudo kubeadm init --feature-gates CoreDNS=true --pod-network-cidr=10.1.0.0/16 --service-cidr=10.3.3.0/24 會得到下列輸出，我們利用下列輸出的資訊將其他節點加入叢集。\n1 2 3 4 You can now join any number of machines by running the following on each node as root: kubeadm join 10.0.0.181:6443 --token pieol0.2kfzpwhosxuqhe6t --discovery-token-ca-cert-hash sha256:e55b423135642404ffc60bcae4793732f18b4ce2866a8419c87b7dd92724a481 在其他節點上我們可以利用以下指令加入叢集。\n1 $kubeadm join 10.0.0.181:6443 --token pieol0.2kfzpwhosxuqhe6t --discovery-token-ca-cert-hash sha256:e55b423135642404ffc60bcae4793732f18b4ce2866a8419c87b7dd92724a481 在master節點設定kube config。\n1 2 3 $mkdir -p $HOME/.kube $sudo -H cp /etc/kubernetes/admin.conf $HOME/.kube/config $sudo -H chown $(id -u):$(id -g) $HOME/.kube/config 在master 安裝Kubernetes CNI，這邊採用的是Canal。\n1 2 3 4 5 $wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/canal/rbac.yaml $kubectl apply -f rbac.yaml $wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/canal/canal.yaml $sed -i \u0026#34;s@10.244.0.0/16@10.1.0.0/16@\u0026#34; canal.yaml $kubectl apply -f canal.yaml 因為我們要把kubernetes master當作openstack controller的角色，我們將kubernetes master 的節點污染拿掉。\n1 $kubectl taint nodes --all=true node-role.kubernetes.io/master:NoSchedule- 在CNI安裝完成後可透過下列指令來檢查，Node \u0026amp; Pod是否都準備好了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $kubectl get pod,node -o wide --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/canal-297sw 3/3 Running 0 2m kube-system pod/canal-f82qj 3/3 Running 0 2m kube-system pod/canal-zxfbk 3/3 Running 0 2m kube-system pod/coredns-7997f8864c-cglcr 1/1 Running 0 9m kube-system pod/coredns-7997f8864c-sxf2c 1/1 Running 0 9m kube-system pod/etcd-node1 1/1 Running 0 8m kube-system pod/kube-apiserver-node1 1/1 Running 0 8m kube-system pod/kube-controller-manager-node1 1/1 Running 0 8m kube-system pod/kube-proxy-6gwws 1/1 Running 0 6m kube-system pod/kube-proxy-9xbdq 1/1 Running 0 6m kube-system pod/kube-proxy-z54k4 1/1 Running 0 9m kube-system pod/kube-scheduler-node1 1/1 Running 0 8m NAME STATUS ROLES AGE VERSION node1 Ready master 8m v1.10.3 node2 Ready master 8m v1.10.3 node3 Ready master 8m v1.10.3 利用BusyBox ，驗證Kubernetes 環境例如DNS 是否有通。\n1 2 3 4 5 6 7 8 9 $kubectl run -i -t $(uuidgen) --image=busybox --restart=Never $nslookup kubernetes Server: 10.3.3.10 Address 1: 10.3.3.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1: 10.3.3.1 kubernetes.default.svc.cluster.local 為kubernetes Helm 建立Tiller Service Account以及綁定Cluster-Admin Role\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $kubectl create serviceaccount tiller --namespace kube-system $cat \u0026lt;\u0026lt;EOF | kubectl create -f - kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects: - kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026#34;\u0026#34; EOF 接著我們安裝Kubernetes Helm，用來管理Helm package的元件。\n1 2 3 4 $curl -L https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; get_helm.sh $chmod 700 get_helm.sh $./get_helm.sh $helm init --service-account tiller 由於kolla kubernetes 需要用到ansible git ，我們在master的節點需要安裝ansible ，其他節點需安裝python。\n1 2 3 $sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y software-properties-common git python python-pip $sudo apt-add-repository -y ppa:ansible/ansible $sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y ansible 建立一個資料夾方便我們接下來的步驟\n1 2 $mkdir kolla-bringup $cd kolla-bringup 下載 kolla-kubernetes \u0026amp; kolla ansible\n1 2 3 4 5 6 7 $git clone http://github.com/openstack/kolla-ansible $git clone http://github.com/openstack/kolla-kubernetes $cd kolla-kubernetes $git checkout 22ed0c232d7666afb6e288001b8814deea664992 $cd ../kolla-ansible $git checkout origin/stable/pike $cd .. 使用pip 安裝kolla-kubernetes \u0026amp; kolla-ansible所需要的套件\n1 $sudo pip install -U kolla-ansible/ kolla-kubernetes 把相關設定檔製到/etc底下\n1 2 $cp -Ra kolla-kubernetes/etc/kolla/ /etc $cp -Ra kolla-kubernetes/etc/kolla-kubernetes/ /etc pip剛剛安裝的項目這時候可以幫我們生成default passwords\n1 $sudo kolla-kubernetes-genpwd 建立一個Kubernetes namespaces來隔離Kolla deployment\n1 $kubectl create namespace kolla 使用Label標記要成為Controller及Compute的節點，我們這邊將node1當成Controller，node2 node3當成Compute\n1 2 3 $kubectl label node node1 kolla_controller=true $kubectl label node node2 kolla_compute=true $kubectl label node node3 kolla_compute=true 將/etc/kolla/globals.yml設定檔，修改成與自己環境相符\n將/etc/kolla/globals.yml中的network_interface設置為Management interface name。例如：eth1\n將/etc/kolla/globals.yml中的neutron_external_interface設置為neutron external interface name。例如：eth2\n將相關的openstack設定加入/etc/kolla/globals.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 $cat \u0026lt;\u0026lt;EOF \u0026gt; add-to-globals.yml kolla_install_type: \u0026#34;source\u0026#34; tempest_image_alt_id: \u0026#34;{{ tempest_image_id }}\u0026#34; tempest_flavor_ref_alt_id: \u0026#34;{{ tempest_flavor_ref_id }}\u0026#34; neutron_plugin_agent: \u0026#34;openvswitch\u0026#34; api_interface_address: 0.0.0.0 tunnel_interface_address: 0.0.0.0 orchestration_engine: KUBERNETES memcached_servers: \u0026#34;memcached\u0026#34; keystone_admin_url: \u0026#34;http://keystone-admin:35357/v3\u0026#34; keystone_internal_url: \u0026#34;http://keystone-internal:5000/v3\u0026#34; keystone_public_url: \u0026#34;http://keystone-public:5000/v3\u0026#34; glance_registry_host: \u0026#34;glance-registry\u0026#34; neutron_host: \u0026#34;neutron\u0026#34; keystone_database_address: \u0026#34;mariadb\u0026#34; glance_database_address: \u0026#34;mariadb\u0026#34; nova_database_address: \u0026#34;mariadb\u0026#34; nova_api_database_address: \u0026#34;mariadb\u0026#34; neutron_database_address: \u0026#34;mariadb\u0026#34; cinder_database_address: \u0026#34;mariadb\u0026#34; ironic_database_address: \u0026#34;mariadb\u0026#34; placement_database_address: \u0026#34;mariadb\u0026#34; rabbitmq_servers: \u0026#34;rabbitmq\u0026#34; openstack_logging_debug: \u0026#34;True\u0026#34; enable_heat: \u0026#34;no\u0026#34; enable_cinder: \u0026#34;yes\u0026#34; enable_cinder_backend_lvm: \u0026#34;yes\u0026#34; enable_cinder_backend_iscsi: \u0026#34;yes\u0026#34; enable_cinder_backend_rbd: \u0026#34;no\u0026#34; enable_ceph: \u0026#34;no\u0026#34; enable_elasticsearch: \u0026#34;no\u0026#34; enable_kibana: \u0026#34;no\u0026#34; glance_backend_ceph: \u0026#34;no\u0026#34; cinder_backend_ceph: \u0026#34;no\u0026#34; nova_backend_ceph: \u0026#34;no\u0026#34; EOF $cat ./add-to-globals.yml | sudo tee -a /etc/kolla/globals.yml 接下來透過ansible幫我們產生OpneStack設定檔\n1 $ansible-playbook -e @/etc/kolla/globals.yml -e @/etc/kolla/passwords.yml -e CONFIG_DIR=/etc/kolla kolla-kubernetes/ansible/site.yml 使用官方提供的腳本幫助我們快速的把OpenStack的password，加到Kubernetes secrets\n1 $kolla-kubernetes/tools/secret-generator.py create 使用之前在pip安裝的檔案，快速的把OpenStack的設定檔，加入Kubernetes config maps裡\n1 2 3 4 5 6 7 8 9 10 $kollakube res create configmap \\ mariadb keystone horizon rabbitmq memcached nova-api nova-conductor \\ nova-scheduler glance-api-haproxy glance-registry-haproxy glance-api \\ glance-registry neutron-server neutron-dhcp-agent neutron-l3-agent \\ neutron-metadata-agent neutron-openvswitch-agent openvswitch-db-server \\ openvswitch-vswitchd nova-libvirt nova-compute nova-consoleauth \\ nova-novncproxy nova-novncproxy-haproxy neutron-server-haproxy \\ nova-api-haproxy cinder-api cinder-api-haproxy cinder-backup \\ cinder-scheduler cinder-volume iscsid tgtd keepalived \\ placement-api placement-api-haproxy 透過官方提供的腳本建立Kolla Helm Chart\n1 $kolla-kubernetes/tools/helm_build_all.sh . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 $cat \u0026lt;\u0026lt;EOF \u0026gt; cloud.yaml global: kolla: all: docker_registry: docker.io image_tag: \u0026#34;4.0.0\u0026#34; kube_logger: false external_vip: \u0026#34;192.168.7.105\u0026#34; base_distro: \u0026#34;centos\u0026#34; install_type: \u0026#34;source\u0026#34; tunnel_interface: \u0026#34;docker0\u0026#34; keystone: all: admin_port_external: \u0026#34;true\u0026#34; dns_name: \u0026#34;192.168.7.105\u0026#34; public: all: port_external: \u0026#34;true\u0026#34; rabbitmq: all: cookie: 67 glance: api: all: port_external: \u0026#34;true\u0026#34; cinder: api: all: port_external: \u0026#34;true\u0026#34; volume_lvm: all: element_name: cinder-volume daemonset: lvm_backends: - \u0026#39;192.168.7.105\u0026#39;: \u0026#39;cinder-volumes\u0026#39; ironic: conductor: daemonset: selector_key: \u0026#34;kolla_conductor\u0026#34; nova: placement_api: all: port_external: true novncproxy: all: port: 6080 port_external: true openvswitch: all: add_port: true ext_bridge_name: br-ex ext_interface_name: enp1s0f1 setup_bridge: true horizon: all: port_external: true EOF 將該數值修改成環境上Management interface的IP。例如10.0.0.178\n1 $sed -i \u0026#34;s@192.168.7.105@10.0.0.178@g\u0026#34; ./cloud.yaml 將該數值修改成環境上ext_interface_name的interface。例如：eth1\n1 $sed -i \u0026#34;s@enp1s0f1@eth1@g\u0026#34; ./cloud.yaml 將該數值修改成環境上Management interface。例如：eth2\n1 $sed -i \u0026#34;s@docker0@eth2@g\u0026#34; ./cloud.yaml 在這邊建立一個給Openstack rbac ，讓後來helm啟動的元件去取得Kubernetes資源不會有權限問題（OpenStack官方RBAC這一點還沒修正\u0026hellip;）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: default namespace: kolla EOF 一個一個把openstack的服務使用helm啟動起來。（例如mariadb,rabbitmq,memcached\u0026hellip;等)\n1 2 3 4 5 6 7 8 9 10 11 $helm install --debug kolla-kubernetes/helm/service/mariadb --namespace kolla --name mariadb --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/rabbitmq --namespace kolla --name rabbitmq --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/memcached --namespace kolla --name memcached --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/keystone --namespace kolla --name keystone --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/glance --namespace kolla --name glance --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/cinder-control --namespace kolla --name cinder-control --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/horizon --namespace kolla --name horizon --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/openvswitch --namespace kolla --name openvswitch --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/neutron --namespace kolla --name neutron --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/nova-control --namespace kolla --name nova-control --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/nova-compute --namespace kolla --name nova-compute --values ./cloud.yaml 這邊有可能遇到第一個問題，也就是nova-compute init 會卡在這個畫面。\n1 nova-api-create-cell-r288q 0/1 Init:2/3 0 5min 這邊發生了一些問題，官方沒有去修正他。先把這個helm chart 刪除掉，我在這邊修正了helm的設定檔案，去修改keystone的url。\n1 2 3 4 $helm delete --purge nova-compute $vim kolla-bringup/kolla-kubernetes/helm/microservice/nova-api-create-simple-cell-job/templates/nova-api-create-cell.yaml {{- $keystonePort := include \u0026#34;kolla_val_get_str\u0026#34; (dict \u0026#34;key\u0026#34; \u0026#34;port\u0026#34; \u0026#34;searchPath\u0026#34; $keystoneSearchPath \u0026#34;Values\u0026#34; .Values )| default \u0026#34;5000\u0026#34; }} 再重新build一次helm chart\n1 $kolla-kubernetes/tools/helm_build_all.sh . 再重新run一次修正過後的chart.\n1 helm install --debug kolla-kubernetes/helm/service/nova-compute --namespace kolla --name nova-compute --values ./cloud.yaml 確認所有服務運作正常\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 $ kubectl get pod -n kolla NAME READY STATUS RESTARTS AGE cinder-api-5d6fd874b5-tzwlr 3/3 Running 0 11h cinder-create-db-x552q 0/2 Completed 0 11h cinder-create-keystone-endpoint-admin-7xhjp 0/1 Completed 0 11h cinder-create-keystone-endpoint-adminv2-252bg 0/1 Completed 0 11h cinder-create-keystone-endpoint-adminv3-4zcv7 0/1 Completed 0 11h cinder-create-keystone-endpoint-internal-s2n77 0/1 Completed 0 11h cinder-create-keystone-endpoint-internalv2-9wr7f 0/1 Completed 0 11h cinder-create-keystone-endpoint-internalv3-2srh2 0/1 Completed 0 11h cinder-create-keystone-endpoint-public-7mksf 0/1 Completed 0 11h cinder-create-keystone-endpoint-publicv2-pqhms 0/1 Completed 0 11h cinder-create-keystone-endpoint-publicv3-8z6xg 0/1 Completed 0 11h cinder-create-keystone-service-4sbp6 0/1 Completed 0 11h cinder-create-keystone-servicev2-9h88v 0/1 Completed 0 11h cinder-create-keystone-servicev3-p89wk 0/1 Completed 0 11h cinder-create-keystone-user-4whfz 0/1 Completed 0 11h cinder-manage-db-hgppr 0/1 Completed 0 11h cinder-scheduler-0 1/1 Running 0 11h glance-api-6f649fbf8d-9hwzn 1/1 Running 0 11h glance-create-db-76lwc 0/2 Completed 0 11h glance-create-keystone-endpoint-admin-m4mxm 0/1 Completed 0 11h glance-create-keystone-endpoint-internal-q9whd 0/1 Completed 0 11h glance-create-keystone-endpoint-public-stszm 0/1 Completed 0 11h glance-create-keystone-service-hcznf 0/1 Completed 0 11h glance-create-keystone-user-9f2g7 0/1 Completed 0 11h glance-manage-db-ch6rp 0/1 Completed 0 11h glance-registry-684d9cc765-d5g5p 3/3 Running 0 11h horizon-7bc45d8df6-8ndt6 1/1 Running 0 11h keystone-b55d658-4bmpf 1/1 Running 0 12h keystone-create-db-rb9wq 0/2 Completed 0 12h keystone-create-endpoints-65m86 0/1 Completed 0 12h keystone-fernet-setup-job-knfm8 0/1 Completed 0 12h keystone-manage-db-x4m2n 0/1 Completed 0 12h mariadb-0 1/1 Running 0 12h mariadb-init-element-ndbxt 0/1 Completed 0 12h memcached-7b95fd6b69-v8f4v 2/2 Running 0 12h neutron-create-db-w2hqk 0/2 Completed 0 11h neutron-create-keystone-endpoint-admin-hkg8p 0/1 Completed 0 11h neutron-create-keystone-endpoint-internal-cwzrt 0/1 Completed 0 11h neutron-create-keystone-endpoint-public-bjzzk 0/1 Completed 0 11h neutron-create-keystone-service-q7ms9 0/1 Completed 0 11h neutron-create-keystone-user-zvqnw 0/1 Completed 0 11h neutron-dhcp-agent-l5qkg 1/1 Running 0 11h neutron-l3-agent-network-64v9x 1/1 Running 0 11h neutron-manage-db-5dqkn 0/1 Completed 0 11h neutron-metadata-agent-network-ttf5v 1/1 Running 0 11h neutron-openvswitch-agent-network-j6llm 1/1 Running 0 11h neutron-server-6d74c78c98-xzdd8 3/3 Running 0 11h nova-api-7d5cf595bc-rxg4k 3/3 Running 0 11h nova-api-create-cell-r288q 0/1 Completed 0 11h nova-api-create-db-5w2lg 0/2 Completed 0 11h nova-api-manage-db-wd4b8 0/1 Completed 0 11h nova-cell0-create-db-5bz6v 0/2 Completed 0 11h nova-compute-wn5lb 1/1 Running 0 11h nova-compute-xkv8r 1/1 Running 0 11h nova-conductor-0 1/1 Running 0 11h nova-consoleauth-0 1/1 Running 0 11h nova-create-db-476gl 0/2 Completed 0 11h nova-create-keystone-endpoint-admin-xbt8x 0/1 Completed 0 11h nova-create-keystone-endpoint-internal-58dvx 0/1 Completed 0 11h nova-create-keystone-endpoint-public-8c56c 0/1 Completed 0 11h nova-create-keystone-service-jngxg 0/1 Completed 0 11h nova-create-keystone-user-4gc62 0/1 Completed 0 11h nova-libvirt-kbcrl 1/1 Running 0 11h nova-libvirt-n2nnj 1/1 Running 0 11h nova-novncproxy-79bf74796f-9p7ct 3/3 Running 0 11h nova-scheduler-0 1/1 Running 0 11h openvswitch-ovsdb-network-rwwrz 1/1 Running 0 11h openvswitch-vswitchd-network-6q9w9 1/1 Running 0 11h placement-api-create-keystone-endpoint-admin-bg9ct 0/1 Completed 0 11h placement-api-create-keystone-endpoint-internal-v998h 0/1 Completed 0 11h placement-api-create-keystone-endpoint-public-p6mvf 0/1 Completed 0 11h placement-api-fc8f68544-rvhwc 1/1 Running 0 11h placement-create-keystone-service-blj57 0/1 Completed 0 11h placement-create-keystone-user-tw5k4 0/1 Completed 0 11h rabbitmq-0 1/1 Running 0 12h rabbitmq-init-element-gtvgw 0/1 Completed 0 12h 使用官方的tool建立admin的openrc file，建立完成的檔案會存在當前使用者的home目錄下。\n1 $kolla-kubernetes/tools/build_local_admin_keystonerc.sh ext 接者安裝OpenStack clients套件\n1 2 3 $sudo pip install \u0026#34;python-openstackclient\u0026#34; $sudo pip install \u0026#34;python-neutronclient\u0026#34; $sudo pip install \u0026#34;python-cinderclient\u0026#34; 使用openstack語法去產生image,network\u0026hellip;等\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $source ~/keystonerc_admin $IMAGE_URL=http://download.cirros-cloud.net/0.3.5/ $IMAGE=cirros-0.3.5-x86_64-disk.img $IMAGE_NAME=cirros $IMAGE_TYPE=linux EXT_NET_CIDR=\u0026#39;172.24.10.1/24\u0026#39; EXT_NET_RANGE=\u0026#39;start=172.24.10.10,end=172.24.10.200\u0026#39; EXT_NET_GATEWAY=\u0026#39;172.24.10.1\u0026#39; $curl -L -o ./${IMAGE} ${IMAGE_URL}/${IMAGE} $openstack image create --disk-format qcow2 --container-format bare --public \\ --property os_type=${IMAGE_TYPE} --file ./${IMAGE} ${IMAGE_NAME} $openstack network create --external --provider-physical-network physnet1 \\ --provider-network-type flat public1 $openstack subnet create --no-dhcp \\ --allocation-pool ${EXT_NET_RANGE} --network public1 \\ --subnet-range ${EXT_NET_CIDR} --gateway ${EXT_NET_GATEWAY} public1-subnet openstack flavor create --id 1 --ram 512 --disk 1 --vcpus 1 m1.tiny openstack flavor create --id 2 --ram 2048 --disk 20 --vcpus 1 m1.small openstack flavor create --id 3 --ram 4096 --disk 40 --vcpus 2 m1.medium openstack flavor create --id 4 --ram 8192 --disk 80 --vcpus 4 m1.large openstack flavor create --id 5 --ram 16384 --disk 160 --vcpus 8 m1.xlarge 可以使用openstack horizon操作openstack dashboard，使用admin user的帳號、密碼進行登入。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 $kubectl get service -n kolla NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cinder-api ClusterIP 10.3.3.125 10.0.0.182 8776/TCP 12h glance-api ClusterIP 10.3.3.141 10.0.0.182 9292/TCP 12h glance-registry ClusterIP 10.3.3.15 \u0026lt;none\u0026gt; 9191/TCP 12h horizon ClusterIP 10.3.3.11 10.0.0.182 80/TCP 12h keystone-admin ClusterIP 10.3.3.35 10.0.0.182 35357/TCP 12h keystone-internal ClusterIP 10.3.3.228 \u0026lt;none\u0026gt; 5000/TCP 12h keystone-public ClusterIP 10.3.3.124 10.0.0.182 5000/TCP 12h mariadb ClusterIP 10.3.3.98 \u0026lt;none\u0026gt; 3306/TCP 12h memcached ClusterIP 10.3.3.140 \u0026lt;none\u0026gt; 11211/TCP 12h neutron-server ClusterIP 10.3.3.21 10.0.0.182 9696/TCP 12h nova-api ClusterIP 10.3.3.150 10.0.0.182 8774/TCP 12h nova-metadata ClusterIP 10.3.3.217 \u0026lt;none\u0026gt; 8775/TCP 12h nova-novncproxy ClusterIP 10.3.3.4 10.0.0.182 6080/TCP 12h nova-placement-api ClusterIP 10.3.3.159 10.0.0.182 8780/TCP 12h rabbitmq ClusterIP 10.3.3.66 \u0026lt;none\u0026gt; 5672/TCP 12h rabbitmq-mgmt ClusterIP 10.3.3.37 \u0026lt;none\u0026gt; 15672/TCP 12h $cat keystonerc_admin unset OS_SERVICE_TOKEN export OS_USERNAME=admin export OS_PASSWORD=kQHEss3THBmHCQWdqNd2b51U8xRB3hKPH6KD4kx3 export OS_AUTH_URL=http://10.0.0.182:5000/v3 export PS1=\u0026#39;[\\u@\\h \\W(keystone_admin)]$ \u0026#39; export OS_PROJECT_NAME=admin export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_IDENTITY_API_VERSION=3 export OS_REGION_NAME=RegionOne export OS_VOLUME_API_VERSION=2 如果需要拆除你的OpenStack Kolla Kubernetes環境 在你的master上，使用helm拆除OpenStack相關元件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $helm install --debug ~/kolla-bringup/kolla-kubernetes/helm/service/nova-cleanup --namespace kolla --name nova-cleanup --values ~/kolla-bringup/cloud.yaml $helm delete mariadb --purge $helm delete mariadb --purge $helm delete rabbitmq --purge $helm delete memcached --purge $helm delete keystone --purge $helm delete glance --purge $helm delete cinder-control --purge $helm delete horizon --purge $helm delete openvswitch --purge $helm delete neutron --purge $helm delete nova-control --purge $helm delete nova-compute --purge $helm delete nova-cell0-create-db-job --purge $helm delete cinder-volume-lvm --purge 在每個節點上拆除相關的OpenStack volume\n1 $sudo rm -rf /var/lib/kolla/volumes/* 在每個節點上刪除Kubernetes\n1 2 3 4 $sudo kubeadm reset $sudo rm -rf /etc/kolla $sudo rm -rf /etc/kubernetes $sudo rm -rf /etc/kolla-kubernetes ","description":"","id":69,"section":"posts","tags":["openStack","kubernetes"],"title":"Kolla Kubernetes","uri":"https://blog.jjmengze.website/zh-tw/posts/openstack/kolla-kubernetes/"},{"content":" TODO","description":"","id":70,"section":"posts","tags":["GRPC","proto"],"title":"GRPC 與 Proto buffer 附身合體","uri":"https://blog.jjmengze.website/zh-tw/posts/protocol/grpc-server-client/"},{"content":" 什麼是Protocol Buffer Protocol Buffer 是由是 Google 所推出的一種輕量且高效的結構化資料存儲格式。\n將結構化的資料進行序列化，實現資料的傳輸以及資料的儲存。\nProtocol Buffer 比 XML、Json 更小、更快、使用及維護上更加的簡單（可以將 api speic 直接轉換成魏應的程式語言）！\n優點 體積小 跨平台 跨語言 傳輸速度快 維護成本低 序列化速度快 定義資料結構 結構的定義很簡單，檔案以 .proto 作為後輟。\nCoding style可以參考這裡：https://developers.google.com/protocol-buffers/docs/style\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 syntax = \u0026#34;proto3\u0026#34;; package tutorial; message Employee { string name = 1; int32 id = 2; enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } message PhoneNumber { string number = 1; PhoneType type = 2; } repeated PhoneNumber phones = 3; } 第一行指定您正在使用proto3語法\n接下來package定義在golang裡所屬於的package\nmessage定義傳輸的訊息\n訊息內容有name、id、枚舉以及一組內部phone number的message\n編譯所撰寫好的.proto 安裝編譯時所需要用的套件，也可以參考官方的方法決定安裝的方式。\n1 go get -u github.com/golang/protobuf/protoc-gen-go 我們使用剛剛安裝的套件進行編譯成 go library 或是可以編成 java python \u0026hellip;. 等的 library 以達到跨平台跨語言的支持，生成的 library 直接幫我們寫好壓縮以及解壓縮的方法開發者只要專心撰寫自己的業務邏輯即可，本篇文章以 go 作為範例。\n1 protoc --go_out=. *.proto 編譯完成後的檔案會是樣的命名方式：employee.pb.go\n檔案內容會如同下面的範例所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 // Code generated by protoc-gen-go. DO NOT EDIT. // source: employee.proto package tutorial import proto \u0026#34;github.com/golang/protobuf/proto\u0026#34; import fmt \u0026#34;fmt\u0026#34; import math \u0026#34;math\u0026#34; // Reference imports to suppress errors if they are not otherwise used. var _ = proto.Marshal var _ = fmt.Errorf var _ = math.Inf // This is a compile-time assertion to ensure that this generated file // is compatible with the proto package it is being compiled against. // A compilation error at this line likely means your copy of the // proto package needs to be updated. const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package type Employee_PhoneType int32 const ( Employee_MOBILE Employee_PhoneType = 0 Employee_HOME Employee_PhoneType = 1 Employee_WORK Employee_PhoneType = 2 ) var Employee_PhoneType_name = map[int32]string{ 0: \u0026#34;MOBILE\u0026#34;, 1: \u0026#34;HOME\u0026#34;, 2: \u0026#34;WORK\u0026#34;, } var Employee_PhoneType_value = map[string]int32{ \u0026#34;MOBILE\u0026#34;: 0, \u0026#34;HOME\u0026#34;: 1, \u0026#34;WORK\u0026#34;: 2, } func (x Employee_PhoneType) String() string { return proto.EnumName(Employee_PhoneType_name, int32(x)) } func (Employee_PhoneType) EnumDescriptor() ([]byte, []int) { return fileDescriptor_employee_7c804fd7a46a4aa3, []int{0, 0} } type Employee struct { Name string `protobuf:\u0026#34;bytes,1,opt,name=name\u0026#34; json:\u0026#34;name,omitempty\u0026#34;` Id int32 `protobuf:\u0026#34;varint,2,opt,name=id\u0026#34; json:\u0026#34;id,omitempty\u0026#34;` Phones []*Employee_PhoneNumber `protobuf:\u0026#34;bytes,3,rep,name=phones\u0026#34; json:\u0026#34;phones,omitempty\u0026#34;` XXX_NoUnkeyedLiteral struct{} `json:\u0026#34;-\u0026#34;` XXX_unrecognized []byte `json:\u0026#34;-\u0026#34;` XXX_sizecache int32 `json:\u0026#34;-\u0026#34;` } func (m *Employee) Reset() { *m = Employee{} } func (m *Employee) String() string { return proto.CompactTextString(m) } func (*Employee) ProtoMessage() {} func (*Employee) Descriptor() ([]byte, []int) { return fileDescriptor_employee_7c804fd7a46a4aa3, []int{0} } func (m *Employee) XXX_Unmarshal(b []byte) error { return xxx_messageInfo_Employee.Unmarshal(m, b) } func (m *Employee) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) { return xxx_messageInfo_Employee.Marshal(b, m, deterministic) } func (dst *Employee) XXX_Merge(src proto.Message) { xxx_messageInfo_Employee.Merge(dst, src) } func (m *Employee) XXX_Size() int { return xxx_messageInfo_Employee.Size(m) } func (m *Employee) XXX_DiscardUnknown() { xxx_messageInfo_Employee.DiscardUnknown(m) } var xxx_messageInfo_Employee proto.InternalMessageInfo func (m *Employee) GetName() string { if m != nil { return m.Name } return \u0026#34;\u0026#34; } func (m *Employee) GetId() int32 { if m != nil { return m.Id } return 0 } func (m *Employee) GetPhones() []*Employee_PhoneNumber { if m != nil { return m.Phones } return nil } type Employee_PhoneNumber struct { Number string `protobuf:\u0026#34;bytes,1,opt,name=number\u0026#34; json:\u0026#34;number,omitempty\u0026#34;` Type Employee_PhoneType `protobuf:\u0026#34;varint,2,opt,name=type,enum=tutorial.Employee_PhoneType\u0026#34; json:\u0026#34;type,omitempty\u0026#34;` XXX_NoUnkeyedLiteral struct{} `json:\u0026#34;-\u0026#34;` XXX_unrecognized []byte `json:\u0026#34;-\u0026#34;` XXX_sizecache int32 `json:\u0026#34;-\u0026#34;` } func (m *Employee_PhoneNumber) Reset() { *m = Employee_PhoneNumber{} } func (m *Employee_PhoneNumber) String() string { return proto.CompactTextString(m) } func (*Employee_PhoneNumber) ProtoMessage() {} func (*Employee_PhoneNumber) Descriptor() ([]byte, []int) { return fileDescriptor_employee_7c804fd7a46a4aa3, []int{0, 0} } func (m *Employee_PhoneNumber) XXX_Unmarshal(b []byte) error { return xxx_messageInfo_Employee_PhoneNumber.Unmarshal(m, b) } func (m *Employee_PhoneNumber) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) { return xxx_messageInfo_Employee_PhoneNumber.Marshal(b, m, deterministic) } func (dst *Employee_PhoneNumber) XXX_Merge(src proto.Message) { xxx_messageInfo_Employee_PhoneNumber.Merge(dst, src) } func (m *Employee_PhoneNumber) XXX_Size() int { return xxx_messageInfo_Employee_PhoneNumber.Size(m) } func (m *Employee_PhoneNumber) XXX_DiscardUnknown() { xxx_messageInfo_Employee_PhoneNumber.DiscardUnknown(m) } var xxx_messageInfo_Employee_PhoneNumber proto.InternalMessageInfo func (m *Employee_PhoneNumber) GetNumber() string { if m != nil { return m.Number } return \u0026#34;\u0026#34; } func (m *Employee_PhoneNumber) GetType() Employee_PhoneType { if m != nil { return m.Type } return Employee_MOBILE } func init() { proto.RegisterType((*Employee)(nil), \u0026#34;tutorial.Employee\u0026#34;) proto.RegisterType((*Employee_PhoneNumber)(nil), \u0026#34;tutorial.Employee.PhoneNumber\u0026#34;) proto.RegisterEnum(\u0026#34;tutorial.Employee_PhoneType\u0026#34;, Employee_PhoneType_name, Employee_PhoneType_value) } func init() { proto.RegisterFile(\u0026#34;employee.proto\u0026#34;, fileDescriptor_employee_7c804fd7a46a4aa3) } var fileDescriptor_employee_7c804fd7a46a4aa3 = []byte{ // 208 bytes of a gzipped FileDescriptorProto 0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xe2, 0xe2, 0x4b, 0xcd, 0x2d, 0xc8, 0xc9, 0xaf, 0x4c, 0x4d, 0xd5, 0x2b, 0x28, 0xca, 0x2f, 0xc9, 0x17, 0xe2, 0x28, 0x29, 0x2d, 0xc9, 0x2f, 0xca, 0x4c, 0xcc, 0x51, 0x7a, 0xc3, 0xc8, 0xc5, 0xe1, 0x0a, 0x95, 0x14, 0x12, 0xe2, 0x62, 0xc9, 0x4b, 0xcc, 0x4d, 0x95, 0x60, 0x54, 0x60, 0xd4, 0xe0, 0x0c, 0x02, 0xb3, 0x85, 0xf8, 0xb8, 0x98, 0x32, 0x53, 0x24, 0x98, 0x14, 0x18, 0x35, 0x58, 0x83, 0x98, 0x32, 0x53, 0x84, 0xcc, 0xb8, 0xd8, 0x0a, 0x32, 0xf2, 0xf3, 0x52, 0x8b, 0x25, 0x98, 0x15, 0x98, 0x35, 0xb8, 0x8d, 0xe4, 0xf4, 0x60, 0x66, 0xe9, 0xc1, 0xcc, 0xd1, 0x0b, 0x00, 0x29, 0xf0, 0x2b, 0xcd, 0x4d, 0x4a, 0x2d, 0x0a, 0x82, 0xaa, 0x96, 0x0a, 0xe7, 0xe2, 0x46, 0x12, 0x16, 0x12, 0xe3, 0x62, 0xcb, 0x03, 0xb3, 0xa0, 0x96, 0x41, 0x79, 0x42, 0x06, 0x5c, 0x2c, 0x25, 0x95, 0x05, 0xa9, 0x60, 0x0b, 0xf9, 0x8c, 0x64, 0x70, 0x19, 0x1e, 0x52, 0x59, 0x90, 0x1a, 0x04, 0x56, 0xa9, 0xa4, 0xcd, 0xc5, 0x09, 0x17, 0x12, 0xe2, 0xe2, 0x62, 0xf3, 0xf5, 0x77, 0xf2, 0xf4, 0x71, 0x15, 0x60, 0x10, 0xe2, 0xe0, 0x62, 0xf1, 0xf0, 0xf7, 0x75, 0x15, 0x60, 0x04, 0xb1, 0xc2, 0xfd, 0x83, 0xbc, 0x05, 0x98, 0x92, 0xd8, 0xc0, 0xfe, 0x37, 0x06, 0x04, 0x00, 0x00, 0xff, 0xff, 0x5c, 0x1e, 0x53, 0x56, 0x11, 0x01, 0x00, 0x00, } 小結 下一章節會使用本篇基於 protobuffer 建置出來的 go library 去撰寫一個簡單的 client server，並且透過 GRPC 去傳輸 protobuffer。\n","description":"","id":71,"section":"posts","tags":["GRPC","proto"],"title":"初嚐Protocol Buffer好滋味","uri":"https://blog.jjmengze.website/zh-tw/posts/protocol/grpc/"},{"content":" 記錄一下，Open vSwitch (OVS)常用的指令。\n在ubuntu上安裝ovs\nsudo apt install openvswitch-switch -y 新增一個bridge sudo ovs-vsctl add-br br0 顯示所有bridge sudo ovs-vsctl list-br 給某個bridge新增一個port sudo ovs-vsctl add-port br0 veth1 顯示綁定到該port的bridge sudo ovs-vsctl port-to-br veth1 連接controller到ovs br0上 sudo ovs-vsctl set-controller br0 tcp:\u0026lt;controller IP\u0026gt; 顯示所有的bridge、連接的port以及controller sudo ovs-vsctl show ","description":"","id":72,"section":"posts","tags":["SDN"],"title":"OVS 常用的指令","uri":"https://blog.jjmengze.website/zh-tw/posts/sdn/ovs-commonly-command/"},{"content":" kubernetes Controller Manager 的那些元件 在 Kubernetes Master Node 另外一個相當重要的元件可視為整體 Kubernetes 系統架構的監控狀態中心，也是就是 Controller Manager 元件，主要由以下幾種控制器控制 Kubernetes Container 叢集的狀態：\nReplication Controller ：監控叢集的副本狀態並盡可能修正副本狀態。\nNode Controller ： 監控叢集的 Node 狀態並負責 Node 狀態的更新。\nCronJob Controller ：監控叢集週期性任務的狀態並負責任務狀態的更新。\nDaemonSet Controller ：監控叢集 Daemon 資源型態的狀態並負責其狀態的更新。\nDeployment Controller ：監控叢集 Deployment 資源型態的狀態並負責與 Replication Controller 進行交互與狀態的更新。\nEndpoint Controller ：監控叢集 Pod 的 IP 狀態並負責與 Service Controller 進行資料的交換。\nGarbage Collector Controller ：收集已被叢集刪除的元件確認是否還有依賴的資源未被刪除。\nNamespace Controller ：維護並且監控叢集 Namespace 資源型態建立與刪除的狀態。\nJob Controller ：監控叢集單次任務的狀態並負責任務狀態的更新。\nPod AutoScaler Controller ：監控叢集 Pod 水平擴展以及負責該擴展的狀態。\nRelicaSet Controller ：監控叢集副本升級狀態以及維護該升級資訊。\nService Controller ：監控叢集服務暴露資訊之狀態並與 Endpoint Controller 進行資料的交互更新。\nServiceAccount Controller ：負責監控叢集服務應用的帳戶認證與權限控管。\nStatefulSet Controller ：監控叢集現有狀態服務的變化負責其資源的建立與刪除。\nVolume Controller ：監控叢集掛載 volume 資訊與狀態並負責即時更新。\nResource quota Controller ：監控叢集 CPU 與 Memory 資源資訊並回饋給 API-Server 當前資源狀態。\n圖引用自superuser\n","description":"","id":73,"section":"posts","tags":["kubernetes"],"title":"Kubernetes Control Plane 這件小小事(controller-manger)","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/kubernetes-controller-manger-overview/"},{"content":" kubernetes control plane 的那些元件 如下圖所示， Kubernektes 整體架構是主從式架構( Client–server model )由 Master Node 負責 Container 叢集管理與調度相當於 Kubernetes 的大腦， Worker Node 負責運行 Master Node 所派送過來的任務，並將任務確實執行並且定期回報 Master Node 目前的節點的狀態，所以了解 Kubernetes 的大腦是一件相當重要的事情。\n在 Master Node 主要由 API-Server 、 Controller Manager 以及 Scheduler 三大基礎元件所組合而成，其中 API-Server 元件可視為整體 Kubernetes Container 調度系統架構的管理中心，也是 Kubernetes Container 叢集調度系統中唯一能與後端儲存 etcd 元件溝通的元件，主要提供以下服務：\nAPI-Server 提供叢集安全且可靠的使用機制，所有的 Client 請求都需要經過 API-Server 的認證。 API-Server 支持多種認證機制如 WebSocket 、 Keystone 、 Token 等等，如果 API-Server 認證成功， Client 請求將會傳入 Kubernetes 叢集內進行處理；而對於認證失敗的請求則返回 HTTP 401 的錯誤。\nHTTPS Certificate Authority ，基於 Certificate Authority (CA) 證書簽名的雙向數字證書認證方式。 HTTP Token Authority ，透過發放 Token 的方式識別每個使用者的身份，以及透過該 Token 限制使用者能存取的範圍。 HTTP Basic Authority ，當使用者向 API Server 發起 HTTP 請求時，會在資料中帶入 Username 、 UID 、 Groups 、 Extra fields 作為使用者的身份驗證的依據。 API-Server 同時提供 REST API 的呼叫介面如 Create 、 Read 、 Update 、 Delete 等一系列操作對 Kubernetes 的 Pod 、 Service 、 Deployment 等元件進行操作，例如使用者可以透過 REST API 向 API-Server 請求建立、刪除、更新以及讀取 Kubernetes 的資源。\nAPI-Server 元件提供專有的監控介面給 Controller Manger 元件與 Scheduler 元件，可以透過該專有介面讀取特定資料，例如 Controller Manger 透過 API-Server 提供的介面讀取 Pod 的狀態並且加以控制， Scheduler 透過介面讀取 Kubernetes Node 目前的壓力情況作為排程 Pod 的參考依據。\n大致上流程如下圖所示，當請求送進 kubernetes cluster 會通過Api server的認證，接著檢查該次請求的內容請求，最後再送進 etcd 做永久性儲存。（以後有機會再來談談Authoization跟Admission control 的細節）\n引用giantswarm.io\n","description":"","id":74,"section":"posts","tags":["kubernetes"],"title":"Kubernetes Control Plane 這件小小事(api-server)","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/kubernetes-apiserver-overview/"},{"content":" 來說說etcd Kubernetes Container 叢集調度系統中後端採用 etcd 作為叢集狀態的儲存元件，而該 etcd 元件是由 CoreOS 公司開發並且開源貢獻給 CNCF 的 key/value 儲存專案。 etcd 主要負責將 Kubernetes API-Server 所處理過的資料進行加密儲存。 etcd 元件大多數會位於 Kubernetes 系統架構中的 Master Node 上。 API-Server 若要存取 etcd 上的資料必須先經過 etcd 的身分驗證流程確認使用者存取的身份與範圍是否合法，若身分確認無誤與存取範圍合法則將使用者所請求的資料透過 REST API 回傳資料給使用者。\nKubernetes 在通常情況下只會替系統管理員在 Master Node 上建立一個 etcd 元件，可能因為外在因素或是 etcd 請求負載過於龐大使得 etcd 元件發生故障，進而導致整體 Kubernetes發生API-Server 無法存取後端資料的問題。針對此問題系統管理員可以透過 Kubernetes Container 叢集調度技術建立一個高可用的 etcd 服務叢集，若單一個 etcd 元件發生故障時仍有其他 etcd 元件可以立即地補上作為備援使用。\netcd 怎麼在分散的世界保持一致 etcd 元件組成 etcd 叢集服務時首要任務就是確保後端資料是一制性的，以防使用者存取到錯誤的資料， etcd 採用的是 Raft 一至性共識演算法。\n透過 Raft 演算法使得在同一個時間點上可以維持多個 etcd 元件儲存的狀態，這就保證了後端儲存的資料是保持一致，此外該演算法保證了當少數 etcd 元件崩潰或失效時仍不影響整體叢集的同步。\nRaft 就是一種 leader-based 的共識算法並且使用心跳機制 (Heartbeat mechanism) 觸發同步與選舉，在該演算法裡面分別有三種身份第一種是 Leader 主要是叢集的領導人其他身份都必須跟領導人的資料同步，第二種是 Candidate 是當領導人無回應時欲參加該次選舉的節點稱為候選人，最後一種為 Follower 在選舉期間負責投票給候選人的節點如圖所示。\n選舉的過程 一開始叢集各個節點身份都為Follower，並且節點上預設的選舉逾時時間(Election Time Out)與一般作業時間(Normal Operation)皆為範圍隨機亂數。在一般作業時間內節點會期待自己收到Leader週期性傳送過來的Heartbeat並且重置一般作業時間，若是超過一般作業時間節點都沒有收到Leader所傳送過來的Heartbeat該節點會當作目前沒有Leader的狀態，將觸發Leader的重選。\n由於節點觸發Leader的重選此時節點將從Follower身份轉變為Candidate身份並且紀錄當前任期的號碼向叢集內其餘節點發送投票之請求，在觸發選舉後會有一段選舉逾時時間在該時間內，對於同一任期Follower只能選投一名Candidate，Candidate收到大於等於(2n+1) Follower的同意票，則該節點從Candidate身份轉變為叢集的新Leader。若在選舉逾時時間結束沒有收到(2n+1) Follower的同意票則視為當前選舉無效，再增加一次當前任期重新發起新一輪的選舉。而新的Leader需透過週期性的發送Heartbeat給各個節點來維持該Leader身份。\n當發生在選舉逾時時間結束沒有收到(2n+1) Follower的同意票，無法選出Leader的情況，根據Raft演算法會讓節點縮短選舉逾時時間加速選出叢集的Leader以減少叢集無法同步的時間，另外在選舉期間節點可能會收到來自其他節點宣稱自己是Leader的心跳，該節點會確認心跳訊息內的任期編號是否大於當前任期，若是大於當前任期所有節點身份轉變為Follower身份並且與該Leader同步資料，若是心跳訊息的任期編號小於當前任期則無視該節點，繼續任期投票工作。\n當叢集的Leader被選出之後，Leader會將使用者的每個請求都包裝成一個Commit並且透過Heartbeat週期性的複製到其他節點做為副本，當一個commit超過半數的節點都以複製並儲存才會能算該commit成功。此外該commit的元資料(metadata)會記錄當時commit的Leader任期號碼是多少，該任期號用來判斷節點之間副本不一致情形並且在每個一個節點儲存Commit都會有一個整數索引值來確認其在Commit目前已經儲存到第幾個位址如圖所示。\n各個節點發生儲存內容不一致的情況，若任期三是第三節點繼續當Leader，在Leader發送Heartbeat的同時會將其他節點的資料強制覆蓋過去，利用Leader的強制性解決資料不統一的情況。第三節點發送一次Heartbeat強制同步個節點的index的Commit，如下圖所示。\n後話 會繼續接著把 Kubernetes 的其他元件繼續講解一遍，如果有誤的地方歡迎大家指出謝謝～～\nREF \u0026ldquo;Etcd | Coreos\u0026rdquo;. Coreos.Com, 2019, https://coreos.com/blog/etcd. Oliveira, Caio, et al. \u0026ldquo;Evaluating raft in docker on kubernetes.\u0026rdquo; International Conference on Systems Science. Springer, Cham, 2016. ","description":"","id":75,"section":"posts","tags":["kubernetes"],"title":"etcd 那回事","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/etcd/"},{"content":" 什麼是 Kubernetes Kubernetes 是一個以 Apache 授權條款的開源專案主要作為 Container 叢集的系統管理平台，其系統具備高度可擴展性，由 Joe Beda、Brendan Burns 和 Craig McLuckie 等 Google 工程師建立此套系統。\nKubernetes v1.0於2015年7月21日發布。隨著v1.0版本發布，Google與Linux 基金會合作建立了Cloud Native Computing Foundation (CNCF)，並把Kubernetes作為種子技術來提供。\n來看看架構圖 Kubernetes Container 叢集系統管理平台，將能夠透過Kubernetes 的機制提供VNF擁有更好的可靠性、負載平衡、滾動更新等特性。然而在 Kubernetes Container 叢集管理平台系統中，擁有不同的節點角色與基礎元件。\n如下圖所示， Kubernektes 整體架構是主從式架構( Client–server model )由 Master Node 負責 Container 叢集管理與調度相當於 Kubernetes 的大腦， Worker Node 負責運行 Master Node 所派送過來的任務，並將任務確實執行並且定期回報 Master Node 目前的節點的狀態。\n就 Master Node 而言 Kubernetes 提供了一種方便操作的 kubectl 命令列介面工具，對於管理人員來說，只要透過 kubectl 就能向 Kubernetes Container 叢集下達指令並得到當前叢集服務運行的狀態。 Kubernetes 同時考量安全性的問題，當任何人員想要對叢集進行操作都必須先經過 Kubernetes 的身份驗證，當驗證成功叢集才會接收操作命令反之亦然。\n當驗證成功後所有的操作行為例如：部署 Container 服務、存取 Container 服務執行的狀態、刪除 Container 服務等行為，都會經由 Kubernetes Master Node 上的 API-Server 透過gRPC通訊協定向其他元件進行溝通並取回資料，此外在 Kubernetes 叢集每個節點上都會運行一個名為 kubelet 元件，該 kubelet 元件主要負責接收 Master Node 所指派的任務並在該節點上部署 Container 服務並且時刻的監視 node 上的 Container 服務的狀態，並且透過 gRPC 通訊協定回報給 Kubernetes Master Node上的API-Server，當 API-Server 收到節點資訊並彙整後將存入後端的 etcd key/value 資料庫。\n後話 後續包含Scheduler、Controller Manager、API-Server、etcd、kube-proxy、kubelet 等元件，將一一介紹 Kubenetes 架構中的主要元件與相關的資源型態。\n","description":"","id":76,"section":"posts","tags":["kubernetes"],"title":"Kubernetes 淺入淺出","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/kubernetes/"},{"content":" VM 與 Container 架構圖 與 VM 相比使用 Container 具有各種優點，在要執行數百個應用程式的使用情境之下使用基於 Container 的解決方案顯得相當高效， Container 所佔的空間以及效能損失都優於 VM 。在需要快速回收資源與啟動服務的應用情境下，透過 Container 承載這些服務遠比選擇 VM 的效果要好，由於 Container 啟動的速度只需要幾十秒的時間， VM 啟動所需要的時間卻高達數分鐘。\nVM 與 Container 比較表 VM Container Communication 透過Ethernet Devices 透過IPC：Socket、pipes、Signals等機制 Guest OS 獨立的Kernel 共用宿主Kernel Performance 效能消耗較大 效能消耗較小 Isolation 隔離性較好 隔離性較差 Startup time 數分鐘 幾十秒 Storage 佔較大空間 佔較小空間 僅透過 VM ，將相關的 service 部署到 VM 中以用來處理使用者請求的服務，由於 VM 的效能損失以及啟動時間的問題，採用 container 的方式可以大幅度節省效能的損失以及加速部署與產品上限的時間。\n但是對於部署規模較大的運算叢集來說，Container 技術僅能縮短服務啟動的時間，並且沒有一個方便管理 Container 化叢集的方式與確認當前 Container 服務運行的狀態。若管理人員不小心刪除叢集的某一服務將造成災難性的危害。因此，需要建置一個 Container 叢集管理平台管理 Container 服務。\nGoogle 累積多年管理 Container 叢集經驗，進而開發 Kubernetes Container 叢集調度技術的開源專案，同時也是目前業界廣為採用的 Container 叢集管理系統如公有雲的 AWS 的 EKS 、 Google 提供的 GKE 以及 Azure 的 AKS ，對於管理人員來說使用 Kubernetes 除了能大量減少建置叢集的複雜流程外，也可以透過 Kubernetes 內建的元件建置所需要 Container 服務，透過 Kubernetes 可以提高服務的可用性，因此以下將進一步說明 Kubernetes 的系統架構以及相關元件。\nREF Xavier, Miguel G., et al. \u0026ldquo;Performance evaluation of container-based virtualization for high performance computing environments.\u0026rdquo; 2013 21st Euromicro International Conference on Parallel, Distributed, and Network-Based Processing. IEEE, 2013. Soltesz, Stephen, et al. \u0026ldquo;Container-based operating system virtualization: a scalable, high-performance alternative to hypervisors.\u0026rdquo; ACM SIGOPS Operating Systems Review. Vol. 41. No. 3. ACM, 2007. Burns, Brendan, et al. \u0026ldquo;Borg, omega, and kubernetes.\u0026rdquo; (2016). Cherrueau, Ronan-Alexandre, et al. \u0026ldquo;Edge computing resource management system: a critical building block! initiating the debate via openstack.\u0026rdquo; {USENIX} Workshop on Hot Topics in Edge Computing (HotEdge 18). 2018. Web.Kaust.Edu.Sa, 2019, http://web.kaust.edu.sa/Faculty/MarcoCanini/classes/CS240/F17/slides/L3-cloud-VM.pdf ","description":"","id":77,"section":"posts","tags":["kubernetes"],"title":"來看看Container","uri":"https://blog.jjmengze.website/zh-tw/posts/kubernetes/container/"}]